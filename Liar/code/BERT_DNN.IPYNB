{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完整訓練集形狀: (10232, 14)\n",
      "測試集形狀: (2559, 14)\n",
      "          id        label                                          statement  \\\n",
      "0  7193.json         true  Northern Virginia is the most heavily traffick...   \n",
      "1  1608.json    half-true  The Democratic health care bill will \"collect ...   \n",
      "2  3844.json  mostly-true  Says for the first time ever, Texas lawmakers ...   \n",
      "3  1104.json    half-true  Only 15 percent of drug users are African-Amer...   \n",
      "4  3875.json  barely-true  Part of his ride was to warn the British that ...   \n",
      "\n",
      "                                subjects             speaker  \\\n",
      "0                         transportation       bob-mcdonnell   \n",
      "1                      health-care,taxes         todd-tiahrt   \n",
      "2  education,state-budget,state-finances         wendy-davis   \n",
      "3   crime,legal-issues,marijuana,pundits  arianna-huffington   \n",
      "4                        history,pundits         sarah-palin   \n",
      "\n",
      "                      job_title     state       party  barely_true  false  \\\n",
      "0                      Governor  Virginia  republican          6.0    5.0   \n",
      "1           U.S. Representative    Kansas  republican          0.0    0.0   \n",
      "2                 state senator     Texas    democrat          5.0    1.0   \n",
      "3  Founder, The Huffington Post       NaN        none          0.0    0.0   \n",
      "4                           NaN    Alaska  republican          9.0   19.0   \n",
      "\n",
      "   half_true  mostly_true  pants_on_fire  \\\n",
      "0        7.0          6.0            3.0   \n",
      "1        1.0          0.0            0.0   \n",
      "2        8.0         12.0            1.0   \n",
      "3        2.0          3.0            0.0   \n",
      "4        9.0          6.0            6.0   \n",
      "\n",
      "                                 context  \n",
      "0                     a news conference.  \n",
      "1           a speech on the House floor.  \n",
      "2                          a filibuster.  \n",
      "3   This Week with George Stephanopoulos  \n",
      "4                    an interview on Fox  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "df_train_full = pd.read_csv('train_full.tsv', sep='\\t')\n",
    "df_test_split = pd.read_csv('test_split.tsv', sep='\\t')\n",
    "print(\"完整訓練集形狀:\", df_train_full.shape)\n",
    "print(\"測試集形狀:\", df_test_split.shape)\n",
    "print(df_train_full.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_full = le.fit_transform(df_train_full['label'])\n",
    "y_test = le.transform(df_test_split['label'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "\n",
    "pretrained_model_name = \"bert-base-uncased\" \n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "max_length = 128  # 每個樣本最長序列長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, df, label_encoder, tokenizer, max_length=128):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.texts = self.df['statement'].tolist()\n",
    "        self.labels = label_encoder.transform(self.df['label'])\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 使用 BERT tokenizer 對文本編碼\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # output: input_ids, attention_mask (shape: [1, max_length])\n",
    "        input_ids = encoding['input_ids'].squeeze(0)       # -> [max_length]\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)  # -> [max_length]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids.long(),\n",
    "            'attention_mask': attention_mask.long(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_dataloader(df, label_encoder, tokenizer, max_length, batch_size, shuffle=True):\n",
    "    dataset = BERTDataset(df, label_encoder, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "class BertDNN(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_dim, num_classes, freeze_bert=False):\n",
    "        super(BertDNN, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # 是否凍結 BERT 權重\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # DNN 分類器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # BERT 輸出: last_hidden_state, pooler_output, (hidden_states), (attentions)\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        pooled_output = outputs.pooler_output \n",
    "        \n",
    "        # 通過自定義的 DNN\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, num_epochs, lr=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        running_loss = 0.0 \n",
    "        correct = 0 \n",
    "        total = 0 \n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * input_ids.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # --- Testing ---\n",
    "        model.eval()\n",
    "        test_running_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                test_running_loss += loss.item() * input_ids.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        \n",
    "        test_loss = test_running_loss / test_total\n",
    "        test_acc = test_correct / test_total\n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | \"\n",
    "              f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
    "    \n",
    "    return epoch_list, train_losses, test_losses, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完整訓練集形狀: (10232, 14)\n",
      "測試集形狀: (2559, 14)\n",
      "\n",
      "=== 比例 1.0 (約 10232 筆資料) ===\n",
      "Epoch 1: Train Loss=1.7628, Train Acc=0.2040 | Test Loss=1.7230, Test Acc=0.2352\n",
      "Epoch 2: Train Loss=1.7181, Train Acc=0.2341 | Test Loss=1.6936, Test Acc=0.2567\n",
      "Epoch 3: Train Loss=1.6284, Train Acc=0.2937 | Test Loss=1.6922, Test Acc=0.2712\n",
      "分類報告 (Train Subset 100%):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.26      0.22      0.24       421\n",
      "       false       0.32      0.33      0.32       501\n",
      "   half-true       0.24      0.25      0.25       526\n",
      " mostly-true       0.28      0.40      0.33       491\n",
      "  pants-fire       0.27      0.15      0.20       209\n",
      "        true       0.25      0.18      0.21       411\n",
      "\n",
      "    accuracy                           0.27      2559\n",
      "   macro avg       0.27      0.26      0.26      2559\n",
      "weighted avg       0.27      0.27      0.27      2559\n",
      "\n",
      "\n",
      "=== 比例 0.5 (約 5116 筆資料) ===\n",
      "Epoch 1: Train Loss=1.7614, Train Acc=0.2113 | Test Loss=1.7229, Test Acc=0.2450\n",
      "Epoch 2: Train Loss=1.7145, Train Acc=0.2428 | Test Loss=1.7040, Test Acc=0.2427\n",
      "Epoch 3: Train Loss=1.6275, Train Acc=0.3038 | Test Loss=1.6889, Test Acc=0.2755\n",
      "分類報告 (Train Subset 50%):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.28      0.19      0.23       421\n",
      "       false       0.29      0.44      0.35       501\n",
      "   half-true       0.25      0.28      0.26       526\n",
      " mostly-true       0.28      0.26      0.27       491\n",
      "  pants-fire       0.29      0.18      0.22       209\n",
      "        true       0.27      0.23      0.25       411\n",
      "\n",
      "    accuracy                           0.28      2559\n",
      "   macro avg       0.28      0.26      0.26      2559\n",
      "weighted avg       0.27      0.28      0.27      2559\n",
      "\n",
      "\n",
      "=== 比例 0.25 (約 2558 筆資料) ===\n",
      "Epoch 1: Train Loss=1.7712, Train Acc=0.1962 | Test Loss=1.7493, Test Acc=0.2079\n",
      "Epoch 2: Train Loss=1.7343, Train Acc=0.2338 | Test Loss=1.7268, Test Acc=0.2470\n",
      "Epoch 3: Train Loss=1.6775, Train Acc=0.2940 | Test Loss=1.7055, Test Acc=0.2536\n",
      "分類報告 (Train Subset 25%):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.23      0.14      0.17       421\n",
      "       false       0.23      0.45      0.31       501\n",
      "   half-true       0.26      0.29      0.28       526\n",
      " mostly-true       0.28      0.42      0.34       491\n",
      "  pants-fire       0.30      0.01      0.03       209\n",
      "        true       0.31      0.01      0.02       411\n",
      "\n",
      "    accuracy                           0.25      2559\n",
      "   macro avg       0.27      0.22      0.19      2559\n",
      "weighted avg       0.27      0.25      0.22      2559\n",
      "\n",
      "\n",
      "=== 比例 0.1 (約 1023 筆資料) ===\n",
      "Epoch 1: Train Loss=1.7802, Train Acc=0.2023 | Test Loss=1.7582, Test Acc=0.2173\n",
      "Epoch 2: Train Loss=1.7532, Train Acc=0.2239 | Test Loss=1.7535, Test Acc=0.2110\n",
      "Epoch 3: Train Loss=1.7304, Train Acc=0.2581 | Test Loss=1.7480, Test Acc=0.2403\n",
      "分類報告 (Train Subset 10%):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.20      0.66      0.31       421\n",
      "       false       0.29      0.21      0.24       501\n",
      "   half-true       0.28      0.17      0.21       526\n",
      " mostly-true       0.29      0.29      0.29       491\n",
      "  pants-fire       0.00      0.00      0.00       209\n",
      "        true       0.20      0.00      0.00       411\n",
      "\n",
      "    accuracy                           0.24      2559\n",
      "   macro avg       0.21      0.22      0.18      2559\n",
      "weighted avg       0.24      0.24      0.20      2559\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "fractions = [1.0, 0.5, 0.25, 0.1]\n",
    "\n",
    "# 超參數設定\n",
    "hidden_dim = 128        \n",
    "num_epochs = 3       \n",
    "batch_size = 64      \n",
    "learning_rate = 5e-5    \n",
    "\n",
    "for frac in fractions:\n",
    "    num_train_samples = int(df_train_full.shape[0] * frac)\n",
    "    df_train_subset = df_train_full.iloc[:num_train_samples].reset_index(drop=True)\n",
    "    \n",
    "    subset_percentage = int(frac * 100)\n",
    "    df_train_subset.to_csv(f'train_subset_{subset_percentage}.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    print(f\"\\n=== 比例 {frac} (約 {num_train_samples} 筆資料) ===\")\n",
    "    \n",
    "    # 建立 DataLoader\n",
    "    train_loader = create_dataloader(df_train_subset, le, tokenizer, max_length, batch_size, shuffle=True)\n",
    "    test_loader = create_dataloader(df_test_split, le, tokenizer, max_length, batch_size, shuffle=False)\n",
    "    \n",
    "    # 初始化 BERT + DNN\n",
    "    model = BertDNN(pretrained_model_name, hidden_dim, num_classes, freeze_bert=False).to(device)\n",
    "    \n",
    "    # 訓練與評估\n",
    "    epoch_list, train_losses, test_losses, train_accuracies, test_accuracies = \\\n",
    "        train_and_evaluate(model, train_loader, test_loader, num_epochs, lr=learning_rate)\n",
    "    \n",
    "    # 繪製曲線\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss 圖\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_list, train_losses, label='Train Loss')\n",
    "    plt.plot(epoch_list, test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss vs Epoch (Train Subset {subset_percentage}%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy 圖\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_list, train_accuracies, label='Train Acc')\n",
    "    plt.plot(epoch_list, test_accuracies, label='Test Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy vs Epoch (Train Subset {subset_percentage}%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_progress_{subset_percentage}_bert_dnn.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 最終模型的分類報告\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(f\"分類報告 (Train Subset {subset_percentage}%):\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
