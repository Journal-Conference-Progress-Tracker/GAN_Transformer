{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始資料形狀: (9840, 3)\n",
      "訓練集形狀: (7872, 3) 測試集形狀: (1968, 3)\n",
      "標籤類別: ['CONTRADICTION' 'ENTAILMENT' 'NEUTRAL']\n",
      "\n",
      "[比例 100%] 訓練資料筆數: 7872\n",
      "Epoch 1: Train Loss=0.8124, Train Acc=0.5879 | Test Loss=0.7267, Test Acc=0.6052\n",
      "Epoch 2: Train Loss=0.7414, Train Acc=0.6018 | Test Loss=0.7092, Test Acc=0.6128\n",
      "Epoch 3: Train Loss=0.7144, Train Acc=0.6157 | Test Loss=0.7073, Test Acc=0.5971\n",
      "Epoch 4: Train Loss=0.6680, Train Acc=0.6632 | Test Loss=0.6953, Test Acc=0.6347\n",
      "Epoch 5: Train Loss=0.6293, Train Acc=0.6838 | Test Loss=0.7111, Test Acc=0.6225\n",
      "分類報告 (Train Subset 100%):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.57      0.87      0.69       285\n",
      "   ENTAILMENT       0.54      0.41      0.47       564\n",
      "      NEUTRAL       0.67      0.66      0.67      1119\n",
      "\n",
      "     accuracy                           0.62      1968\n",
      "    macro avg       0.60      0.65      0.61      1968\n",
      " weighted avg       0.62      0.62      0.62      1968\n",
      "\n",
      "\n",
      "[比例 50%] 訓練資料筆數: 3936\n",
      "Epoch 1: Train Loss=0.8256, Train Acc=0.5739 | Test Loss=0.7290, Test Acc=0.6062\n",
      "Epoch 2: Train Loss=0.7434, Train Acc=0.5953 | Test Loss=0.7090, Test Acc=0.6179\n",
      "Epoch 3: Train Loss=0.7171, Train Acc=0.6209 | Test Loss=0.6956, Test Acc=0.6291\n",
      "Epoch 4: Train Loss=0.6580, Train Acc=0.6687 | Test Loss=0.7214, Test Acc=0.6159\n",
      "Epoch 5: Train Loss=0.5925, Train Acc=0.7160 | Test Loss=0.7738, Test Acc=0.6103\n",
      "分類報告 (Train Subset 50%):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.59      0.69      0.63       285\n",
      "   ENTAILMENT       0.53      0.35      0.42       564\n",
      "      NEUTRAL       0.64      0.72      0.68      1119\n",
      "\n",
      "     accuracy                           0.61      1968\n",
      "    macro avg       0.59      0.59      0.58      1968\n",
      " weighted avg       0.60      0.61      0.60      1968\n",
      "\n",
      "\n",
      "[比例 25%] 訓練資料筆數: 1968\n",
      "Epoch 1: Train Loss=0.9016, Train Acc=0.5732 | Test Loss=0.7828, Test Acc=0.6052\n",
      "Epoch 2: Train Loss=0.7836, Train Acc=0.5996 | Test Loss=0.7501, Test Acc=0.6052\n",
      "Epoch 3: Train Loss=0.7464, Train Acc=0.6077 | Test Loss=0.7341, Test Acc=0.6077\n",
      "Epoch 4: Train Loss=0.7236, Train Acc=0.6148 | Test Loss=0.7271, Test Acc=0.6128\n",
      "Epoch 5: Train Loss=0.6688, Train Acc=0.6519 | Test Loss=0.7396, Test Acc=0.6032\n",
      "分類報告 (Train Subset 25%):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.60      0.73      0.66       285\n",
      "   ENTAILMENT       0.49      0.29      0.37       564\n",
      "      NEUTRAL       0.63      0.73      0.68      1119\n",
      "\n",
      "     accuracy                           0.60      1968\n",
      "    macro avg       0.57      0.58      0.57      1968\n",
      " weighted avg       0.59      0.60      0.59      1968\n",
      "\n",
      "\n",
      "[比例 10%] 訓練資料筆數: 787\n",
      "Epoch 1: Train Loss=0.9502, Train Acc=0.5451 | Test Loss=0.8517, Test Acc=0.5666\n",
      "Epoch 2: Train Loss=0.8194, Train Acc=0.5909 | Test Loss=0.7925, Test Acc=0.5986\n",
      "Epoch 3: Train Loss=0.7519, Train Acc=0.6086 | Test Loss=0.7679, Test Acc=0.6032\n",
      "Epoch 4: Train Loss=0.7307, Train Acc=0.6112 | Test Loss=0.7797, Test Acc=0.5960\n",
      "Epoch 5: Train Loss=0.6365, Train Acc=0.6912 | Test Loss=0.8191, Test Acc=0.5391\n",
      "分類報告 (Train Subset 10%):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.58      0.53      0.56       285\n",
      "   ENTAILMENT       0.43      0.61      0.50       564\n",
      "      NEUTRAL       0.63      0.51      0.56      1119\n",
      "\n",
      "     accuracy                           0.54      1968\n",
      "    macro avg       0.54      0.55      0.54      1968\n",
      " weighted avg       0.56      0.54      0.54      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# 設定隨機種子與裝置\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"SICK_filtered.tsv\", sep=\"\\t\")\n",
    "print(\"原始資料形狀:\", df.shape)\n",
    "\n",
    "# 以 entailment_label 做 stratified 切分：80% 訓練，20% 測試\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"entailment_label\"])\n",
    "print(\"訓練集形狀:\", df_train.shape, \"測試集形狀:\", df_test.shape)\n",
    "\n",
    "# 標籤編碼：將 entailment_label 轉換為整數\n",
    "le = LabelEncoder()\n",
    "df_train[\"label_enc\"] = le.fit_transform(df_train[\"entailment_label\"])\n",
    "df_test[\"label_enc\"] = le.transform(df_test[\"entailment_label\"])\n",
    "num_classes = len(le.classes_)\n",
    "print(\"標籤類別:\", le.classes_)\n",
    "\n",
    "pretrained_model = \"bert-base-uncased\"  # 若資料為中文，可改用 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# 設定最大序列長度\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "class SICKBERTDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentences_A = df[\"sentence_A\"].tolist()\n",
    "        self.sentences_B = df[\"sentence_B\"].tolist()\n",
    "        self.labels = df[\"label_enc\"].tolist()  # 直接使用標籤編碼結果\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_A = self.sentences_A[idx]\n",
    "        text_B = self.sentences_B[idx]\n",
    "        label = self.labels[idx]\n",
    "        # 對 sentence_A 進行 tokenize\n",
    "        encoding_A = self.tokenizer(\n",
    "            text_A,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # 對 sentence_B 進行 tokenize\n",
    "        encoding_B = self.tokenizer(\n",
    "            text_B,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # 移除 batch 維度 (變成一維張量)\n",
    "        input_ids_A = encoding_A[\"input_ids\"].squeeze(0)\n",
    "        attention_mask_A = encoding_A[\"attention_mask\"].squeeze(0)\n",
    "        input_ids_B = encoding_B[\"input_ids\"].squeeze(0)\n",
    "        attention_mask_B = encoding_B[\"attention_mask\"].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids_A\": input_ids_A,\n",
    "            \"attention_mask_A\": attention_mask_A,\n",
    "            \"input_ids_B\": input_ids_B,\n",
    "            \"attention_mask_B\": attention_mask_B,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_dataloader(df, tokenizer, max_length, batch_size, shuffle=True):\n",
    "    dataset = SICKBERTDataset(df, tokenizer, max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "class BertDNNClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_dim, num_classes, dropout=0.5):\n",
    "        super(BertDNNClassifier, self).__init__()\n",
    "        # 分別為 sentence_A 與 sentence_B 使用獨立的 BERT encoder\n",
    "        self.bert_A = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.bert_B = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 取 BERT 的 pooler_output (768 維) 串接後為 1536 維\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768 * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids_A, attention_mask_A, input_ids_B, attention_mask_B):\n",
    "        outputs_A = self.bert_A(input_ids=input_ids_A, attention_mask=attention_mask_A)\n",
    "        pooled_A = outputs_A.pooler_output  # (batch, 768)\n",
    "        \n",
    "        outputs_B = self.bert_B(input_ids=input_ids_B, attention_mask=attention_mask_B)\n",
    "        pooled_B = outputs_B.pooler_output  # (batch, 768)\n",
    "        \n",
    "        # 串接兩個向量（不相加）\n",
    "        features = torch.cat([pooled_A, pooled_B], dim=1)  # (batch, 1536)\n",
    "        logits = self.fc(features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, num_epochs, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids_A = batch[\"input_ids_A\"].to(device)\n",
    "            attention_mask_A = batch[\"attention_mask_A\"].to(device)\n",
    "            input_ids_B = batch[\"input_ids_B\"].to(device)\n",
    "            attention_mask_B = batch[\"attention_mask_B\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids_A, attention_mask_A, input_ids_B, attention_mask_B)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        model.eval()\n",
    "        test_running_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids_A = batch[\"input_ids_A\"].to(device)\n",
    "                attention_mask_A = batch[\"attention_mask_A\"].to(device)\n",
    "                input_ids_B = batch[\"input_ids_B\"].to(device)\n",
    "                attention_mask_B = batch[\"attention_mask_B\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                \n",
    "                outputs = model(input_ids_A, attention_mask_A, input_ids_B, attention_mask_B)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_running_loss += loss.item() * labels.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                test_correct += (preds == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        \n",
    "        test_loss = test_running_loss / test_total\n",
    "        test_acc = test_correct / test_total\n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f} | Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
    "    \n",
    "    return epoch_list, train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "batch_size = 16       \n",
    "num_epochs = 5         \n",
    "learning_rate = 2e-5   \n",
    "\n",
    "X = df_train.copy()\n",
    "\n",
    "fractions = [1.0, 0.5, 0.25, 0.1]\n",
    "\n",
    "for frac in fractions:\n",
    "    num_samples = int(len(X) * frac)\n",
    "    df_train_subset = X.iloc[:num_samples].reset_index(drop=True)\n",
    "    subset_percentage = int(frac * 100)\n",
    "    df_train_subset.to_csv(f\"SICK_train_subset_{subset_percentage}.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    train_loader = create_dataloader(df_train_subset, tokenizer, max_length, batch_size, shuffle=True)\n",
    "    test_loader = create_dataloader(df_test, tokenizer, max_length, batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\n[比例 {subset_percentage}%] 訓練資料筆數: {len(df_train_subset)}\")\n",
    "    \n",
    "    # 初始化 BERT+DNN 模型\n",
    "    model = BertDNNClassifier(pretrained_model, hidden_dim=256, num_classes=num_classes, dropout=0.5)\n",
    "    model.to(device)\n",
    "    \n",
    "    epoch_list, train_losses, test_losses, train_accs, test_accs = train_and_evaluate(model, train_loader, test_loader, num_epochs, learning_rate)\n",
    "    \n",
    "    # 畫圖：Loss 與 Accuracy 隨 Epoch 變化\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].plot(epoch_list, train_losses, label=\"Train Loss\")\n",
    "    axs[0].plot(epoch_list, test_losses, label=\"Test Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].set_title(f\"Loss vs Epoch (Train Subset {subset_percentage}%)\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(epoch_list, train_accs, label=\"Train Acc\")\n",
    "    axs[1].plot(epoch_list, test_accs, label=\"Test Acc\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_title(f\"Accuracy vs Epoch (Train Subset {subset_percentage}%)\")\n",
    "    axs[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"BertDNN_SICK_{subset_percentage}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 最終在測試集上評估模型\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids_A = batch[\"input_ids_A\"].to(device)\n",
    "            attention_mask_A = batch[\"attention_mask_A\"].to(device)\n",
    "            input_ids_B = batch[\"input_ids_B\"].to(device)\n",
    "            attention_mask_B = batch[\"attention_mask_B\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids_A, attention_mask_A, input_ids_B, attention_mask_B)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    print(f\"分類報告 (Train Subset {subset_percentage}%):\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=le.classes_))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(le.classes_))\n",
    "    plt.xticks(tick_marks, le.classes_, rotation=45)\n",
    "    plt.yticks(tick_marks, le.classes_)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
