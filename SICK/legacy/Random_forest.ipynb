{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始資料形狀: (9840, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 設定隨機種子與裝置\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"SICK.txt\", sep=\"\\t\")\n",
    "# 只保留所需欄位\n",
    "df = df[[\"sentence_A\", \"sentence_B\", \"entailment_label\"]]\n",
    "print(\"原始資料形狀:\", df.shape)\n",
    "\n",
    "# 若需要可先存檔整理後的資料\n",
    "df.to_csv(\"SICK_filtered.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# 載入 BERT tokenizer 與模型（英文資料集，此處以 bert-base-uncased 為例）\n",
    "pretrained_model = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding sentence_A: 100%|██████████| 5/5 [00:24<00:00,  4.97s/it]\n",
      "Encoding sentence_B: 100%|██████████| 5/5 [00:24<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (9840, 1536)\n",
      "Label classes: ['CONTRADICTION' 'ENTAILMENT' 'NEUTRAL']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 將 sentence_A 與 sentence_B 轉成列表\n",
    "sentences_A = df[\"sentence_A\"].tolist()\n",
    "sentences_B = df[\"sentence_B\"].tolist()\n",
    "\n",
    "batch_size_encode = 2048  # 可根據硬體資源調整\n",
    "embeddings_A = []\n",
    "embeddings_B = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 逐批編碼 sentence_A\n",
    "    for i in tqdm(range(0, len(sentences_A), batch_size_encode), desc=\"Encoding sentence_A\"):\n",
    "        batch_texts = sentences_A[i:i+batch_size_encode]\n",
    "        encodings = tokenizer.batch_encode_plus(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encodings[\"input_ids\"].to(device)\n",
    "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # 取 pooler_output (shape: [batch, 768])\n",
    "        embeddings_A.append(outputs.pooler_output.cpu().numpy())\n",
    "        \n",
    "    # 逐批編碼 sentence_B\n",
    "    for i in tqdm(range(0, len(sentences_B), batch_size_encode), desc=\"Encoding sentence_B\"):\n",
    "        batch_texts = sentences_B[i:i+batch_size_encode]\n",
    "        encodings = tokenizer.batch_encode_plus(\n",
    "            batch_texts,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encodings[\"input_ids\"].to(device)\n",
    "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "        outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings_B.append(outputs.pooler_output.cpu().numpy())\n",
    "\n",
    "# 合併所有批次的結果\n",
    "embeddings_A = np.concatenate(embeddings_A, axis=0)\n",
    "embeddings_B = np.concatenate(embeddings_B, axis=0)\n",
    "\n",
    "# 將 sentence_A 與 sentence_B 的表示串接（concatenate，非相加）\n",
    "X_features = np.concatenate([embeddings_A, embeddings_B], axis=1)  # shape: (n_samples, 1536)\n",
    "print(\"Feature matrix shape:\", X_features.shape)\n",
    "\n",
    "# 標籤編碼（entailment_label）\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"entailment_label\"])\n",
    "print(\"Label classes:\", le.classes_)\n",
    "\n",
    "# 存檔 embeddings 與標籤\n",
    "np.save(\"SICK_embeddings.npy\", X_features)\n",
    "np.save(\"SICK_labels.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入先前存好的 embeddings 與標籤\n",
    "X_features = np.load(\"SICK_embeddings.npy\")\n",
    "y = np.load(\"SICK_labels.npy\")\n",
    "print(\"Loaded embeddings shape:\", X_features.shape)\n",
    "\n",
    "# 以 80%/20% 切分訓練與測試集（stratify 依據標籤）\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train shape:\", X_train_full.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# 可選：另存完整訓練與測試集\n",
    "#np.save(\"SICK_X_train_full.npy\", X_train_full)\n",
    "#np.save(\"SICK_X_test.npy\", X_test)\n",
    "#np.save(\"SICK_y_train_full.npy\", y_train_full)\n",
    "#np.save(\"SICK_y_test.npy\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[比例 100%] 訓練資料筆數: 7872\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.57      0.32      0.41       285\n",
      "   ENTAILMENT       0.55      0.20      0.29       564\n",
      "      NEUTRAL       0.60      0.87      0.71      1119\n",
      "\n",
      "     accuracy                           0.60      1968\n",
      "    macro avg       0.58      0.46      0.47      1968\n",
      " weighted avg       0.58      0.60      0.55      1968\n",
      "\n",
      "\n",
      "[比例 50%] 訓練資料筆數: 3936\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.56      0.25      0.34       285\n",
      "   ENTAILMENT       0.51      0.15      0.23       564\n",
      "      NEUTRAL       0.59      0.89      0.71      1119\n",
      "\n",
      "     accuracy                           0.58      1968\n",
      "    macro avg       0.55      0.43      0.43      1968\n",
      " weighted avg       0.56      0.58      0.52      1968\n",
      "\n",
      "\n",
      "[比例 25%] 訓練資料筆數: 1968\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.51      0.19      0.28       285\n",
      "   ENTAILMENT       0.52      0.11      0.19       564\n",
      "      NEUTRAL       0.59      0.91      0.71      1119\n",
      "\n",
      "     accuracy                           0.58      1968\n",
      "    macro avg       0.54      0.41      0.39      1968\n",
      " weighted avg       0.56      0.58      0.50      1968\n",
      "\n",
      "\n",
      "[比例 10%] 訓練資料筆數: 787\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "CONTRADICTION       0.53      0.14      0.23       285\n",
      "   ENTAILMENT       0.47      0.13      0.21       564\n",
      "      NEUTRAL       0.59      0.91      0.71      1119\n",
      "\n",
      "     accuracy                           0.58      1968\n",
      "    macro avg       0.53      0.39      0.38      1968\n",
      " weighted avg       0.55      0.58      0.50      1968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定義要使用的訓練資料比例（100%, 50%, 25%, 10%）\n",
    "fractions = [1.0, 0.5, 0.25, 0.1]\n",
    "final_n_estimators = 200  # 逐步增加樹數\n",
    "\n",
    "for frac in fractions:\n",
    "    num_train_samples = int(X_train_full.shape[0] * frac)\n",
    "    # 取出該比例的子資料集\n",
    "    X_train = X_train_full[:num_train_samples]\n",
    "    y_train = y_train_full[:num_train_samples]\n",
    "    \n",
    "    print(f\"\\n[比例 {int(frac*100)}%] 訓練資料筆數: {num_train_samples}\")\n",
    "    \n",
    "    # 可另存該子資料集\n",
    "    np.save(f\"SICK_X_train_{int(frac*100)}.npy\", X_train)\n",
    "    np.save(f\"SICK_y_train_{int(frac*100)}.npy\", y_train)\n",
    "    \n",
    "    # 使用 warm_start 模式建立 Random Forest\n",
    "    model_rf = RandomForestClassifier(\n",
    "        warm_start=True, \n",
    "        n_estimators=0,   \n",
    "        min_samples_split=5, \n",
    "        min_samples_leaf=2, \n",
    "        max_features=\"sqrt\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    n_estimators_list = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for i in range(1, final_n_estimators + 1):\n",
    "        model_rf.n_estimators = i\n",
    "        model_rf.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_proba = model_rf.predict_proba(X_train)\n",
    "        y_test_proba = model_rf.predict_proba(X_test)\n",
    "        train_loss = log_loss(y_train, y_train_proba)\n",
    "        test_loss = log_loss(y_test, y_test_proba)\n",
    "        \n",
    "        y_train_pred = model_rf.predict(X_train)\n",
    "        y_test_pred = model_rf.predict(X_test)\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        n_estimators_list.append(i)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "    \n",
    "    # 繪製 Loss 與 Accuracy 變化圖\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].plot(n_estimators_list, train_losses, label=\"Train Loss\")\n",
    "    axs[0].plot(n_estimators_list, test_losses, label=\"Test Loss\")\n",
    "    axs[0].set_xlabel(\"Number of Trees\")\n",
    "    axs[0].set_ylabel(\"Log Loss\")\n",
    "    axs[0].set_title(f\"Loss vs Trees (Train Subset {int(frac*100)}%)\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(n_estimators_list, train_accuracies, label=\"Train Accuracy\")\n",
    "    axs[1].plot(n_estimators_list, test_accuracies, label=\"Test Accuracy\")\n",
    "    axs[1].set_xlabel(\"Number of Trees\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].set_title(f\"Accuracy vs Trees (Train Subset {int(frac*100)}%)\")\n",
    "    axs[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"RF_SICK_{int(frac*100)}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    final_pred = model_rf.predict(X_test)\n",
    "    print(classification_report(y_test, final_pred, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
