{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "parent_dir = os.path.join(os.getcwd(), '..', '..')\n",
    "if parent_dir not in sys.path: sys.path.append(parent_dir)\n",
    "from utility.data import get_loader, EmbeddingDataset\n",
    "from model.gan import Generator, Discriminator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from datasets import load_from_disk\n",
    "from model.dnn import DNNClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "latent_dim = 128          \n",
    "condition_dim = 10        \n",
    "gan_epochs = 150            \n",
    "generation_size = 1000\n",
    "\n",
    "full_dataset = load_from_disk('../../data/full_dataset_new', keep_in_memory=True)\n",
    "split_datasets = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "test_dataset = split_datasets['test']\n",
    "\n",
    "\n",
    "train_x_full = np.array(train_dataset['embedding'])\n",
    "train_y_full = np.array(train_dataset['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_dnn(model, train_loader, test_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_y.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}\")\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    return test_acc, all_preds, all_labels\n",
    "\n",
    "\n",
    "train_ds = EmbeddingDataset(train_dataset)\n",
    "test_ds = EmbeddingDataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Real Data Only] Training size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN on real data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.1846, Train Acc=0.2000\n",
      "Epoch 2: Train Loss=0.6889, Train Acc=0.8000\n",
      "Epoch 3: Train Loss=0.4228, Train Acc=0.9000\n",
      "Epoch 4: Train Loss=0.3475, Train Acc=0.9000\n",
      "Epoch 5: Train Loss=0.2187, Train Acc=0.9500\n",
      "Epoch 6: Train Loss=0.2163, Train Acc=0.9000\n",
      "Epoch 7: Train Loss=0.2360, Train Acc=0.9000\n",
      "Epoch 8: Train Loss=0.1576, Train Acc=0.9500\n",
      "Epoch 9: Train Loss=0.0900, Train Acc=0.9500\n",
      "Epoch 10: Train Loss=0.1418, Train Acc=0.9500\n",
      "DNN Test Accuracy (Real Data) for sample size 20: 0.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 1/150] D loss: 0.6878, G loss: 0.7275\n",
      "[GAN Epoch 2/150] D loss: 0.6234, G loss: 0.7286\n",
      "[GAN Epoch 3/150] D loss: 0.5666, G loss: 0.7318\n",
      "[GAN Epoch 4/150] D loss: 0.5127, G loss: 0.7348\n",
      "[GAN Epoch 5/150] D loss: 0.4656, G loss: 0.7320\n",
      "[GAN Epoch 6/150] D loss: 0.4254, G loss: 0.7326\n",
      "[GAN Epoch 7/150] D loss: 0.3972, G loss: 0.7323\n",
      "[GAN Epoch 8/150] D loss: 0.3737, G loss: 0.7486\n",
      "[GAN Epoch 9/150] D loss: 0.3494, G loss: 0.7917\n",
      "[GAN Epoch 10/150] D loss: 0.3242, G loss: 0.8606\n",
      "[GAN Epoch 11/150] D loss: 0.3058, G loss: 0.9455\n",
      "[GAN Epoch 12/150] D loss: 0.2992, G loss: 0.9914\n",
      "[GAN Epoch 13/150] D loss: 0.2966, G loss: 0.9803\n",
      "[GAN Epoch 14/150] D loss: 0.3118, G loss: 0.9586\n",
      "[GAN Epoch 15/150] D loss: 0.3275, G loss: 0.9472\n",
      "[GAN Epoch 16/150] D loss: 0.3379, G loss: 0.9032\n",
      "[GAN Epoch 17/150] D loss: 0.3110, G loss: 1.0349\n",
      "[GAN Epoch 18/150] D loss: 0.2872, G loss: 0.9690\n",
      "[GAN Epoch 19/150] D loss: 0.2456, G loss: 1.4021\n",
      "[GAN Epoch 20/150] D loss: 0.2588, G loss: 0.9828\n",
      "[GAN Epoch 21/150] D loss: 0.1953, G loss: 1.3635\n",
      "[GAN Epoch 22/150] D loss: 0.1964, G loss: 1.5583\n",
      "[GAN Epoch 23/150] D loss: 0.2379, G loss: 1.0527\n",
      "[GAN Epoch 24/150] D loss: 0.2059, G loss: 1.3079\n",
      "[GAN Epoch 25/150] D loss: 0.2117, G loss: 1.5298\n",
      "[GAN Epoch 26/150] D loss: 0.2559, G loss: 1.0444\n",
      "[GAN Epoch 27/150] D loss: 0.1638, G loss: 1.5948\n",
      "[GAN Epoch 28/150] D loss: 0.1519, G loss: 1.7370\n",
      "[GAN Epoch 29/150] D loss: 0.1688, G loss: 1.4427\n",
      "[GAN Epoch 30/150] D loss: 0.1273, G loss: 2.0974\n",
      "[GAN Epoch 31/150] D loss: 0.1238, G loss: 1.7617\n",
      "[GAN Epoch 32/150] D loss: 0.0890, G loss: 2.3000\n",
      "[GAN Epoch 33/150] D loss: 0.0997, G loss: 2.0429\n",
      "[GAN Epoch 34/150] D loss: 0.0919, G loss: 2.4172\n",
      "[GAN Epoch 35/150] D loss: 0.1077, G loss: 2.0108\n",
      "[GAN Epoch 36/150] D loss: 0.1063, G loss: 3.3515\n",
      "[GAN Epoch 37/150] D loss: 0.4220, G loss: 0.7114\n",
      "[GAN Epoch 38/150] D loss: 0.0301, G loss: 4.3135\n",
      "[GAN Epoch 39/150] D loss: 0.0609, G loss: 4.3708\n",
      "[GAN Epoch 40/150] D loss: 0.0975, G loss: 1.9225\n",
      "[GAN Epoch 41/150] D loss: 0.1757, G loss: 1.4455\n",
      "[GAN Epoch 42/150] D loss: 0.1274, G loss: 3.4476\n",
      "[GAN Epoch 43/150] D loss: 0.1303, G loss: 1.7316\n",
      "[GAN Epoch 44/150] D loss: 0.0368, G loss: 2.8900\n",
      "[GAN Epoch 45/150] D loss: 0.0280, G loss: 3.2982\n",
      "[GAN Epoch 46/150] D loss: 0.0459, G loss: 2.9850\n",
      "[GAN Epoch 47/150] D loss: 0.0541, G loss: 2.8330\n",
      "[GAN Epoch 48/150] D loss: 0.0638, G loss: 2.7409\n",
      "[GAN Epoch 49/150] D loss: 0.0577, G loss: 2.8355\n",
      "[GAN Epoch 50/150] D loss: 0.0450, G loss: 3.0959\n",
      "[GAN Epoch 51/150] D loss: 0.0396, G loss: 3.1614\n",
      "[GAN Epoch 52/150] D loss: 0.0365, G loss: 3.2853\n",
      "[GAN Epoch 53/150] D loss: 0.0370, G loss: 3.1526\n",
      "[GAN Epoch 54/150] D loss: 0.0416, G loss: 3.0473\n",
      "[GAN Epoch 55/150] D loss: 0.0438, G loss: 3.1794\n",
      "[GAN Epoch 56/150] D loss: 0.0406, G loss: 3.1293\n",
      "[GAN Epoch 57/150] D loss: 0.0424, G loss: 3.0529\n",
      "[GAN Epoch 58/150] D loss: 0.0402, G loss: 3.3513\n",
      "[GAN Epoch 59/150] D loss: 0.0487, G loss: 2.7343\n",
      "[GAN Epoch 60/150] D loss: 0.0502, G loss: 4.0937\n",
      "[GAN Epoch 61/150] D loss: 0.1440, G loss: 1.6570\n",
      "[GAN Epoch 62/150] D loss: 0.2511, G loss: 7.3667\n",
      "[GAN Epoch 63/150] D loss: 0.3009, G loss: 0.9976\n",
      "[GAN Epoch 64/150] D loss: 0.0028, G loss: 5.6137\n",
      "[GAN Epoch 65/150] D loss: 0.0019, G loss: 6.5378\n",
      "[GAN Epoch 66/150] D loss: 0.0035, G loss: 5.5583\n",
      "[GAN Epoch 67/150] D loss: 0.0205, G loss: 3.6118\n",
      "[GAN Epoch 68/150] D loss: 0.1262, G loss: 1.6566\n",
      "[GAN Epoch 69/150] D loss: 0.1817, G loss: 4.4572\n",
      "[GAN Epoch 70/150] D loss: 0.8329, G loss: 0.2919\n",
      "[GAN Epoch 71/150] D loss: 1.3637, G loss: 7.4900\n",
      "[GAN Epoch 72/150] D loss: 0.5455, G loss: 0.5086\n",
      "[GAN Epoch 73/150] D loss: 0.0978, G loss: 2.0054\n",
      "[GAN Epoch 74/150] D loss: 0.0259, G loss: 3.4406\n",
      "[GAN Epoch 75/150] D loss: 0.0370, G loss: 3.1602\n",
      "[GAN Epoch 76/150] D loss: 0.1061, G loss: 2.0702\n",
      "[GAN Epoch 77/150] D loss: 0.1921, G loss: 1.7519\n",
      "[GAN Epoch 78/150] D loss: 0.3033, G loss: 1.5821\n",
      "[GAN Epoch 79/150] D loss: 0.4420, G loss: 0.6419\n",
      "[GAN Epoch 80/150] D loss: 0.8704, G loss: 4.2646\n",
      "[GAN Epoch 81/150] D loss: 0.9836, G loss: 0.1802\n",
      "[GAN Epoch 82/150] D loss: 0.1082, G loss: 1.8434\n",
      "[GAN Epoch 83/150] D loss: 0.0911, G loss: 3.8062\n",
      "[GAN Epoch 84/150] D loss: 0.0965, G loss: 3.3073\n",
      "[GAN Epoch 85/150] D loss: 0.1471, G loss: 1.6661\n",
      "[GAN Epoch 86/150] D loss: 0.1922, G loss: 1.5029\n",
      "[GAN Epoch 87/150] D loss: 0.2476, G loss: 2.3025\n",
      "[GAN Epoch 88/150] D loss: 0.3694, G loss: 0.7766\n",
      "[GAN Epoch 89/150] D loss: 0.3556, G loss: 3.2579\n",
      "[GAN Epoch 90/150] D loss: 0.3820, G loss: 0.7176\n",
      "[GAN Epoch 91/150] D loss: 0.1102, G loss: 3.0463\n",
      "[GAN Epoch 92/150] D loss: 0.1124, G loss: 3.1040\n",
      "[GAN Epoch 93/150] D loss: 0.1665, G loss: 1.5802\n",
      "[GAN Epoch 94/150] D loss: 0.1387, G loss: 2.2516\n",
      "[GAN Epoch 95/150] D loss: 0.1572, G loss: 2.0909\n",
      "[GAN Epoch 96/150] D loss: 0.1837, G loss: 1.5791\n",
      "[GAN Epoch 97/150] D loss: 0.1725, G loss: 2.9415\n",
      "[GAN Epoch 98/150] D loss: 0.2459, G loss: 1.1203\n",
      "[GAN Epoch 99/150] D loss: 0.2013, G loss: 4.7230\n",
      "[GAN Epoch 100/150] D loss: 0.0612, G loss: 2.5609\n",
      "[GAN Epoch 101/150] D loss: 0.0908, G loss: 2.0227\n",
      "[GAN Epoch 102/150] D loss: 0.0395, G loss: 3.5154\n",
      "[GAN Epoch 103/150] D loss: 0.0520, G loss: 3.7359\n",
      "[GAN Epoch 104/150] D loss: 0.0740, G loss: 2.6443\n",
      "[GAN Epoch 105/150] D loss: 0.0825, G loss: 2.5275\n",
      "[GAN Epoch 106/150] D loss: 0.0925, G loss: 2.7082\n",
      "[GAN Epoch 107/150] D loss: 0.0988, G loss: 2.2775\n",
      "[GAN Epoch 108/150] D loss: 0.0861, G loss: 3.1272\n",
      "[GAN Epoch 109/150] D loss: 0.0895, G loss: 2.1316\n",
      "[GAN Epoch 110/150] D loss: 0.0671, G loss: 3.8097\n",
      "[GAN Epoch 111/150] D loss: 0.0520, G loss: 2.8877\n",
      "[GAN Epoch 112/150] D loss: 0.0540, G loss: 2.8641\n",
      "[GAN Epoch 113/150] D loss: 0.0527, G loss: 3.5509\n",
      "[GAN Epoch 114/150] D loss: 0.0584, G loss: 2.8680\n",
      "[GAN Epoch 115/150] D loss: 0.0533, G loss: 3.3490\n",
      "[GAN Epoch 116/150] D loss: 0.0545, G loss: 3.0701\n",
      "[GAN Epoch 117/150] D loss: 0.0508, G loss: 3.3318\n",
      "[GAN Epoch 118/150] D loss: 0.0448, G loss: 3.2965\n",
      "[GAN Epoch 119/150] D loss: 0.0456, G loss: 3.1612\n",
      "[GAN Epoch 120/150] D loss: 0.0449, G loss: 3.5094\n",
      "[GAN Epoch 121/150] D loss: 0.0528, G loss: 3.1167\n",
      "[GAN Epoch 122/150] D loss: 0.0506, G loss: 4.1166\n",
      "[GAN Epoch 123/150] D loss: 0.0496, G loss: 2.8070\n",
      "[GAN Epoch 124/150] D loss: 0.0394, G loss: 4.3034\n",
      "[GAN Epoch 125/150] D loss: 0.0333, G loss: 3.5458\n",
      "[GAN Epoch 126/150] D loss: 0.0405, G loss: 3.3696\n",
      "[GAN Epoch 127/150] D loss: 0.0434, G loss: 4.4430\n",
      "[GAN Epoch 128/150] D loss: 0.0660, G loss: 2.6247\n",
      "[GAN Epoch 129/150] D loss: 0.1144, G loss: 6.1562\n",
      "[GAN Epoch 130/150] D loss: 0.1977, G loss: 1.4423\n",
      "[GAN Epoch 131/150] D loss: 0.4219, G loss: 10.2796\n",
      "[GAN Epoch 132/150] D loss: 0.0162, G loss: 4.0811\n",
      "[GAN Epoch 133/150] D loss: 0.3024, G loss: 1.1750\n",
      "[GAN Epoch 134/150] D loss: 0.0440, G loss: 9.8204\n",
      "[GAN Epoch 135/150] D loss: 0.2300, G loss: 11.6544\n",
      "[GAN Epoch 136/150] D loss: 0.0064, G loss: 7.2375\n",
      "[GAN Epoch 137/150] D loss: 0.0149, G loss: 4.0249\n",
      "[GAN Epoch 138/150] D loss: 0.1442, G loss: 1.7410\n",
      "[GAN Epoch 139/150] D loss: 0.0236, G loss: 5.5148\n",
      "[GAN Epoch 140/150] D loss: 0.0906, G loss: 5.9306\n",
      "[GAN Epoch 141/150] D loss: 0.0614, G loss: 2.7986\n",
      "[GAN Epoch 142/150] D loss: 0.0870, G loss: 2.4929\n",
      "[GAN Epoch 143/150] D loss: 0.1867, G loss: 4.5566\n",
      "[GAN Epoch 144/150] D loss: 0.5174, G loss: 0.6369\n",
      "[GAN Epoch 145/150] D loss: 1.5913, G loss: 10.7399\n",
      "[GAN Epoch 146/150] D loss: 0.1283, G loss: 5.0028\n",
      "[GAN Epoch 147/150] D loss: 0.3142, G loss: 0.9283\n",
      "[GAN Epoch 148/150] D loss: 0.0467, G loss: 3.9236\n",
      "[GAN Epoch 149/150] D loss: 0.0813, G loss: 4.7775\n",
      "[GAN Epoch 150/150] D loss: 0.0495, G loss: 4.0343\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1343, Train Acc=0.3197\n",
      "Epoch 2: Train Loss=1.1048, Train Acc=0.3433\n",
      "Epoch 3: Train Loss=1.1004, Train Acc=0.3263\n",
      "Epoch 4: Train Loss=1.1000, Train Acc=0.3273\n",
      "Epoch 5: Train Loss=1.0987, Train Acc=0.3377\n",
      "Epoch 6: Train Loss=1.0989, Train Acc=0.3297\n",
      "Epoch 7: Train Loss=1.0984, Train Acc=0.3413\n",
      "Epoch 8: Train Loss=1.0986, Train Acc=0.3370\n",
      "Epoch 9: Train Loss=1.0986, Train Acc=0.3473\n",
      "Epoch 10: Train Loss=1.0984, Train Acc=0.3353\n",
      "DNN Test Accuracy (After Concatenation) for sample size 20: 0.3725\n",
      "\n",
      "[Real Data Only] Training size: 50\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.1297, Train Acc=0.2400\n",
      "Epoch 2: Train Loss=0.7669, Train Acc=0.6800\n",
      "Epoch 3: Train Loss=0.5563, Train Acc=0.8200\n",
      "Epoch 4: Train Loss=0.4561, Train Acc=0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.3756, Train Acc=0.8800\n",
      "Epoch 6: Train Loss=0.3218, Train Acc=0.9000\n",
      "Epoch 7: Train Loss=0.2684, Train Acc=0.9400\n",
      "Epoch 8: Train Loss=0.2860, Train Acc=0.9200\n",
      "Epoch 9: Train Loss=0.2718, Train Acc=0.9000\n",
      "Epoch 10: Train Loss=0.1996, Train Acc=0.9000\n",
      "DNN Test Accuracy (Real Data) for sample size 50: 0.8720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 1/150] D loss: 0.6885, G loss: 0.6932\n",
      "[GAN Epoch 2/150] D loss: 0.6303, G loss: 0.6914\n",
      "[GAN Epoch 3/150] D loss: 0.5793, G loss: 0.6921\n",
      "[GAN Epoch 4/150] D loss: 0.5302, G loss: 0.6926\n",
      "[GAN Epoch 5/150] D loss: 0.4845, G loss: 0.6924\n",
      "[GAN Epoch 6/150] D loss: 0.4448, G loss: 0.6947\n",
      "[GAN Epoch 7/150] D loss: 0.4149, G loss: 0.6983\n",
      "[GAN Epoch 8/150] D loss: 0.3889, G loss: 0.7180\n",
      "[GAN Epoch 9/150] D loss: 0.3624, G loss: 0.7615\n",
      "[GAN Epoch 10/150] D loss: 0.3386, G loss: 0.8229\n",
      "[GAN Epoch 11/150] D loss: 0.3197, G loss: 0.8955\n",
      "[GAN Epoch 12/150] D loss: 0.3140, G loss: 0.9354\n",
      "[GAN Epoch 13/150] D loss: 0.3196, G loss: 0.9241\n",
      "[GAN Epoch 14/150] D loss: 0.3356, G loss: 0.9065\n",
      "[GAN Epoch 15/150] D loss: 0.3495, G loss: 0.8931\n",
      "[GAN Epoch 16/150] D loss: 0.3546, G loss: 0.8697\n",
      "[GAN Epoch 17/150] D loss: 0.3294, G loss: 0.9830\n",
      "[GAN Epoch 18/150] D loss: 0.3010, G loss: 0.9396\n",
      "[GAN Epoch 19/150] D loss: 0.2650, G loss: 1.3450\n",
      "[GAN Epoch 20/150] D loss: 0.3086, G loss: 0.8366\n",
      "[GAN Epoch 21/150] D loss: 0.1936, G loss: 1.3638\n",
      "[GAN Epoch 22/150] D loss: 0.1922, G loss: 1.5871\n",
      "[GAN Epoch 23/150] D loss: 0.2153, G loss: 1.1578\n",
      "[GAN Epoch 24/150] D loss: 0.1810, G loss: 1.3899\n",
      "[GAN Epoch 25/150] D loss: 0.1756, G loss: 1.5862\n",
      "[GAN Epoch 26/150] D loss: 0.2066, G loss: 1.2101\n",
      "[GAN Epoch 27/150] D loss: 0.1789, G loss: 1.5477\n",
      "[GAN Epoch 28/150] D loss: 0.1772, G loss: 1.4657\n",
      "[GAN Epoch 29/150] D loss: 0.1741, G loss: 1.5277\n",
      "[GAN Epoch 30/150] D loss: 0.1506, G loss: 1.7635\n",
      "[GAN Epoch 31/150] D loss: 0.1454, G loss: 1.6127\n",
      "[GAN Epoch 32/150] D loss: 0.1216, G loss: 2.3183\n",
      "[GAN Epoch 33/150] D loss: 0.2102, G loss: 1.2206\n",
      "[GAN Epoch 34/150] D loss: 0.0884, G loss: 3.1099\n",
      "[GAN Epoch 35/150] D loss: 0.0986, G loss: 1.9532\n",
      "[GAN Epoch 36/150] D loss: 0.1091, G loss: 1.9237\n",
      "[GAN Epoch 37/150] D loss: 0.1346, G loss: 2.7737\n",
      "[GAN Epoch 38/150] D loss: 0.3855, G loss: 0.7138\n",
      "[GAN Epoch 39/150] D loss: 0.0797, G loss: 4.2874\n",
      "[GAN Epoch 40/150] D loss: 0.0324, G loss: 3.7783\n",
      "[GAN Epoch 41/150] D loss: 0.0950, G loss: 1.9411\n",
      "[GAN Epoch 42/150] D loss: 0.1298, G loss: 1.6418\n",
      "[GAN Epoch 43/150] D loss: 0.1345, G loss: 3.0438\n",
      "[GAN Epoch 44/150] D loss: 0.1642, G loss: 1.4531\n",
      "[GAN Epoch 45/150] D loss: 0.0325, G loss: 3.4717\n",
      "[GAN Epoch 46/150] D loss: 0.0333, G loss: 3.7715\n",
      "[GAN Epoch 47/150] D loss: 0.0442, G loss: 2.9050\n",
      "[GAN Epoch 48/150] D loss: 0.0716, G loss: 2.2917\n",
      "[GAN Epoch 49/150] D loss: 0.0554, G loss: 3.0782\n",
      "[GAN Epoch 50/150] D loss: 0.0520, G loss: 2.8963\n",
      "[GAN Epoch 51/150] D loss: 0.0564, G loss: 2.6183\n",
      "[GAN Epoch 52/150] D loss: 0.0445, G loss: 3.2094\n",
      "[GAN Epoch 53/150] D loss: 0.0451, G loss: 2.9273\n",
      "[GAN Epoch 54/150] D loss: 0.0449, G loss: 2.9763\n",
      "[GAN Epoch 55/150] D loss: 0.0423, G loss: 3.1841\n",
      "[GAN Epoch 56/150] D loss: 0.0436, G loss: 2.9646\n",
      "[GAN Epoch 57/150] D loss: 0.0437, G loss: 3.1126\n",
      "[GAN Epoch 58/150] D loss: 0.0423, G loss: 3.1400\n",
      "[GAN Epoch 59/150] D loss: 0.0415, G loss: 3.0765\n",
      "[GAN Epoch 60/150] D loss: 0.0396, G loss: 3.2344\n",
      "[GAN Epoch 61/150] D loss: 0.0412, G loss: 3.0475\n",
      "[GAN Epoch 62/150] D loss: 0.0380, G loss: 3.5141\n",
      "[GAN Epoch 63/150] D loss: 0.0488, G loss: 2.6741\n",
      "[GAN Epoch 64/150] D loss: 0.0612, G loss: 4.6295\n",
      "[GAN Epoch 65/150] D loss: 0.2083, G loss: 1.2539\n",
      "[GAN Epoch 66/150] D loss: 0.4644, G loss: 8.5096\n",
      "[GAN Epoch 67/150] D loss: 0.4982, G loss: 0.6155\n",
      "[GAN Epoch 68/150] D loss: 0.0029, G loss: 5.5201\n",
      "[GAN Epoch 69/150] D loss: 0.0029, G loss: 6.1448\n",
      "[GAN Epoch 70/150] D loss: 0.0095, G loss: 4.7803\n",
      "[GAN Epoch 71/150] D loss: 0.1053, G loss: 1.9846\n",
      "[GAN Epoch 72/150] D loss: 0.1872, G loss: 2.1175\n",
      "[GAN Epoch 73/150] D loss: 0.2627, G loss: 1.6986\n",
      "[GAN Epoch 74/150] D loss: 0.1469, G loss: 2.1104\n",
      "[GAN Epoch 75/150] D loss: 0.0811, G loss: 3.3071\n",
      "[GAN Epoch 76/150] D loss: 0.1878, G loss: 1.4297\n",
      "[GAN Epoch 77/150] D loss: 1.0090, G loss: 6.6470\n",
      "[GAN Epoch 78/150] D loss: 2.8750, G loss: 0.0046\n",
      "[GAN Epoch 79/150] D loss: 1.9661, G loss: 0.0255\n",
      "[GAN Epoch 80/150] D loss: 0.1572, G loss: 1.4930\n",
      "[GAN Epoch 81/150] D loss: 0.1656, G loss: 4.0891\n",
      "[GAN Epoch 82/150] D loss: 0.1688, G loss: 3.3354\n",
      "[GAN Epoch 83/150] D loss: 0.1793, G loss: 1.5542\n",
      "[GAN Epoch 84/150] D loss: 0.2512, G loss: 1.1525\n",
      "[GAN Epoch 85/150] D loss: 0.2015, G loss: 2.0545\n",
      "[GAN Epoch 86/150] D loss: 0.1924, G loss: 2.0211\n",
      "[GAN Epoch 87/150] D loss: 0.2035, G loss: 1.5104\n",
      "[GAN Epoch 88/150] D loss: 0.1622, G loss: 2.2360\n",
      "[GAN Epoch 89/150] D loss: 0.1602, G loss: 1.9515\n",
      "[GAN Epoch 90/150] D loss: 0.1659, G loss: 1.8940\n",
      "[GAN Epoch 91/150] D loss: 0.1883, G loss: 2.0168\n",
      "[GAN Epoch 92/150] D loss: 0.2368, G loss: 1.4671\n",
      "[GAN Epoch 93/150] D loss: 0.3026, G loss: 2.6482\n",
      "[GAN Epoch 94/150] D loss: 0.9547, G loss: 0.2060\n",
      "[GAN Epoch 95/150] D loss: 0.6133, G loss: 4.4603\n",
      "[GAN Epoch 96/150] D loss: 0.2037, G loss: 1.3938\n",
      "[GAN Epoch 97/150] D loss: 0.1457, G loss: 1.7213\n",
      "[GAN Epoch 98/150] D loss: 0.1135, G loss: 3.0040\n",
      "[GAN Epoch 99/150] D loss: 0.1116, G loss: 2.6764\n",
      "[GAN Epoch 100/150] D loss: 0.1428, G loss: 1.8733\n",
      "[GAN Epoch 101/150] D loss: 0.1244, G loss: 2.5991\n",
      "[GAN Epoch 102/150] D loss: 0.1235, G loss: 2.4344\n",
      "[GAN Epoch 103/150] D loss: 0.1285, G loss: 2.2069\n",
      "[GAN Epoch 104/150] D loss: 0.1121, G loss: 2.9235\n",
      "[GAN Epoch 105/150] D loss: 0.1228, G loss: 2.0815\n",
      "[GAN Epoch 106/150] D loss: 0.1072, G loss: 3.3042\n",
      "[GAN Epoch 107/150] D loss: 0.1043, G loss: 2.2019\n",
      "[GAN Epoch 108/150] D loss: 0.0883, G loss: 3.2099\n",
      "[GAN Epoch 109/150] D loss: 0.0988, G loss: 2.3165\n",
      "[GAN Epoch 110/150] D loss: 0.0944, G loss: 3.2565\n",
      "[GAN Epoch 111/150] D loss: 0.1259, G loss: 1.9034\n",
      "[GAN Epoch 112/150] D loss: 0.1605, G loss: 4.3452\n",
      "[GAN Epoch 113/150] D loss: 0.2227, G loss: 1.2353\n",
      "[GAN Epoch 114/150] D loss: 0.2171, G loss: 6.0362\n",
      "[GAN Epoch 115/150] D loss: 0.0325, G loss: 3.8592\n",
      "[GAN Epoch 116/150] D loss: 0.1112, G loss: 1.9315\n",
      "[GAN Epoch 117/150] D loss: 0.0372, G loss: 3.9856\n",
      "[GAN Epoch 118/150] D loss: 0.0566, G loss: 3.9291\n",
      "[GAN Epoch 119/150] D loss: 0.0807, G loss: 2.4976\n",
      "[GAN Epoch 120/150] D loss: 0.0868, G loss: 2.7845\n",
      "[GAN Epoch 121/150] D loss: 0.0988, G loss: 2.7037\n",
      "[GAN Epoch 122/150] D loss: 0.0982, G loss: 2.4630\n",
      "[GAN Epoch 123/150] D loss: 0.0895, G loss: 3.2315\n",
      "[GAN Epoch 124/150] D loss: 0.0918, G loss: 2.3992\n",
      "[GAN Epoch 125/150] D loss: 0.0817, G loss: 4.1002\n",
      "[GAN Epoch 126/150] D loss: 0.0938, G loss: 2.2792\n",
      "[GAN Epoch 127/150] D loss: 0.0836, G loss: 4.5936\n",
      "[GAN Epoch 128/150] D loss: 0.0657, G loss: 2.6856\n",
      "[GAN Epoch 129/150] D loss: 0.0537, G loss: 3.4628\n",
      "[GAN Epoch 130/150] D loss: 0.0626, G loss: 3.3890\n",
      "[GAN Epoch 131/150] D loss: 0.1040, G loss: 2.3833\n",
      "[GAN Epoch 132/150] D loss: 0.2290, G loss: 5.0841\n",
      "[GAN Epoch 133/150] D loss: 0.9401, G loss: 0.2738\n",
      "[GAN Epoch 134/150] D loss: 1.1874, G loss: 11.8008\n",
      "[GAN Epoch 135/150] D loss: 0.0767, G loss: 7.2014\n",
      "[GAN Epoch 136/150] D loss: 0.0370, G loss: 3.3452\n",
      "[GAN Epoch 137/150] D loss: 0.1991, G loss: 1.5521\n",
      "[GAN Epoch 138/150] D loss: 0.0389, G loss: 3.5957\n",
      "[GAN Epoch 139/150] D loss: 0.0694, G loss: 4.1958\n",
      "[GAN Epoch 140/150] D loss: 0.0923, G loss: 3.1178\n",
      "[GAN Epoch 141/150] D loss: 0.2156, G loss: 1.7470\n",
      "[GAN Epoch 142/150] D loss: 0.2154, G loss: 3.4409\n",
      "[GAN Epoch 143/150] D loss: 0.2387, G loss: 2.0143\n",
      "[GAN Epoch 144/150] D loss: 0.2126, G loss: 2.6908\n",
      "[GAN Epoch 145/150] D loss: 0.2229, G loss: 2.1737\n",
      "[GAN Epoch 146/150] D loss: 0.2019, G loss: 3.3527\n",
      "[GAN Epoch 147/150] D loss: 0.3008, G loss: 1.3516\n",
      "[GAN Epoch 148/150] D loss: 0.4411, G loss: 5.7500\n",
      "[GAN Epoch 149/150] D loss: 0.1412, G loss: 2.4186\n",
      "[GAN Epoch 150/150] D loss: 0.1902, G loss: 1.6274\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1559, Train Acc=0.3163\n",
      "Epoch 2: Train Loss=1.1144, Train Acc=0.3220\n",
      "Epoch 3: Train Loss=1.1007, Train Acc=0.3380\n",
      "Epoch 4: Train Loss=1.0995, Train Acc=0.3273\n",
      "Epoch 5: Train Loss=1.0989, Train Acc=0.3353\n",
      "Epoch 6: Train Loss=1.0989, Train Acc=0.3387\n",
      "Epoch 7: Train Loss=1.0990, Train Acc=0.3270\n",
      "Epoch 8: Train Loss=1.0989, Train Acc=0.3370\n",
      "Epoch 9: Train Loss=1.0983, Train Acc=0.3357\n",
      "Epoch 10: Train Loss=1.0977, Train Acc=0.3413\n",
      "DNN Test Accuracy (After Concatenation) for sample size 50: 0.4304\n",
      "\n",
      "[Real Data Only] Training size: 70\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.1748, Train Acc=0.2000\n",
      "Epoch 2: Train Loss=0.7077, Train Acc=0.7571\n",
      "Epoch 3: Train Loss=0.5153, Train Acc=0.8143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.3903, Train Acc=0.8714\n",
      "Epoch 5: Train Loss=0.3933, Train Acc=0.8857\n",
      "Epoch 6: Train Loss=0.3624, Train Acc=0.8714\n",
      "Epoch 7: Train Loss=0.2771, Train Acc=0.8857\n",
      "Epoch 8: Train Loss=0.3017, Train Acc=0.8857\n",
      "Epoch 9: Train Loss=0.2941, Train Acc=0.8857\n",
      "Epoch 10: Train Loss=0.2921, Train Acc=0.9000\n",
      "DNN Test Accuracy (Real Data) for sample size 70: 0.8831\n",
      "[GAN Epoch 1/150] D loss: 0.6880, G loss: 0.6666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 2/150] D loss: 0.6292, G loss: 0.6670\n",
      "[GAN Epoch 3/150] D loss: 0.5775, G loss: 0.6692\n",
      "[GAN Epoch 4/150] D loss: 0.5283, G loss: 0.6705\n",
      "[GAN Epoch 5/150] D loss: 0.4818, G loss: 0.6725\n",
      "[GAN Epoch 6/150] D loss: 0.4441, G loss: 0.6728\n",
      "[GAN Epoch 7/150] D loss: 0.4160, G loss: 0.6777\n",
      "[GAN Epoch 8/150] D loss: 0.3949, G loss: 0.6923\n",
      "[GAN Epoch 9/150] D loss: 0.3723, G loss: 0.7314\n",
      "[GAN Epoch 10/150] D loss: 0.3497, G loss: 0.7922\n",
      "[GAN Epoch 11/150] D loss: 0.3297, G loss: 0.8699\n",
      "[GAN Epoch 12/150] D loss: 0.3173, G loss: 0.9157\n",
      "[GAN Epoch 13/150] D loss: 0.3147, G loss: 0.9188\n",
      "[GAN Epoch 14/150] D loss: 0.3225, G loss: 0.9281\n",
      "[GAN Epoch 15/150] D loss: 0.3364, G loss: 0.9049\n",
      "[GAN Epoch 16/150] D loss: 0.3482, G loss: 0.8772\n",
      "[GAN Epoch 17/150] D loss: 0.3378, G loss: 0.9298\n",
      "[GAN Epoch 18/150] D loss: 0.3080, G loss: 0.9399\n",
      "[GAN Epoch 19/150] D loss: 0.2704, G loss: 1.1706\n",
      "[GAN Epoch 20/150] D loss: 0.2591, G loss: 1.0171\n",
      "[GAN Epoch 21/150] D loss: 0.2058, G loss: 1.3904\n",
      "[GAN Epoch 22/150] D loss: 0.1960, G loss: 1.3004\n",
      "[GAN Epoch 23/150] D loss: 0.1925, G loss: 1.3691\n",
      "[GAN Epoch 24/150] D loss: 0.1842, G loss: 1.4533\n",
      "[GAN Epoch 25/150] D loss: 0.1852, G loss: 1.3921\n",
      "[GAN Epoch 26/150] D loss: 0.1583, G loss: 1.6245\n",
      "[GAN Epoch 27/150] D loss: 0.1559, G loss: 1.4907\n",
      "[GAN Epoch 28/150] D loss: 0.1343, G loss: 1.8880\n",
      "[GAN Epoch 29/150] D loss: 0.1492, G loss: 1.5543\n",
      "[GAN Epoch 30/150] D loss: 0.1187, G loss: 2.2970\n",
      "[GAN Epoch 31/150] D loss: 0.1750, G loss: 1.3615\n",
      "[GAN Epoch 32/150] D loss: 0.0915, G loss: 2.6596\n",
      "[GAN Epoch 33/150] D loss: 0.1260, G loss: 1.8009\n",
      "[GAN Epoch 34/150] D loss: 0.1151, G loss: 2.3151\n",
      "[GAN Epoch 35/150] D loss: 0.1352, G loss: 1.7659\n",
      "[GAN Epoch 36/150] D loss: 0.1326, G loss: 2.7883\n",
      "[GAN Epoch 37/150] D loss: 0.4725, G loss: 0.6838\n",
      "[GAN Epoch 38/150] D loss: 0.0956, G loss: 4.2629\n",
      "[GAN Epoch 39/150] D loss: 0.0323, G loss: 3.4349\n",
      "[GAN Epoch 40/150] D loss: 0.1866, G loss: 1.4210\n",
      "[GAN Epoch 41/150] D loss: 0.0876, G loss: 2.4656\n",
      "[GAN Epoch 42/150] D loss: 0.1155, G loss: 2.3065\n",
      "[GAN Epoch 43/150] D loss: 0.1443, G loss: 1.5847\n",
      "[GAN Epoch 44/150] D loss: 0.0683, G loss: 3.6339\n",
      "[GAN Epoch 45/150] D loss: 0.0369, G loss: 2.9906\n",
      "[GAN Epoch 46/150] D loss: 0.0593, G loss: 2.4016\n",
      "[GAN Epoch 47/150] D loss: 0.0318, G loss: 3.3128\n",
      "[GAN Epoch 48/150] D loss: 0.0400, G loss: 3.2949\n",
      "[GAN Epoch 49/150] D loss: 0.0633, G loss: 2.5531\n",
      "[GAN Epoch 50/150] D loss: 0.0522, G loss: 3.3720\n",
      "[GAN Epoch 51/150] D loss: 0.0642, G loss: 2.4159\n",
      "[GAN Epoch 52/150] D loss: 0.0442, G loss: 3.8677\n",
      "[GAN Epoch 53/150] D loss: 0.0570, G loss: 2.5892\n",
      "[GAN Epoch 54/150] D loss: 0.0396, G loss: 3.7281\n",
      "[GAN Epoch 55/150] D loss: 0.0588, G loss: 2.7351\n",
      "[GAN Epoch 56/150] D loss: 0.0557, G loss: 3.7610\n",
      "[GAN Epoch 57/150] D loss: 0.1300, G loss: 1.7442\n",
      "[GAN Epoch 58/150] D loss: 0.2643, G loss: 6.1495\n",
      "[GAN Epoch 59/150] D loss: 0.3990, G loss: 0.7486\n",
      "[GAN Epoch 60/150] D loss: 0.0020, G loss: 6.2374\n",
      "[GAN Epoch 61/150] D loss: 0.0042, G loss: 7.5536\n",
      "[GAN Epoch 62/150] D loss: 0.0091, G loss: 7.0445\n",
      "[GAN Epoch 63/150] D loss: 0.0108, G loss: 5.6794\n",
      "[GAN Epoch 64/150] D loss: 0.0165, G loss: 4.0479\n",
      "[GAN Epoch 65/150] D loss: 0.0699, G loss: 2.3857\n",
      "[GAN Epoch 66/150] D loss: 0.0684, G loss: 2.6233\n",
      "[GAN Epoch 67/150] D loss: 0.0998, G loss: 3.0373\n",
      "[GAN Epoch 68/150] D loss: 0.1985, G loss: 1.3228\n",
      "[GAN Epoch 69/150] D loss: 0.5797, G loss: 5.0881\n",
      "[GAN Epoch 70/150] D loss: 2.0669, G loss: 0.0221\n",
      "[GAN Epoch 71/150] D loss: 0.2730, G loss: 1.0113\n",
      "[GAN Epoch 72/150] D loss: 0.1903, G loss: 5.2235\n",
      "[GAN Epoch 73/150] D loss: 0.1002, G loss: 4.5211\n",
      "[GAN Epoch 74/150] D loss: 0.0653, G loss: 2.5077\n",
      "[GAN Epoch 75/150] D loss: 0.1923, G loss: 1.3430\n",
      "[GAN Epoch 76/150] D loss: 0.1093, G loss: 2.6538\n",
      "[GAN Epoch 77/150] D loss: 0.1364, G loss: 2.6830\n",
      "[GAN Epoch 78/150] D loss: 0.2153, G loss: 1.3798\n",
      "[GAN Epoch 79/150] D loss: 0.2299, G loss: 2.8448\n",
      "[GAN Epoch 80/150] D loss: 0.5075, G loss: 0.5943\n",
      "[GAN Epoch 81/150] D loss: 0.8889, G loss: 4.2920\n",
      "[GAN Epoch 82/150] D loss: 1.1706, G loss: 0.1271\n",
      "[GAN Epoch 83/150] D loss: 0.2839, G loss: 1.0171\n",
      "[GAN Epoch 84/150] D loss: 0.3936, G loss: 3.4992\n",
      "[GAN Epoch 85/150] D loss: 0.1707, G loss: 2.0688\n",
      "[GAN Epoch 86/150] D loss: 0.2808, G loss: 1.0545\n",
      "[GAN Epoch 87/150] D loss: 0.1877, G loss: 1.9114\n",
      "[GAN Epoch 88/150] D loss: 0.2107, G loss: 2.1489\n",
      "[GAN Epoch 89/150] D loss: 0.2472, G loss: 1.3591\n",
      "[GAN Epoch 90/150] D loss: 0.2503, G loss: 1.6738\n",
      "[GAN Epoch 91/150] D loss: 0.2625, G loss: 1.6275\n",
      "[GAN Epoch 92/150] D loss: 0.2695, G loss: 1.4005\n",
      "[GAN Epoch 93/150] D loss: 0.2520, G loss: 1.9627\n",
      "[GAN Epoch 94/150] D loss: 0.2638, G loss: 1.2026\n",
      "[GAN Epoch 95/150] D loss: 0.2557, G loss: 3.0888\n",
      "[GAN Epoch 96/150] D loss: 0.2655, G loss: 1.0418\n",
      "[GAN Epoch 97/150] D loss: 0.1507, G loss: 3.5531\n",
      "[GAN Epoch 98/150] D loss: 0.0840, G loss: 2.7304\n",
      "[GAN Epoch 99/150] D loss: 0.1278, G loss: 1.7880\n",
      "[GAN Epoch 100/150] D loss: 0.0979, G loss: 2.8050\n",
      "[GAN Epoch 101/150] D loss: 0.1105, G loss: 2.4373\n",
      "[GAN Epoch 102/150] D loss: 0.1417, G loss: 1.8561\n",
      "[GAN Epoch 103/150] D loss: 0.1464, G loss: 2.8460\n",
      "[GAN Epoch 104/150] D loss: 0.1826, G loss: 1.4898\n",
      "[GAN Epoch 105/150] D loss: 0.1942, G loss: 3.9334\n",
      "[GAN Epoch 106/150] D loss: 0.1403, G loss: 1.6494\n",
      "[GAN Epoch 107/150] D loss: 0.0641, G loss: 3.3120\n",
      "[GAN Epoch 108/150] D loss: 0.0644, G loss: 3.1961\n",
      "[GAN Epoch 109/150] D loss: 0.0920, G loss: 2.1901\n",
      "[GAN Epoch 110/150] D loss: 0.0892, G loss: 2.8551\n",
      "[GAN Epoch 111/150] D loss: 0.1109, G loss: 2.2172\n",
      "[GAN Epoch 112/150] D loss: 0.1137, G loss: 2.6755\n",
      "[GAN Epoch 113/150] D loss: 0.1139, G loss: 2.1502\n",
      "[GAN Epoch 114/150] D loss: 0.1003, G loss: 3.2639\n",
      "[GAN Epoch 115/150] D loss: 0.1168, G loss: 1.9202\n",
      "[GAN Epoch 116/150] D loss: 0.1049, G loss: 4.1757\n",
      "[GAN Epoch 117/150] D loss: 0.0561, G loss: 2.7346\n",
      "[GAN Epoch 118/150] D loss: 0.0615, G loss: 2.5244\n",
      "[GAN Epoch 119/150] D loss: 0.0517, G loss: 3.5253\n",
      "[GAN Epoch 120/150] D loss: 0.0549, G loss: 3.0656\n",
      "[GAN Epoch 121/150] D loss: 0.0712, G loss: 2.4942\n",
      "[GAN Epoch 122/150] D loss: 0.0692, G loss: 3.4027\n",
      "[GAN Epoch 123/150] D loss: 0.0700, G loss: 2.5481\n",
      "[GAN Epoch 124/150] D loss: 0.0619, G loss: 3.1642\n",
      "[GAN Epoch 125/150] D loss: 0.0604, G loss: 2.9276\n",
      "[GAN Epoch 126/150] D loss: 0.0560, G loss: 3.0617\n",
      "[GAN Epoch 127/150] D loss: 0.0547, G loss: 3.0474\n",
      "[GAN Epoch 128/150] D loss: 0.0557, G loss: 3.0584\n",
      "[GAN Epoch 129/150] D loss: 0.0537, G loss: 3.2605\n",
      "[GAN Epoch 130/150] D loss: 0.0523, G loss: 3.0399\n",
      "[GAN Epoch 131/150] D loss: 0.0482, G loss: 3.3540\n",
      "[GAN Epoch 132/150] D loss: 0.0499, G loss: 3.0645\n",
      "[GAN Epoch 133/150] D loss: 0.0462, G loss: 3.5448\n",
      "[GAN Epoch 134/150] D loss: 0.0468, G loss: 3.1059\n",
      "[GAN Epoch 135/150] D loss: 0.0442, G loss: 3.6440\n",
      "[GAN Epoch 136/150] D loss: 0.0460, G loss: 3.1400\n",
      "[GAN Epoch 137/150] D loss: 0.0446, G loss: 3.6789\n",
      "[GAN Epoch 138/150] D loss: 0.0591, G loss: 2.7542\n",
      "[GAN Epoch 139/150] D loss: 0.0784, G loss: 4.8064\n",
      "[GAN Epoch 140/150] D loss: 0.1539, G loss: 1.6181\n",
      "[GAN Epoch 141/150] D loss: 0.5948, G loss: 9.2916\n",
      "[GAN Epoch 142/150] D loss: 0.0815, G loss: 2.2123\n",
      "[GAN Epoch 143/150] D loss: 0.0510, G loss: 2.7313\n",
      "[GAN Epoch 144/150] D loss: 0.0084, G loss: 5.1163\n",
      "[GAN Epoch 145/150] D loss: 0.0156, G loss: 5.3391\n",
      "[GAN Epoch 146/150] D loss: 0.0225, G loss: 4.3362\n",
      "[GAN Epoch 147/150] D loss: 0.0590, G loss: 2.7552\n",
      "[GAN Epoch 148/150] D loss: 0.0738, G loss: 3.6071\n",
      "[GAN Epoch 149/150] D loss: 0.1415, G loss: 2.2185\n",
      "[GAN Epoch 150/150] D loss: 0.2485, G loss: 4.6338\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1424, Train Acc=0.3250\n",
      "Epoch 2: Train Loss=1.1044, Train Acc=0.3313\n",
      "Epoch 3: Train Loss=1.1033, Train Acc=0.3307\n",
      "Epoch 4: Train Loss=1.1009, Train Acc=0.3263\n",
      "Epoch 5: Train Loss=1.0999, Train Acc=0.3333\n",
      "Epoch 6: Train Loss=1.0997, Train Acc=0.3287\n",
      "Epoch 7: Train Loss=1.1003, Train Acc=0.3263\n",
      "Epoch 8: Train Loss=1.0991, Train Acc=0.3323\n",
      "Epoch 9: Train Loss=1.0992, Train Acc=0.3347\n",
      "Epoch 10: Train Loss=1.0997, Train Acc=0.3210\n",
      "DNN Test Accuracy (After Concatenation) for sample size 70: 0.3471\n",
      "\n",
      "[Real Data Only] Training size: 100\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.0251, Train Acc=0.5100\n",
      "Epoch 2: Train Loss=0.6682, Train Acc=0.8200\n",
      "Epoch 3: Train Loss=0.4993, Train Acc=0.8500\n",
      "Epoch 4: Train Loss=0.3845, Train Acc=0.8700\n",
      "Epoch 5: Train Loss=0.3369, Train Acc=0.9000\n",
      "Epoch 6: Train Loss=0.3086, Train Acc=0.9000\n",
      "Epoch 7: Train Loss=0.3012, Train Acc=0.9100\n",
      "Epoch 8: Train Loss=0.3137, Train Acc=0.9200\n",
      "Epoch 9: Train Loss=0.3016, Train Acc=0.8800\n",
      "Epoch 10: Train Loss=0.3019, Train Acc=0.8800\n",
      "DNN Test Accuracy (Real Data) for sample size 100: 0.8918\n",
      "[GAN Epoch 1/150] D loss: 0.6933, G loss: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 2/150] D loss: 0.6364, G loss: 0.6660\n",
      "[GAN Epoch 3/150] D loss: 0.5870, G loss: 0.6670\n",
      "[GAN Epoch 4/150] D loss: 0.5402, G loss: 0.6674\n",
      "[GAN Epoch 5/150] D loss: 0.4970, G loss: 0.6668\n",
      "[GAN Epoch 6/150] D loss: 0.4599, G loss: 0.6667\n",
      "[GAN Epoch 7/150] D loss: 0.4305, G loss: 0.6712\n",
      "[GAN Epoch 8/150] D loss: 0.4043, G loss: 0.6911\n",
      "[GAN Epoch 9/150] D loss: 0.3780, G loss: 0.7323\n",
      "[GAN Epoch 10/150] D loss: 0.3511, G loss: 0.7972\n",
      "[GAN Epoch 11/150] D loss: 0.3285, G loss: 0.8774\n",
      "[GAN Epoch 12/150] D loss: 0.3138, G loss: 0.9440\n",
      "[GAN Epoch 13/150] D loss: 0.3095, G loss: 0.9583\n",
      "[GAN Epoch 14/150] D loss: 0.3238, G loss: 0.9350\n",
      "[GAN Epoch 15/150] D loss: 0.3459, G loss: 0.9372\n",
      "[GAN Epoch 16/150] D loss: 0.3705, G loss: 0.8391\n",
      "[GAN Epoch 17/150] D loss: 0.3708, G loss: 0.9577\n",
      "[GAN Epoch 18/150] D loss: 0.3644, G loss: 0.7847\n",
      "[GAN Epoch 19/150] D loss: 0.3144, G loss: 1.1640\n",
      "[GAN Epoch 20/150] D loss: 0.3022, G loss: 0.9066\n",
      "[GAN Epoch 21/150] D loss: 0.2421, G loss: 1.1847\n",
      "[GAN Epoch 22/150] D loss: 0.2196, G loss: 1.3370\n",
      "[GAN Epoch 23/150] D loss: 0.2113, G loss: 1.2344\n",
      "[GAN Epoch 24/150] D loss: 0.1995, G loss: 1.3195\n",
      "[GAN Epoch 25/150] D loss: 0.1943, G loss: 1.4116\n",
      "[GAN Epoch 26/150] D loss: 0.1989, G loss: 1.3557\n",
      "[GAN Epoch 27/150] D loss: 0.1928, G loss: 1.4238\n",
      "[GAN Epoch 28/150] D loss: 0.1822, G loss: 1.5038\n",
      "[GAN Epoch 29/150] D loss: 0.1792, G loss: 1.4680\n",
      "[GAN Epoch 30/150] D loss: 0.1724, G loss: 1.6835\n",
      "[GAN Epoch 31/150] D loss: 0.1701, G loss: 1.4955\n",
      "[GAN Epoch 32/150] D loss: 0.1581, G loss: 2.1981\n",
      "[GAN Epoch 33/150] D loss: 0.3739, G loss: 0.7351\n",
      "[GAN Epoch 34/150] D loss: 0.0962, G loss: 2.8459\n",
      "[GAN Epoch 35/150] D loss: 0.0997, G loss: 2.1830\n",
      "[GAN Epoch 36/150] D loss: 0.2444, G loss: 1.1352\n",
      "[GAN Epoch 37/150] D loss: 0.2335, G loss: 2.4893\n",
      "[GAN Epoch 38/150] D loss: 0.4602, G loss: 0.5960\n",
      "[GAN Epoch 39/150] D loss: 0.0582, G loss: 2.9056\n",
      "[GAN Epoch 40/150] D loss: 0.0801, G loss: 3.0833\n",
      "[GAN Epoch 41/150] D loss: 0.1420, G loss: 1.7211\n",
      "[GAN Epoch 42/150] D loss: 0.1694, G loss: 1.5337\n",
      "[GAN Epoch 43/150] D loss: 0.1475, G loss: 2.4045\n",
      "[GAN Epoch 44/150] D loss: 0.1246, G loss: 1.8003\n",
      "[GAN Epoch 45/150] D loss: 0.0745, G loss: 2.5118\n",
      "[GAN Epoch 46/150] D loss: 0.0663, G loss: 2.7556\n",
      "[GAN Epoch 47/150] D loss: 0.0780, G loss: 2.3676\n",
      "[GAN Epoch 48/150] D loss: 0.0744, G loss: 2.5320\n",
      "[GAN Epoch 49/150] D loss: 0.0682, G loss: 2.7282\n",
      "[GAN Epoch 50/150] D loss: 0.0673, G loss: 2.5563\n",
      "[GAN Epoch 51/150] D loss: 0.0605, G loss: 2.7705\n",
      "[GAN Epoch 52/150] D loss: 0.0620, G loss: 2.7483\n",
      "[GAN Epoch 53/150] D loss: 0.0626, G loss: 2.7040\n",
      "[GAN Epoch 54/150] D loss: 0.0667, G loss: 2.6862\n",
      "[GAN Epoch 55/150] D loss: 0.0671, G loss: 2.6881\n",
      "[GAN Epoch 56/150] D loss: 0.0696, G loss: 2.6192\n",
      "[GAN Epoch 57/150] D loss: 0.0630, G loss: 2.8995\n",
      "[GAN Epoch 58/150] D loss: 0.0663, G loss: 2.5092\n",
      "[GAN Epoch 59/150] D loss: 0.0599, G loss: 3.5437\n",
      "[GAN Epoch 60/150] D loss: 0.0921, G loss: 2.0328\n",
      "[GAN Epoch 61/150] D loss: 0.0627, G loss: 4.5695\n",
      "[GAN Epoch 62/150] D loss: 0.0398, G loss: 2.8140\n",
      "[GAN Epoch 63/150] D loss: 0.0547, G loss: 2.5118\n",
      "[GAN Epoch 64/150] D loss: 0.0526, G loss: 3.7939\n",
      "[GAN Epoch 65/150] D loss: 0.1126, G loss: 1.8665\n",
      "[GAN Epoch 66/150] D loss: 0.3538, G loss: 4.3982\n",
      "[GAN Epoch 67/150] D loss: 2.4709, G loss: 0.0091\n",
      "[GAN Epoch 68/150] D loss: 0.3049, G loss: 0.9175\n",
      "[GAN Epoch 69/150] D loss: 0.6474, G loss: 6.4544\n",
      "[GAN Epoch 70/150] D loss: 0.0588, G loss: 2.5423\n",
      "[GAN Epoch 71/150] D loss: 0.4684, G loss: 0.5625\n",
      "[GAN Epoch 72/150] D loss: 0.0700, G loss: 2.9722\n",
      "[GAN Epoch 73/150] D loss: 0.1350, G loss: 3.6882\n",
      "[GAN Epoch 74/150] D loss: 0.1237, G loss: 2.3388\n",
      "[GAN Epoch 75/150] D loss: 0.2660, G loss: 1.1363\n",
      "[GAN Epoch 76/150] D loss: 0.3223, G loss: 2.6800\n",
      "[GAN Epoch 77/150] D loss: 0.7046, G loss: 0.3497\n",
      "[GAN Epoch 78/150] D loss: 0.7961, G loss: 3.3364\n",
      "[GAN Epoch 79/150] D loss: 1.0611, G loss: 0.1452\n",
      "[GAN Epoch 80/150] D loss: 0.4087, G loss: 0.7523\n",
      "[GAN Epoch 81/150] D loss: 0.5086, G loss: 3.0701\n",
      "[GAN Epoch 82/150] D loss: 0.2308, G loss: 1.5536\n",
      "[GAN Epoch 83/150] D loss: 0.3031, G loss: 0.9758\n",
      "[GAN Epoch 84/150] D loss: 0.2059, G loss: 1.8706\n",
      "[GAN Epoch 85/150] D loss: 0.2142, G loss: 2.0313\n",
      "[GAN Epoch 86/150] D loss: 0.2387, G loss: 1.3687\n",
      "[GAN Epoch 87/150] D loss: 0.2347, G loss: 1.6157\n",
      "[GAN Epoch 88/150] D loss: 0.2584, G loss: 1.6262\n",
      "[GAN Epoch 89/150] D loss: 0.2883, G loss: 1.2857\n",
      "[GAN Epoch 90/150] D loss: 0.2838, G loss: 1.9101\n",
      "[GAN Epoch 91/150] D loss: 0.3543, G loss: 0.8820\n",
      "[GAN Epoch 92/150] D loss: 0.4128, G loss: 3.2919\n",
      "[GAN Epoch 93/150] D loss: 0.4425, G loss: 0.6076\n",
      "[GAN Epoch 94/150] D loss: 0.1363, G loss: 3.1309\n",
      "[GAN Epoch 95/150] D loss: 0.1216, G loss: 3.0967\n",
      "[GAN Epoch 96/150] D loss: 0.1353, G loss: 1.7932\n",
      "[GAN Epoch 97/150] D loss: 0.1459, G loss: 1.8260\n",
      "[GAN Epoch 98/150] D loss: 0.1582, G loss: 2.5223\n",
      "[GAN Epoch 99/150] D loss: 0.1860, G loss: 1.6108\n",
      "[GAN Epoch 100/150] D loss: 0.1658, G loss: 2.3491\n",
      "[GAN Epoch 101/150] D loss: 0.1677, G loss: 1.6848\n",
      "[GAN Epoch 102/150] D loss: 0.1428, G loss: 2.7060\n",
      "[GAN Epoch 103/150] D loss: 0.1432, G loss: 1.7521\n",
      "[GAN Epoch 104/150] D loss: 0.1150, G loss: 3.0224\n",
      "[GAN Epoch 105/150] D loss: 0.1141, G loss: 2.0146\n",
      "[GAN Epoch 106/150] D loss: 0.0892, G loss: 2.8413\n",
      "[GAN Epoch 107/150] D loss: 0.0932, G loss: 2.3365\n",
      "[GAN Epoch 108/150] D loss: 0.0954, G loss: 2.4969\n",
      "[GAN Epoch 109/150] D loss: 0.1017, G loss: 2.5169\n",
      "[GAN Epoch 110/150] D loss: 0.0993, G loss: 2.4852\n",
      "[GAN Epoch 111/150] D loss: 0.0923, G loss: 2.6531\n",
      "[GAN Epoch 112/150] D loss: 0.0861, G loss: 2.5170\n",
      "[GAN Epoch 113/150] D loss: 0.0809, G loss: 2.8158\n",
      "[GAN Epoch 114/150] D loss: 0.0815, G loss: 2.5042\n",
      "[GAN Epoch 115/150] D loss: 0.0779, G loss: 3.0344\n",
      "[GAN Epoch 116/150] D loss: 0.0921, G loss: 2.3251\n",
      "[GAN Epoch 117/150] D loss: 0.0946, G loss: 3.8536\n",
      "[GAN Epoch 118/150] D loss: 0.1199, G loss: 1.8201\n",
      "[GAN Epoch 119/150] D loss: 0.1020, G loss: 4.9283\n",
      "[GAN Epoch 120/150] D loss: 0.0374, G loss: 3.2998\n",
      "[GAN Epoch 121/150] D loss: 0.0750, G loss: 2.3080\n",
      "[GAN Epoch 122/150] D loss: 0.0496, G loss: 4.0863\n",
      "[GAN Epoch 123/150] D loss: 0.0514, G loss: 3.3835\n",
      "[GAN Epoch 124/150] D loss: 0.1008, G loss: 2.1223\n",
      "[GAN Epoch 125/150] D loss: 0.1363, G loss: 4.3994\n",
      "[GAN Epoch 126/150] D loss: 0.1717, G loss: 1.5767\n",
      "[GAN Epoch 127/150] D loss: 0.1618, G loss: 6.0705\n",
      "[GAN Epoch 128/150] D loss: 0.0223, G loss: 4.0080\n",
      "[GAN Epoch 129/150] D loss: 0.0765, G loss: 2.3319\n",
      "[GAN Epoch 130/150] D loss: 0.0260, G loss: 4.0069\n",
      "[GAN Epoch 131/150] D loss: 0.0403, G loss: 4.1621\n",
      "[GAN Epoch 132/150] D loss: 0.0633, G loss: 2.9304\n",
      "[GAN Epoch 133/150] D loss: 0.0806, G loss: 2.9212\n",
      "[GAN Epoch 134/150] D loss: 0.0958, G loss: 2.9523\n",
      "[GAN Epoch 135/150] D loss: 0.1038, G loss: 2.3448\n",
      "[GAN Epoch 136/150] D loss: 0.1368, G loss: 4.2284\n",
      "[GAN Epoch 137/150] D loss: 0.4210, G loss: 0.9024\n",
      "[GAN Epoch 138/150] D loss: 1.0809, G loss: 10.3330\n",
      "[GAN Epoch 139/150] D loss: 0.0201, G loss: 5.1914\n",
      "[GAN Epoch 140/150] D loss: 0.1559, G loss: 1.7857\n",
      "[GAN Epoch 141/150] D loss: 0.0259, G loss: 3.5842\n",
      "[GAN Epoch 142/150] D loss: 0.0355, G loss: 4.0756\n",
      "[GAN Epoch 143/150] D loss: 0.0683, G loss: 3.1815\n",
      "[GAN Epoch 144/150] D loss: 0.1342, G loss: 2.2828\n",
      "[GAN Epoch 145/150] D loss: 0.1857, G loss: 3.0337\n",
      "[GAN Epoch 146/150] D loss: 0.3098, G loss: 1.4780\n",
      "[GAN Epoch 147/150] D loss: 0.4837, G loss: 5.3543\n",
      "[GAN Epoch 148/150] D loss: 0.2184, G loss: 1.7233\n",
      "[GAN Epoch 149/150] D loss: 0.1251, G loss: 4.0404\n",
      "[GAN Epoch 150/150] D loss: 0.0937, G loss: 3.1097\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1383, Train Acc=0.3267\n",
      "Epoch 2: Train Loss=1.1120, Train Acc=0.3150\n",
      "Epoch 3: Train Loss=1.1008, Train Acc=0.3373\n",
      "Epoch 4: Train Loss=1.0998, Train Acc=0.3297\n",
      "Epoch 5: Train Loss=1.0987, Train Acc=0.3430\n",
      "Epoch 6: Train Loss=1.1000, Train Acc=0.3230\n",
      "Epoch 7: Train Loss=1.0978, Train Acc=0.3520\n",
      "Epoch 8: Train Loss=1.0984, Train Acc=0.3353\n",
      "Epoch 9: Train Loss=1.0982, Train Acc=0.3400\n",
      "Epoch 10: Train Loss=1.0993, Train Acc=0.3320\n",
      "DNN Test Accuracy (After Concatenation) for sample size 100: 0.6148\n",
      "\n",
      "[Real Data Only] Training size: 300\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.2181, Train Acc=0.2500\n",
      "Epoch 2: Train Loss=0.6973, Train Acc=0.8100\n",
      "Epoch 3: Train Loss=0.4935, Train Acc=0.8733\n",
      "Epoch 4: Train Loss=0.3840, Train Acc=0.9000\n",
      "Epoch 5: Train Loss=0.3395, Train Acc=0.9000\n",
      "Epoch 6: Train Loss=0.2950, Train Acc=0.9067\n",
      "Epoch 7: Train Loss=0.2836, Train Acc=0.9100\n",
      "Epoch 8: Train Loss=0.3193, Train Acc=0.9033\n",
      "Epoch 9: Train Loss=0.2858, Train Acc=0.8967\n",
      "Epoch 10: Train Loss=0.3229, Train Acc=0.9033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Test Accuracy (Real Data) for sample size 300: 0.8902\n",
      "[GAN Epoch 1/150] D loss: 0.6891, G loss: 0.7074\n",
      "[GAN Epoch 2/150] D loss: 0.6336, G loss: 0.7060\n",
      "[GAN Epoch 3/150] D loss: 0.5832, G loss: 0.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 4/150] D loss: 0.5350, G loss: 0.7034\n",
      "[GAN Epoch 5/150] D loss: 0.4895, G loss: 0.6989\n",
      "[GAN Epoch 6/150] D loss: 0.4507, G loss: 0.6940\n",
      "[GAN Epoch 7/150] D loss: 0.4209, G loss: 0.6934\n",
      "[GAN Epoch 8/150] D loss: 0.3961, G loss: 0.7087\n",
      "[GAN Epoch 9/150] D loss: 0.3700, G loss: 0.7507\n",
      "[GAN Epoch 10/150] D loss: 0.3421, G loss: 0.8220\n",
      "[GAN Epoch 11/150] D loss: 0.3183, G loss: 0.9074\n",
      "[GAN Epoch 12/150] D loss: 0.3031, G loss: 0.9665\n",
      "[GAN Epoch 13/150] D loss: 0.3023, G loss: 0.9672\n",
      "[GAN Epoch 14/150] D loss: 0.3169, G loss: 0.9587\n",
      "[GAN Epoch 15/150] D loss: 0.3477, G loss: 0.9005\n",
      "[GAN Epoch 16/150] D loss: 0.3712, G loss: 0.8503\n",
      "[GAN Epoch 17/150] D loss: 0.3681, G loss: 0.8561\n",
      "[GAN Epoch 18/150] D loss: 0.3345, G loss: 0.9208\n",
      "[GAN Epoch 19/150] D loss: 0.2869, G loss: 1.0481\n",
      "[GAN Epoch 20/150] D loss: 0.2415, G loss: 1.1477\n",
      "[GAN Epoch 21/150] D loss: 0.2074, G loss: 1.3112\n",
      "[GAN Epoch 22/150] D loss: 0.1928, G loss: 1.3243\n",
      "[GAN Epoch 23/150] D loss: 0.1791, G loss: 1.4664\n",
      "[GAN Epoch 24/150] D loss: 0.1787, G loss: 1.4134\n",
      "[GAN Epoch 25/150] D loss: 0.1722, G loss: 1.5354\n",
      "[GAN Epoch 26/150] D loss: 0.1781, G loss: 1.4621\n",
      "[GAN Epoch 27/150] D loss: 0.1614, G loss: 1.7052\n",
      "[GAN Epoch 28/150] D loss: 0.1818, G loss: 1.3827\n",
      "[GAN Epoch 29/150] D loss: 0.1513, G loss: 2.0704\n",
      "[GAN Epoch 30/150] D loss: 0.2686, G loss: 1.0141\n",
      "[GAN Epoch 31/150] D loss: 0.1011, G loss: 2.3146\n",
      "[GAN Epoch 32/150] D loss: 0.1220, G loss: 1.9348\n",
      "[GAN Epoch 33/150] D loss: 0.1809, G loss: 1.4458\n",
      "[GAN Epoch 34/150] D loss: 0.1619, G loss: 2.4020\n",
      "[GAN Epoch 35/150] D loss: 0.3563, G loss: 0.8033\n",
      "[GAN Epoch 36/150] D loss: 0.0600, G loss: 3.3453\n",
      "[GAN Epoch 37/150] D loss: 0.0634, G loss: 3.2095\n",
      "[GAN Epoch 38/150] D loss: 0.1773, G loss: 1.4332\n",
      "[GAN Epoch 39/150] D loss: 0.1215, G loss: 2.1880\n",
      "[GAN Epoch 40/150] D loss: 0.1324, G loss: 2.0020\n",
      "[GAN Epoch 41/150] D loss: 0.1233, G loss: 1.9499\n",
      "[GAN Epoch 42/150] D loss: 0.0888, G loss: 2.7808\n",
      "[GAN Epoch 43/150] D loss: 0.1055, G loss: 1.9139\n",
      "[GAN Epoch 44/150] D loss: 0.0648, G loss: 3.2806\n",
      "[GAN Epoch 45/150] D loss: 0.0810, G loss: 2.2195\n",
      "[GAN Epoch 46/150] D loss: 0.0605, G loss: 3.0002\n",
      "[GAN Epoch 47/150] D loss: 0.0787, G loss: 2.4152\n",
      "[GAN Epoch 48/150] D loss: 0.0647, G loss: 3.2713\n",
      "[GAN Epoch 49/150] D loss: 0.0964, G loss: 2.0179\n",
      "[GAN Epoch 50/150] D loss: 0.0597, G loss: 4.5641\n",
      "[GAN Epoch 51/150] D loss: 0.0260, G loss: 3.3410\n",
      "[GAN Epoch 52/150] D loss: 0.0658, G loss: 2.3272\n",
      "[GAN Epoch 53/150] D loss: 0.0231, G loss: 4.1645\n",
      "[GAN Epoch 54/150] D loss: 0.0305, G loss: 3.9469\n",
      "[GAN Epoch 55/150] D loss: 0.0615, G loss: 2.5082\n",
      "[GAN Epoch 56/150] D loss: 0.0461, G loss: 3.7658\n",
      "[GAN Epoch 57/150] D loss: 0.0495, G loss: 2.8240\n",
      "[GAN Epoch 58/150] D loss: 0.0399, G loss: 3.4400\n",
      "[GAN Epoch 59/150] D loss: 0.0409, G loss: 3.2964\n",
      "[GAN Epoch 60/150] D loss: 0.0457, G loss: 3.0068\n",
      "[GAN Epoch 61/150] D loss: 0.0428, G loss: 3.5259\n",
      "[GAN Epoch 62/150] D loss: 0.0539, G loss: 2.7395\n",
      "[GAN Epoch 63/150] D loss: 0.0569, G loss: 4.4824\n",
      "[GAN Epoch 64/150] D loss: 0.1230, G loss: 1.8433\n",
      "[GAN Epoch 65/150] D loss: 0.2025, G loss: 6.9310\n",
      "[GAN Epoch 66/150] D loss: 0.1341, G loss: 1.6696\n",
      "[GAN Epoch 67/150] D loss: 0.0083, G loss: 4.6251\n",
      "[GAN Epoch 68/150] D loss: 0.0107, G loss: 4.7789\n",
      "[GAN Epoch 69/150] D loss: 0.0300, G loss: 3.3618\n",
      "[GAN Epoch 70/150] D loss: 0.1062, G loss: 1.9889\n",
      "[GAN Epoch 71/150] D loss: 0.3881, G loss: 5.1180\n",
      "[GAN Epoch 72/150] D loss: 2.5870, G loss: 0.0116\n",
      "[GAN Epoch 73/150] D loss: 0.0543, G loss: 3.7634\n",
      "[GAN Epoch 74/150] D loss: 0.2539, G loss: 5.5670\n",
      "[GAN Epoch 75/150] D loss: 0.0738, G loss: 2.6514\n",
      "[GAN Epoch 76/150] D loss: 0.3144, G loss: 0.9562\n",
      "[GAN Epoch 77/150] D loss: 0.2434, G loss: 3.6879\n",
      "[GAN Epoch 78/150] D loss: 0.1565, G loss: 1.9016\n",
      "[GAN Epoch 79/150] D loss: 0.2267, G loss: 1.4128\n",
      "[GAN Epoch 80/150] D loss: 0.3422, G loss: 2.9636\n",
      "[GAN Epoch 81/150] D loss: 0.7649, G loss: 0.3285\n",
      "[GAN Epoch 82/150] D loss: 0.5315, G loss: 4.1137\n",
      "[GAN Epoch 83/150] D loss: 0.2150, G loss: 1.4228\n",
      "[GAN Epoch 84/150] D loss: 0.1916, G loss: 1.5479\n",
      "[GAN Epoch 85/150] D loss: 0.1694, G loss: 2.6951\n",
      "[GAN Epoch 86/150] D loss: 0.1973, G loss: 2.0310\n",
      "[GAN Epoch 87/150] D loss: 0.2733, G loss: 1.4002\n",
      "[GAN Epoch 88/150] D loss: 0.3194, G loss: 2.5711\n",
      "[GAN Epoch 89/150] D loss: 0.5233, G loss: 0.5859\n",
      "[GAN Epoch 90/150] D loss: 0.5588, G loss: 4.2865\n",
      "[GAN Epoch 91/150] D loss: 0.1964, G loss: 1.3886\n",
      "[GAN Epoch 92/150] D loss: 0.1501, G loss: 1.7253\n",
      "[GAN Epoch 93/150] D loss: 0.1238, G loss: 2.9373\n",
      "[GAN Epoch 94/150] D loss: 0.1352, G loss: 2.4089\n",
      "[GAN Epoch 95/150] D loss: 0.2142, G loss: 1.4440\n",
      "[GAN Epoch 96/150] D loss: 0.2441, G loss: 2.7713\n",
      "[GAN Epoch 97/150] D loss: 0.3328, G loss: 1.0066\n",
      "[GAN Epoch 98/150] D loss: 0.3891, G loss: 3.7090\n",
      "[GAN Epoch 99/150] D loss: 0.3228, G loss: 0.9290\n",
      "[GAN Epoch 100/150] D loss: 0.1544, G loss: 3.5206\n",
      "[GAN Epoch 101/150] D loss: 0.1084, G loss: 2.9114\n",
      "[GAN Epoch 102/150] D loss: 0.1801, G loss: 1.6073\n",
      "[GAN Epoch 103/150] D loss: 0.1451, G loss: 2.9790\n",
      "[GAN Epoch 104/150] D loss: 0.1607, G loss: 2.0339\n",
      "[GAN Epoch 105/150] D loss: 0.1838, G loss: 2.0348\n",
      "[GAN Epoch 106/150] D loss: 0.1979, G loss: 2.4619\n",
      "[GAN Epoch 107/150] D loss: 0.2200, G loss: 1.5684\n",
      "[GAN Epoch 108/150] D loss: 0.2748, G loss: 3.9063\n",
      "[GAN Epoch 109/150] D loss: 0.3380, G loss: 1.0056\n",
      "[GAN Epoch 110/150] D loss: 0.2203, G loss: 4.7777\n",
      "[GAN Epoch 111/150] D loss: 0.0721, G loss: 3.4211\n",
      "[GAN Epoch 112/150] D loss: 0.1618, G loss: 1.7335\n",
      "[GAN Epoch 113/150] D loss: 0.0989, G loss: 3.1615\n",
      "[GAN Epoch 114/150] D loss: 0.1295, G loss: 2.7629\n",
      "[GAN Epoch 115/150] D loss: 0.2096, G loss: 1.7150\n",
      "[GAN Epoch 116/150] D loss: 0.2773, G loss: 3.4984\n",
      "[GAN Epoch 117/150] D loss: 0.4520, G loss: 0.9008\n",
      "[GAN Epoch 118/150] D loss: 0.4805, G loss: 5.4043\n",
      "[GAN Epoch 119/150] D loss: 0.0845, G loss: 2.9873\n",
      "[GAN Epoch 120/150] D loss: 0.2693, G loss: 1.2545\n",
      "[GAN Epoch 121/150] D loss: 0.1277, G loss: 4.1442\n",
      "[GAN Epoch 122/150] D loss: 0.1323, G loss: 3.8002\n",
      "[GAN Epoch 123/150] D loss: 0.1786, G loss: 1.7940\n",
      "[GAN Epoch 124/150] D loss: 0.1512, G loss: 2.6510\n",
      "[GAN Epoch 125/150] D loss: 0.1886, G loss: 2.2094\n",
      "[GAN Epoch 126/150] D loss: 0.2112, G loss: 2.1177\n",
      "[GAN Epoch 127/150] D loss: 0.2057, G loss: 2.4662\n",
      "[GAN Epoch 128/150] D loss: 0.2428, G loss: 1.6683\n",
      "[GAN Epoch 129/150] D loss: 0.3439, G loss: 4.3318\n",
      "[GAN Epoch 130/150] D loss: 0.4201, G loss: 1.0270\n",
      "[GAN Epoch 131/150] D loss: 0.2660, G loss: 5.2933\n",
      "[GAN Epoch 132/150] D loss: 0.0857, G loss: 3.7426\n",
      "[GAN Epoch 133/150] D loss: 0.1730, G loss: 1.7355\n",
      "[GAN Epoch 134/150] D loss: 0.1057, G loss: 2.8679\n",
      "[GAN Epoch 135/150] D loss: 0.1473, G loss: 2.6881\n",
      "[GAN Epoch 136/150] D loss: 0.2523, G loss: 1.5922\n",
      "[GAN Epoch 137/150] D loss: 0.3841, G loss: 3.6628\n",
      "[GAN Epoch 138/150] D loss: 0.5923, G loss: 0.6762\n",
      "[GAN Epoch 139/150] D loss: 0.4479, G loss: 5.1774\n",
      "[GAN Epoch 140/150] D loss: 0.1067, G loss: 3.3198\n",
      "[GAN Epoch 141/150] D loss: 0.2129, G loss: 1.5266\n",
      "[GAN Epoch 142/150] D loss: 0.1060, G loss: 2.9674\n",
      "[GAN Epoch 143/150] D loss: 0.1325, G loss: 2.9920\n",
      "[GAN Epoch 144/150] D loss: 0.1854, G loss: 1.8636\n",
      "[GAN Epoch 145/150] D loss: 0.1958, G loss: 2.4441\n",
      "[GAN Epoch 146/150] D loss: 0.2493, G loss: 1.7315\n",
      "[GAN Epoch 147/150] D loss: 0.2673, G loss: 2.7850\n",
      "[GAN Epoch 148/150] D loss: 0.3632, G loss: 1.1215\n",
      "[GAN Epoch 149/150] D loss: 0.5057, G loss: 4.6927\n",
      "[GAN Epoch 150/150] D loss: 0.2767, G loss: 1.5156\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1402, Train Acc=0.3280\n",
      "Epoch 2: Train Loss=1.1086, Train Acc=0.3240\n",
      "Epoch 3: Train Loss=1.1027, Train Acc=0.3357\n",
      "Epoch 4: Train Loss=1.0991, Train Acc=0.3367\n",
      "Epoch 5: Train Loss=1.1007, Train Acc=0.3250\n",
      "Epoch 6: Train Loss=1.0987, Train Acc=0.3330\n",
      "Epoch 7: Train Loss=1.0985, Train Acc=0.3390\n",
      "Epoch 8: Train Loss=1.0972, Train Acc=0.3573\n",
      "Epoch 9: Train Loss=1.0983, Train Acc=0.3427\n",
      "Epoch 10: Train Loss=1.0993, Train Acc=0.3390\n",
      "DNN Test Accuracy (After Concatenation) for sample size 300: 0.1499\n",
      "\n",
      "[Real Data Only] Training size: 1000\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=0.9446, Train Acc=0.5220\n",
      "Epoch 2: Train Loss=0.4103, Train Acc=0.9030\n",
      "Epoch 3: Train Loss=0.2928, Train Acc=0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.2785, Train Acc=0.9120\n",
      "Epoch 5: Train Loss=0.2724, Train Acc=0.9090\n",
      "Epoch 6: Train Loss=0.2955, Train Acc=0.9130\n",
      "Epoch 7: Train Loss=0.2955, Train Acc=0.9100\n",
      "Epoch 8: Train Loss=0.2745, Train Acc=0.9140\n",
      "Epoch 9: Train Loss=0.2587, Train Acc=0.9220\n",
      "Epoch 10: Train Loss=0.2595, Train Acc=0.9140\n",
      "DNN Test Accuracy (Real Data) for sample size 1000: 0.9029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 1/150] D loss: 0.6666, G loss: 0.7252\n",
      "[GAN Epoch 2/150] D loss: 0.5581, G loss: 0.7274\n",
      "[GAN Epoch 3/150] D loss: 0.4646, G loss: 0.7236\n",
      "[GAN Epoch 4/150] D loss: 0.4039, G loss: 0.7242\n",
      "[GAN Epoch 5/150] D loss: 0.3577, G loss: 0.7997\n",
      "[GAN Epoch 6/150] D loss: 0.3173, G loss: 0.9481\n",
      "[GAN Epoch 7/150] D loss: 0.3172, G loss: 0.9677\n",
      "[GAN Epoch 8/150] D loss: 0.3510, G loss: 0.9066\n",
      "[GAN Epoch 9/150] D loss: 0.3304, G loss: 0.9568\n",
      "[GAN Epoch 10/150] D loss: 0.2579, G loss: 1.1614\n",
      "[GAN Epoch 11/150] D loss: 0.2185, G loss: 1.2492\n",
      "[GAN Epoch 12/150] D loss: 0.2009, G loss: 1.3522\n",
      "[GAN Epoch 13/150] D loss: 0.1876, G loss: 1.4434\n",
      "[GAN Epoch 14/150] D loss: 0.1698, G loss: 1.5769\n",
      "[GAN Epoch 15/150] D loss: 0.1564, G loss: 1.6785\n",
      "[GAN Epoch 16/150] D loss: 0.2352, G loss: 1.5117\n",
      "[GAN Epoch 17/150] D loss: 0.2495, G loss: 1.8892\n",
      "[GAN Epoch 18/150] D loss: 0.1140, G loss: 2.1918\n",
      "[GAN Epoch 19/150] D loss: 0.2801, G loss: 1.8951\n",
      "[GAN Epoch 20/150] D loss: 0.2994, G loss: 1.8025\n",
      "[GAN Epoch 21/150] D loss: 0.0739, G loss: 2.8749\n",
      "[GAN Epoch 22/150] D loss: 0.2142, G loss: 1.9966\n",
      "[GAN Epoch 23/150] D loss: 0.1218, G loss: 2.2173\n",
      "[GAN Epoch 24/150] D loss: 0.0666, G loss: 2.8366\n",
      "[GAN Epoch 25/150] D loss: 0.0940, G loss: 2.6315\n",
      "[GAN Epoch 26/150] D loss: 0.0875, G loss: 2.7327\n",
      "[GAN Epoch 27/150] D loss: 0.0764, G loss: 2.6360\n",
      "[GAN Epoch 28/150] D loss: 0.0798, G loss: 2.7407\n",
      "[GAN Epoch 29/150] D loss: 0.0835, G loss: 2.7556\n",
      "[GAN Epoch 30/150] D loss: 0.1105, G loss: 2.9772\n",
      "[GAN Epoch 31/150] D loss: 0.0472, G loss: 4.5276\n",
      "[GAN Epoch 32/150] D loss: 0.0626, G loss: 2.8720\n",
      "[GAN Epoch 33/150] D loss: 0.0874, G loss: 2.6054\n",
      "[GAN Epoch 34/150] D loss: 0.5793, G loss: 2.1417\n",
      "[GAN Epoch 35/150] D loss: 0.5728, G loss: 4.5227\n",
      "[GAN Epoch 36/150] D loss: 0.3481, G loss: 2.2780\n",
      "[GAN Epoch 37/150] D loss: 0.1970, G loss: 3.0260\n",
      "[GAN Epoch 38/150] D loss: 0.2894, G loss: 2.2819\n",
      "[GAN Epoch 39/150] D loss: 0.1972, G loss: 1.7336\n",
      "[GAN Epoch 40/150] D loss: 0.3722, G loss: 1.6243\n",
      "[GAN Epoch 41/150] D loss: 1.4749, G loss: 2.0996\n",
      "[GAN Epoch 42/150] D loss: 0.7517, G loss: 0.7657\n",
      "[GAN Epoch 43/150] D loss: 0.3265, G loss: 2.3997\n",
      "[GAN Epoch 44/150] D loss: 0.2462, G loss: 1.3556\n",
      "[GAN Epoch 45/150] D loss: 0.2157, G loss: 1.8577\n",
      "[GAN Epoch 46/150] D loss: 0.2478, G loss: 1.5942\n",
      "[GAN Epoch 47/150] D loss: 0.2868, G loss: 1.5380\n",
      "[GAN Epoch 48/150] D loss: 0.2967, G loss: 1.5031\n",
      "[GAN Epoch 49/150] D loss: 0.2617, G loss: 1.6402\n",
      "[GAN Epoch 50/150] D loss: 0.2239, G loss: 1.8700\n",
      "[GAN Epoch 51/150] D loss: 0.2209, G loss: 2.1102\n",
      "[GAN Epoch 52/150] D loss: 0.2099, G loss: 2.1600\n",
      "[GAN Epoch 53/150] D loss: 0.1540, G loss: 1.9783\n",
      "[GAN Epoch 54/150] D loss: 0.1815, G loss: 1.9381\n",
      "[GAN Epoch 55/150] D loss: 0.2972, G loss: 2.1409\n",
      "[GAN Epoch 56/150] D loss: 0.1989, G loss: 3.3857\n",
      "[GAN Epoch 57/150] D loss: 0.1599, G loss: 2.5474\n",
      "[GAN Epoch 58/150] D loss: 0.1732, G loss: 2.0187\n",
      "[GAN Epoch 59/150] D loss: 0.2769, G loss: 2.1039\n",
      "[GAN Epoch 60/150] D loss: 0.2396, G loss: 3.0630\n",
      "[GAN Epoch 61/150] D loss: 0.1137, G loss: 2.6540\n",
      "[GAN Epoch 62/150] D loss: 0.1489, G loss: 2.2629\n",
      "[GAN Epoch 63/150] D loss: 0.1946, G loss: 2.1577\n",
      "[GAN Epoch 64/150] D loss: 0.2762, G loss: 2.6031\n",
      "[GAN Epoch 65/150] D loss: 0.1389, G loss: 3.9188\n",
      "[GAN Epoch 66/150] D loss: 0.2115, G loss: 2.8952\n",
      "[GAN Epoch 67/150] D loss: 0.1703, G loss: 1.9798\n",
      "[GAN Epoch 68/150] D loss: 0.3803, G loss: 2.6352\n",
      "[GAN Epoch 69/150] D loss: 0.2270, G loss: 3.6365\n",
      "[GAN Epoch 70/150] D loss: 0.2026, G loss: 2.7781\n",
      "[GAN Epoch 71/150] D loss: 0.1695, G loss: 1.9731\n",
      "[GAN Epoch 72/150] D loss: 0.3910, G loss: 1.8761\n",
      "[GAN Epoch 73/150] D loss: 0.4729, G loss: 4.4695\n",
      "[GAN Epoch 74/150] D loss: 0.2387, G loss: 2.6230\n",
      "[GAN Epoch 75/150] D loss: 0.1990, G loss: 2.3738\n",
      "[GAN Epoch 76/150] D loss: 0.2352, G loss: 2.6010\n",
      "[GAN Epoch 77/150] D loss: 0.2288, G loss: 1.9481\n",
      "[GAN Epoch 78/150] D loss: 0.2682, G loss: 1.8161\n",
      "[GAN Epoch 79/150] D loss: 0.5710, G loss: 2.1052\n",
      "[GAN Epoch 80/150] D loss: 0.3177, G loss: 4.3931\n",
      "[GAN Epoch 81/150] D loss: 0.2434, G loss: 2.3793\n",
      "[GAN Epoch 82/150] D loss: 0.2876, G loss: 1.7866\n",
      "[GAN Epoch 83/150] D loss: 0.5747, G loss: 2.5708\n",
      "[GAN Epoch 84/150] D loss: 0.2435, G loss: 2.3425\n",
      "[GAN Epoch 85/150] D loss: 0.2853, G loss: 2.0063\n",
      "[GAN Epoch 86/150] D loss: 0.6903, G loss: 2.8018\n",
      "[GAN Epoch 87/150] D loss: 0.3610, G loss: 1.8628\n",
      "[GAN Epoch 88/150] D loss: 0.2585, G loss: 1.7907\n",
      "[GAN Epoch 89/150] D loss: 0.4205, G loss: 1.9851\n",
      "[GAN Epoch 90/150] D loss: 0.4895, G loss: 1.9955\n",
      "[GAN Epoch 91/150] D loss: 0.3737, G loss: 2.4245\n",
      "[GAN Epoch 92/150] D loss: 0.2620, G loss: 1.7545\n",
      "[GAN Epoch 93/150] D loss: 0.3270, G loss: 1.5987\n",
      "[GAN Epoch 94/150] D loss: 0.4893, G loss: 2.3382\n",
      "[GAN Epoch 95/150] D loss: 0.7946, G loss: 1.9866\n",
      "[GAN Epoch 96/150] D loss: 0.2635, G loss: 1.9693\n",
      "[GAN Epoch 97/150] D loss: 0.2701, G loss: 1.9984\n",
      "[GAN Epoch 98/150] D loss: 0.3208, G loss: 1.6317\n",
      "[GAN Epoch 99/150] D loss: 0.6216, G loss: 1.6802\n",
      "[GAN Epoch 100/150] D loss: 0.4352, G loss: 2.8971\n",
      "[GAN Epoch 101/150] D loss: 0.2714, G loss: 2.1668\n",
      "[GAN Epoch 102/150] D loss: 0.3235, G loss: 1.6781\n",
      "[GAN Epoch 103/150] D loss: 0.5667, G loss: 2.3006\n",
      "[GAN Epoch 104/150] D loss: 0.4955, G loss: 1.9480\n",
      "[GAN Epoch 105/150] D loss: 0.2730, G loss: 1.7131\n",
      "[GAN Epoch 106/150] D loss: 0.3053, G loss: 2.1374\n",
      "[GAN Epoch 107/150] D loss: 0.3265, G loss: 1.5635\n",
      "[GAN Epoch 108/150] D loss: 0.4137, G loss: 1.6522\n",
      "[GAN Epoch 109/150] D loss: 0.8648, G loss: 2.6390\n",
      "[GAN Epoch 110/150] D loss: 0.2779, G loss: 1.3672\n",
      "[GAN Epoch 111/150] D loss: 0.2481, G loss: 1.9169\n",
      "[GAN Epoch 112/150] D loss: 0.3136, G loss: 1.5755\n",
      "[GAN Epoch 113/150] D loss: 0.4668, G loss: 1.4531\n",
      "[GAN Epoch 114/150] D loss: 0.5914, G loss: 2.4271\n",
      "[GAN Epoch 115/150] D loss: 0.2058, G loss: 2.4440\n",
      "[GAN Epoch 116/150] D loss: 0.4601, G loss: 2.0832\n",
      "[GAN Epoch 117/150] D loss: 0.5302, G loss: 1.8098\n",
      "[GAN Epoch 118/150] D loss: 0.3450, G loss: 1.5935\n",
      "[GAN Epoch 119/150] D loss: 0.3228, G loss: 1.4018\n",
      "[GAN Epoch 120/150] D loss: 0.3971, G loss: 1.3720\n",
      "[GAN Epoch 121/150] D loss: 0.5791, G loss: 2.0599\n",
      "[GAN Epoch 122/150] D loss: 0.2919, G loss: 2.7627\n",
      "[GAN Epoch 123/150] D loss: 0.3501, G loss: 2.0207\n",
      "[GAN Epoch 124/150] D loss: 0.3086, G loss: 1.3933\n",
      "[GAN Epoch 125/150] D loss: 0.3484, G loss: 1.3765\n",
      "[GAN Epoch 126/150] D loss: 0.5266, G loss: 1.2320\n",
      "[GAN Epoch 127/150] D loss: 0.7096, G loss: 2.9679\n",
      "[GAN Epoch 128/150] D loss: 0.2514, G loss: 1.9637\n",
      "[GAN Epoch 129/150] D loss: 0.3745, G loss: 1.5954\n",
      "[GAN Epoch 130/150] D loss: 0.5498, G loss: 2.0496\n",
      "[GAN Epoch 131/150] D loss: 0.3700, G loss: 1.7412\n",
      "[GAN Epoch 132/150] D loss: 0.2700, G loss: 1.5923\n",
      "[GAN Epoch 133/150] D loss: 0.3546, G loss: 1.4903\n",
      "[GAN Epoch 134/150] D loss: 0.6278, G loss: 1.6596\n",
      "[GAN Epoch 135/150] D loss: 0.4677, G loss: 2.6629\n",
      "[GAN Epoch 136/150] D loss: 0.2192, G loss: 1.9616\n",
      "[GAN Epoch 137/150] D loss: 0.3298, G loss: 1.5511\n",
      "[GAN Epoch 138/150] D loss: 0.5605, G loss: 2.0002\n",
      "[GAN Epoch 139/150] D loss: 0.7113, G loss: 1.9054\n",
      "[GAN Epoch 140/150] D loss: 0.3007, G loss: 1.6888\n",
      "[GAN Epoch 141/150] D loss: 0.2759, G loss: 1.9901\n",
      "[GAN Epoch 142/150] D loss: 0.3817, G loss: 1.6734\n",
      "[GAN Epoch 143/150] D loss: 0.6564, G loss: 2.1805\n",
      "[GAN Epoch 144/150] D loss: 0.3396, G loss: 1.4549\n",
      "[GAN Epoch 145/150] D loss: 0.3153, G loss: 1.3808\n",
      "[GAN Epoch 146/150] D loss: 0.5187, G loss: 1.3784\n",
      "[GAN Epoch 147/150] D loss: 0.5837, G loss: 2.7277\n",
      "[GAN Epoch 148/150] D loss: 0.2882, G loss: 2.1030\n",
      "[GAN Epoch 149/150] D loss: 0.3584, G loss: 1.6701\n",
      "[GAN Epoch 150/150] D loss: 0.4224, G loss: 1.8304\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1422, Train Acc=0.3367\n",
      "Epoch 2: Train Loss=1.1131, Train Acc=0.3473\n",
      "Epoch 3: Train Loss=1.1007, Train Acc=0.3500\n",
      "Epoch 4: Train Loss=1.0995, Train Acc=0.3420\n",
      "Epoch 5: Train Loss=1.0975, Train Acc=0.3520\n",
      "Epoch 6: Train Loss=1.0977, Train Acc=0.3553\n",
      "Epoch 7: Train Loss=1.1007, Train Acc=0.3430\n",
      "Epoch 8: Train Loss=1.0980, Train Acc=0.3490\n",
      "Epoch 9: Train Loss=1.0954, Train Acc=0.3580\n",
      "Epoch 10: Train Loss=1.0967, Train Acc=0.3490\n",
      "DNN Test Accuracy (After Concatenation) for sample size 1000: 0.5452\n",
      "\n",
      "[Real Data Only] Training size: 7872\n",
      "Training DNN on real data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.3960, Train Acc=0.8619\n",
      "Epoch 2: Train Loss=0.2794, Train Acc=0.9055\n",
      "Epoch 3: Train Loss=0.2475, Train Acc=0.9082\n",
      "Epoch 4: Train Loss=0.2421, Train Acc=0.9126\n",
      "Epoch 5: Train Loss=0.2349, Train Acc=0.9134\n",
      "Epoch 6: Train Loss=0.2314, Train Acc=0.9169\n",
      "Epoch 7: Train Loss=0.2289, Train Acc=0.9159\n",
      "Epoch 8: Train Loss=0.2265, Train Acc=0.9155\n",
      "Epoch 9: Train Loss=0.2236, Train Acc=0.9160\n",
      "Epoch 10: Train Loss=0.2211, Train Acc=0.9170\n",
      "DNN Test Accuracy (Real Data) for sample size 7872: 0.9131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ss348\\Desktop\\New folder\\GAN_Transformer\\SICK\\experiment\\GAN_generated_data_only\\..\\..\\utility\\data.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "C:\\Users\\ss348\\AppData\\Local\\Temp\\ipykernel_36088\\900496475.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 1/150] D loss: 0.4442, G loss: 0.7561\n",
      "[GAN Epoch 2/150] D loss: 0.2377, G loss: 1.3396\n",
      "[GAN Epoch 3/150] D loss: 0.1607, G loss: 2.1687\n",
      "[GAN Epoch 4/150] D loss: 0.0782, G loss: 2.8928\n",
      "[GAN Epoch 5/150] D loss: 0.3400, G loss: 2.5897\n",
      "[GAN Epoch 6/150] D loss: 0.3516, G loss: 1.8569\n",
      "[GAN Epoch 7/150] D loss: 0.2369, G loss: 2.3079\n",
      "[GAN Epoch 8/150] D loss: 0.2548, G loss: 2.5052\n",
      "[GAN Epoch 9/150] D loss: 0.2412, G loss: 2.6157\n",
      "[GAN Epoch 10/150] D loss: 0.2738, G loss: 2.5512\n",
      "[GAN Epoch 11/150] D loss: 0.3680, G loss: 2.3234\n",
      "[GAN Epoch 12/150] D loss: 0.3871, G loss: 2.0296\n",
      "[GAN Epoch 13/150] D loss: 0.3571, G loss: 1.9232\n",
      "[GAN Epoch 14/150] D loss: 0.3846, G loss: 1.8551\n",
      "[GAN Epoch 15/150] D loss: 0.3900, G loss: 1.8509\n",
      "[GAN Epoch 16/150] D loss: 0.4155, G loss: 1.8642\n",
      "[GAN Epoch 17/150] D loss: 0.4085, G loss: 1.8339\n",
      "[GAN Epoch 18/150] D loss: 0.4416, G loss: 1.8305\n",
      "[GAN Epoch 19/150] D loss: 0.4010, G loss: 1.9057\n",
      "[GAN Epoch 20/150] D loss: 0.4251, G loss: 1.6451\n",
      "[GAN Epoch 21/150] D loss: 0.4278, G loss: 1.8000\n",
      "[GAN Epoch 22/150] D loss: 0.4838, G loss: 1.6111\n",
      "[GAN Epoch 23/150] D loss: 0.4471, G loss: 1.6835\n",
      "[GAN Epoch 24/150] D loss: 0.4590, G loss: 1.6510\n",
      "[GAN Epoch 25/150] D loss: 0.4172, G loss: 1.7259\n",
      "[GAN Epoch 26/150] D loss: 0.4328, G loss: 1.7926\n",
      "[GAN Epoch 27/150] D loss: 0.4429, G loss: 1.7325\n",
      "[GAN Epoch 28/150] D loss: 0.4761, G loss: 1.6712\n",
      "[GAN Epoch 29/150] D loss: 0.4913, G loss: 1.6607\n",
      "[GAN Epoch 30/150] D loss: 0.4767, G loss: 1.6328\n",
      "[GAN Epoch 31/150] D loss: 0.4568, G loss: 1.6480\n",
      "[GAN Epoch 32/150] D loss: 0.4629, G loss: 1.5631\n",
      "[GAN Epoch 33/150] D loss: 0.4691, G loss: 1.6671\n",
      "[GAN Epoch 34/150] D loss: 0.4713, G loss: 1.5985\n",
      "[GAN Epoch 35/150] D loss: 0.4811, G loss: 1.5978\n",
      "[GAN Epoch 36/150] D loss: 0.4642, G loss: 1.6678\n",
      "[GAN Epoch 37/150] D loss: 0.4519, G loss: 1.6325\n",
      "[GAN Epoch 38/150] D loss: 0.5104, G loss: 1.5194\n",
      "[GAN Epoch 39/150] D loss: 0.5040, G loss: 1.4366\n",
      "[GAN Epoch 40/150] D loss: 0.5133, G loss: 1.5500\n",
      "[GAN Epoch 41/150] D loss: 0.4906, G loss: 1.4788\n",
      "[GAN Epoch 42/150] D loss: 0.5037, G loss: 1.5055\n",
      "[GAN Epoch 43/150] D loss: 0.4067, G loss: 1.3594\n",
      "[GAN Epoch 44/150] D loss: 0.4998, G loss: 1.5653\n",
      "[GAN Epoch 45/150] D loss: 0.5106, G loss: 1.4174\n",
      "[GAN Epoch 46/150] D loss: 0.4774, G loss: 1.4815\n",
      "[GAN Epoch 47/150] D loss: 0.4755, G loss: 1.3976\n",
      "[GAN Epoch 48/150] D loss: 0.4782, G loss: 1.5104\n",
      "[GAN Epoch 49/150] D loss: 0.4822, G loss: 1.4878\n",
      "[GAN Epoch 50/150] D loss: 0.4787, G loss: 1.3522\n",
      "[GAN Epoch 51/150] D loss: 0.5295, G loss: 1.3336\n",
      "[GAN Epoch 52/150] D loss: 0.5670, G loss: 1.2876\n",
      "[GAN Epoch 53/150] D loss: 0.5436, G loss: 1.3005\n",
      "[GAN Epoch 54/150] D loss: 0.5164, G loss: 1.1958\n",
      "[GAN Epoch 55/150] D loss: 0.5521, G loss: 1.3233\n",
      "[GAN Epoch 56/150] D loss: 0.5271, G loss: 1.3290\n",
      "[GAN Epoch 57/150] D loss: 0.4972, G loss: 1.2682\n",
      "[GAN Epoch 58/150] D loss: 0.5237, G loss: 1.2779\n",
      "[GAN Epoch 59/150] D loss: 0.5296, G loss: 1.2514\n",
      "[GAN Epoch 60/150] D loss: 0.5458, G loss: 1.3095\n",
      "[GAN Epoch 61/150] D loss: 0.5351, G loss: 1.1910\n",
      "[GAN Epoch 62/150] D loss: 0.5123, G loss: 1.2657\n",
      "[GAN Epoch 63/150] D loss: 0.5292, G loss: 1.2327\n",
      "[GAN Epoch 64/150] D loss: 0.5084, G loss: 1.2776\n",
      "[GAN Epoch 65/150] D loss: 0.5463, G loss: 1.3750\n",
      "[GAN Epoch 66/150] D loss: 0.5397, G loss: 1.1794\n",
      "[GAN Epoch 67/150] D loss: 0.5062, G loss: 1.2823\n",
      "[GAN Epoch 68/150] D loss: 0.5104, G loss: 1.2422\n",
      "[GAN Epoch 69/150] D loss: 0.5093, G loss: 1.3119\n",
      "[GAN Epoch 70/150] D loss: 0.5244, G loss: 1.1793\n",
      "[GAN Epoch 71/150] D loss: 0.5767, G loss: 1.3284\n",
      "[GAN Epoch 72/150] D loss: 0.4853, G loss: 1.1974\n",
      "[GAN Epoch 73/150] D loss: 0.5677, G loss: 1.2826\n",
      "[GAN Epoch 74/150] D loss: 0.5147, G loss: 1.0994\n",
      "[GAN Epoch 75/150] D loss: 0.5198, G loss: 1.2090\n",
      "[GAN Epoch 76/150] D loss: 0.5628, G loss: 1.2549\n",
      "[GAN Epoch 77/150] D loss: 0.5222, G loss: 1.1497\n",
      "[GAN Epoch 78/150] D loss: 0.5176, G loss: 1.1667\n",
      "[GAN Epoch 79/150] D loss: 0.5202, G loss: 1.3371\n",
      "[GAN Epoch 80/150] D loss: 0.5799, G loss: 1.2981\n",
      "[GAN Epoch 81/150] D loss: 0.5206, G loss: 1.2576\n",
      "[GAN Epoch 82/150] D loss: 0.5300, G loss: 1.0851\n",
      "[GAN Epoch 83/150] D loss: 0.5359, G loss: 1.0983\n",
      "[GAN Epoch 84/150] D loss: 0.5138, G loss: 1.1865\n",
      "[GAN Epoch 85/150] D loss: 0.5524, G loss: 1.2444\n",
      "[GAN Epoch 86/150] D loss: 0.5688, G loss: 1.2373\n",
      "[GAN Epoch 87/150] D loss: 0.5717, G loss: 1.1643\n",
      "[GAN Epoch 88/150] D loss: 0.4910, G loss: 1.0677\n",
      "[GAN Epoch 89/150] D loss: 0.5632, G loss: 1.2030\n",
      "[GAN Epoch 90/150] D loss: 0.5751, G loss: 1.1978\n",
      "[GAN Epoch 91/150] D loss: 0.5224, G loss: 1.0938\n",
      "[GAN Epoch 92/150] D loss: 0.5294, G loss: 1.1356\n",
      "[GAN Epoch 93/150] D loss: 0.5823, G loss: 1.1950\n",
      "[GAN Epoch 94/150] D loss: 0.5668, G loss: 1.1930\n",
      "[GAN Epoch 95/150] D loss: 0.5195, G loss: 1.0401\n",
      "[GAN Epoch 96/150] D loss: 0.5544, G loss: 1.1395\n",
      "[GAN Epoch 97/150] D loss: 0.5284, G loss: 1.0842\n",
      "[GAN Epoch 98/150] D loss: 0.5572, G loss: 1.2032\n",
      "[GAN Epoch 99/150] D loss: 0.5959, G loss: 1.2298\n",
      "[GAN Epoch 100/150] D loss: 0.5589, G loss: 0.9977\n",
      "[GAN Epoch 101/150] D loss: 0.5395, G loss: 1.0981\n",
      "[GAN Epoch 102/150] D loss: 0.5238, G loss: 1.0717\n",
      "[GAN Epoch 103/150] D loss: 0.5841, G loss: 1.2356\n",
      "[GAN Epoch 104/150] D loss: 0.5713, G loss: 1.1336\n",
      "[GAN Epoch 105/150] D loss: 0.5308, G loss: 1.0666\n",
      "[GAN Epoch 106/150] D loss: 0.5532, G loss: 1.1848\n",
      "[GAN Epoch 107/150] D loss: 0.5247, G loss: 1.1481\n",
      "[GAN Epoch 108/150] D loss: 0.5764, G loss: 1.1843\n",
      "[GAN Epoch 109/150] D loss: 0.5287, G loss: 1.1742\n",
      "[GAN Epoch 110/150] D loss: 0.5357, G loss: 1.0272\n",
      "[GAN Epoch 111/150] D loss: 0.5856, G loss: 1.2491\n",
      "[GAN Epoch 112/150] D loss: 0.4961, G loss: 1.2141\n",
      "[GAN Epoch 113/150] D loss: 0.5460, G loss: 1.1059\n",
      "[GAN Epoch 114/150] D loss: 0.5454, G loss: 1.2153\n",
      "[GAN Epoch 115/150] D loss: 0.5802, G loss: 1.1761\n",
      "[GAN Epoch 116/150] D loss: 0.5362, G loss: 1.0435\n",
      "[GAN Epoch 117/150] D loss: 0.5704, G loss: 1.2770\n",
      "[GAN Epoch 118/150] D loss: 0.5437, G loss: 1.1075\n",
      "[GAN Epoch 119/150] D loss: 0.5602, G loss: 1.1589\n",
      "[GAN Epoch 120/150] D loss: 0.5626, G loss: 1.2193\n",
      "[GAN Epoch 121/150] D loss: 0.5982, G loss: 1.1328\n",
      "[GAN Epoch 122/150] D loss: 0.5635, G loss: 1.1033\n",
      "[GAN Epoch 123/150] D loss: 0.5508, G loss: 1.0356\n",
      "[GAN Epoch 124/150] D loss: 0.5562, G loss: 0.9476\n",
      "[GAN Epoch 125/150] D loss: 0.5350, G loss: 1.0212\n",
      "[GAN Epoch 126/150] D loss: 0.6054, G loss: 1.1793\n",
      "[GAN Epoch 127/150] D loss: 0.5230, G loss: 1.0970\n",
      "[GAN Epoch 128/150] D loss: 0.5809, G loss: 1.1806\n",
      "[GAN Epoch 129/150] D loss: 0.5547, G loss: 1.0696\n",
      "[GAN Epoch 130/150] D loss: 0.5607, G loss: 1.1370\n",
      "[GAN Epoch 131/150] D loss: 0.5575, G loss: 0.9643\n",
      "[GAN Epoch 132/150] D loss: 0.5577, G loss: 1.0060\n",
      "[GAN Epoch 133/150] D loss: 0.5703, G loss: 1.0848\n",
      "[GAN Epoch 134/150] D loss: 0.5397, G loss: 1.2003\n",
      "[GAN Epoch 135/150] D loss: 0.5547, G loss: 1.1258\n",
      "[GAN Epoch 136/150] D loss: 0.6169, G loss: 1.1202\n",
      "[GAN Epoch 137/150] D loss: 0.5741, G loss: 1.0271\n",
      "[GAN Epoch 138/150] D loss: 0.5690, G loss: 1.0707\n",
      "[GAN Epoch 139/150] D loss: 0.5778, G loss: 0.9953\n",
      "[GAN Epoch 140/150] D loss: 0.5556, G loss: 1.0114\n",
      "[GAN Epoch 141/150] D loss: 0.5872, G loss: 1.2106\n",
      "[GAN Epoch 142/150] D loss: 0.5671, G loss: 0.9447\n",
      "[GAN Epoch 143/150] D loss: 0.5900, G loss: 1.1413\n",
      "[GAN Epoch 144/150] D loss: 0.5511, G loss: 1.0489\n",
      "[GAN Epoch 145/150] D loss: 0.5894, G loss: 0.9994\n",
      "[GAN Epoch 146/150] D loss: 0.5975, G loss: 1.0634\n",
      "[GAN Epoch 147/150] D loss: 0.5499, G loss: 0.9832\n",
      "[GAN Epoch 148/150] D loss: 0.5754, G loss: 1.1183\n",
      "[GAN Epoch 149/150] D loss: 0.5813, G loss: 1.0630\n",
      "[GAN Epoch 150/150] D loss: 0.5774, G loss: 1.0673\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1369, Train Acc=0.3840\n",
      "Epoch 2: Train Loss=1.0568, Train Acc=0.4400\n",
      "Epoch 3: Train Loss=1.0204, Train Acc=0.4817\n",
      "Epoch 4: Train Loss=0.9899, Train Acc=0.5230\n",
      "Epoch 5: Train Loss=0.9659, Train Acc=0.5173\n",
      "Epoch 6: Train Loss=0.9404, Train Acc=0.5540\n",
      "Epoch 7: Train Loss=0.9214, Train Acc=0.5670\n",
      "Epoch 8: Train Loss=0.8937, Train Acc=0.5870\n",
      "Epoch 9: Train Loss=0.8764, Train Acc=0.5937\n",
      "Epoch 10: Train Loss=0.8560, Train Acc=0.6037\n",
      "DNN Test Accuracy (After Concatenation) for sample size 7872: 0.3872\n",
      "Accuracy Summary:\n",
      "\n",
      "   Train Samples    Real Only Accuracy    After Concatenation Accuracy \n",
      "\n",
      "              20              0.856707                        0.372459 \n",
      "\n",
      "              50              0.871951                        0.430386 \n",
      "\n",
      "              70              0.88313                         0.347053 \n",
      "\n",
      "             100              0.891768                        0.614837 \n",
      "\n",
      "             300              0.890244                        0.149898 \n",
      "\n",
      "            1000              0.902947                        0.545224 \n",
      "\n",
      "            7872              0.91311                         0.387195 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_sizes = [20, 50, 70, 100, 300, 1000, len(train_ds)]\n",
    "dnn_accuracy_before = {}\n",
    "dnn_accuracy_after = {}\n",
    "\n",
    "for size in sample_sizes:\n",
    "    print(f\"\\n[Real Data Only] Training size: {size}\")\n",
    "    train_subset = torch.utils.data.Subset(train_ds, range(size))\n",
    "    train_loader = get_loader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = get_loader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = train_ds[0][0].shape[0]  \n",
    "    hidden_dim = 128\n",
    "    num_classes = len(np.unique(train_y_full)) if 'train_y_full' in globals() else 3\n",
    "    dnn_model = DNNClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, dropout=0.5)\n",
    "    dnn_model.to(device)\n",
    "    \n",
    "    print(\"Training DNN on real data...\")\n",
    "    acc_before, preds_before, labels_before = train_and_evaluate_dnn(dnn_model, train_loader, test_loader, num_epochs, learning_rate, device)\n",
    "    print(f\"DNN Test Accuracy (Real Data) for sample size {size}: {acc_before:.4f}\")\n",
    "    dnn_accuracy_before[size] = acc_before\n",
    "    \n",
    "\n",
    "    generator = Generator(\n",
    "        latent_dim=latent_dim,\n",
    "        condition_dim=condition_dim,\n",
    "        num_classes=num_classes,\n",
    "        start_dim=latent_dim * 2,\n",
    "        n_layer=3,\n",
    "        output_dim=input_dim  \n",
    "    )\n",
    "    discriminator = Discriminator(\n",
    "        condition_dim=condition_dim,\n",
    "        num_classes=num_classes,\n",
    "        start_dim=256,\n",
    "        n_layer=3,\n",
    "        input_dim=input_dim\n",
    "    )\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    gan_loader = get_loader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(gan_epochs):\n",
    "        d_loss_epoch, g_loss_epoch = 0.0, 0.0\n",
    "        for embeddings, labels in gan_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            b_size = embeddings.size(0)\n",
    "            valid = torch.ones(b_size, 1, device=device)\n",
    "            fake = torch.zeros(b_size, 1, device=device)\n",
    "            # -----------------\n",
    "            # Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(b_size, latent_dim, device=device)\n",
    "            gen_data = generator(z, labels)\n",
    "            g_loss = adversarial_loss(discriminator(gen_data, labels), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            # -----------------\n",
    "            # Train Discriminator\n",
    "            # -----------------\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(embeddings, labels), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_data.detach(), labels), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            d_loss_epoch += d_loss.item()\n",
    "            g_loss_epoch += g_loss.item()\n",
    "        print(f\"[GAN Epoch {epoch+1}/{gan_epochs}] D loss: {d_loss_epoch/len(gan_loader):.4f}, G loss: {g_loss_epoch/len(gan_loader):.4f}\")\n",
    "    \n",
    "\n",
    "    synthetic_data_list = []\n",
    "    synthetic_labels_list = []\n",
    "    unique_labels = np.unique(train_y_full) if 'train_y_full' in globals() else [0,1,2]\n",
    "    for lab in unique_labels:\n",
    "        lab_tensor = torch.full((generation_size,), lab, dtype=torch.long, device=device)\n",
    "        z = torch.randn(generation_size, latent_dim, device=device)\n",
    "        synth = generator(z, lab_tensor).cpu().detach().numpy()\n",
    "        synthetic_data_list.append(synth)\n",
    "        synthetic_labels_list.append(np.full((generation_size,), lab))\n",
    "    synthetic_x = np.concatenate(synthetic_data_list, axis=0)\n",
    "    synthetic_y = np.concatenate(synthetic_labels_list, axis=0)\n",
    "    \n",
    "    \n",
    "    train_combined_dataset = TensorDataset(torch.tensor(synthetic_x, dtype=torch.float),\n",
    "                                             torch.tensor(synthetic_y, dtype=torch.long))\n",
    "    train_combined_loader = DataLoader(train_combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dnn_model_aug = DNNClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, dropout=0.5)\n",
    "    dnn_model_aug.to(device)\n",
    "    \n",
    "    print(\"Training DNN on real + synthetic (concatenated) data...\")\n",
    "    acc_after, preds_after, labels_after = train_and_evaluate_dnn(dnn_model_aug, train_combined_loader, test_loader, num_epochs, learning_rate, device)\n",
    "    print(f\"DNN Test Accuracy (After Concatenation) for sample size {size}: {acc_after:.4f}\")\n",
    "    dnn_accuracy_after[size] = acc_after\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    [[s, dnn_accuracy_before[s], dnn_accuracy_after[s]] for s in sample_sizes],\n",
    "    columns=[\"Train Samples\", \"Real Only Accuracy\", \"After Concatenation Accuracy\"]\n",
    ")\n",
    "print(\"Accuracy Summary:\")\n",
    "print(tabulate.tabulate(summary_df.values, headers=summary_df.columns, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sample_sizes, list(dnn_accuracy_before.values()), marker='o', linestyle='-', color='b', markersize=8, label=\"Real Only\")\n",
    "plt.plot(sample_sizes, list(dnn_accuracy_after.values()), marker='s', linestyle='--', color='r', markersize=8, label=\"After Concatenation\")\n",
    "plt.xlabel(\"Training Size\", fontsize=14, fontfamily=\"Times New Roman\")\n",
    "plt.ylabel(\"Accuracy\", fontsize=14, fontfamily=\"Times New Roman\")\n",
    "plt.title(\"DNN Accuracy vs. Training Size (Real vs. Augmented)\", fontsize=16, fontfamily=\"Times New Roman\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
