{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tabulate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "latent_dim = 128          \n",
    "condition_dim = 10        \n",
    "gan_epochs = 150            \n",
    "generation_size = 1000\n",
    "\n",
    "full_dataset = load_from_disk('../data/full_dataset_new', keep_in_memory=True)\n",
    "split_datasets = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "test_dataset = split_datasets['test']\n",
    "\n",
    "\n",
    "train_x_full = np.array(train_dataset['embedding'])\n",
    "train_y_full = np.array(train_dataset['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5):\n",
    "        super(DNNClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, condition_dim=10, num_classes=10, start_dim=128, n_layer=3, output_dim=512):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, condition_dim)\n",
    "        input_dim = latent_dim + condition_dim\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, start_dim))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        current_dim = start_dim\n",
    "        for i in range(1, n_layer):\n",
    "            next_dim = current_dim * 2\n",
    "            layers.append(nn.Linear(current_dim, next_dim))\n",
    "            layers.append(nn.BatchNorm1d(next_dim, momentum=0.8))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            current_dim = next_dim\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, z, labels):\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        x = torch.cat([z, label_embedding], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, condition_dim=10, num_classes=10, start_dim=128, n_layer=3, input_dim=512):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, condition_dim)\n",
    "        input_dim = input_dim + condition_dim\n",
    "        hidden_dim = start_dim * (2 ** (n_layer - 1))\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        current_dim = hidden_dim\n",
    "        for i in range(1, n_layer):\n",
    "            next_dim = current_dim // 2\n",
    "            layers.append(nn.Linear(current_dim, next_dim))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            current_dim = next_dim\n",
    "        layers.append(nn.Linear(current_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    def forward(self, x, labels):\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        x = torch.cat([x, label_embedding], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_and_evaluate_dnn(model, train_loader, test_loader, num_epochs, lr, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_y.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}\")\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "    test_acc = accuracy_score(all_labels, all_preds)\n",
    "    return test_acc, all_preds, all_labels\n",
    "\n",
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.embeddings = hf_dataset[\"embedding\"]\n",
    "        self.labels = hf_dataset[\"labels\"]\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    def __getitem__(self, idx):\n",
    "        emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
    "        label = self.labels[idx]\n",
    "        return emb, label\n",
    "\n",
    "train_ds = EmbeddingDataset(train_dataset)\n",
    "test_ds = EmbeddingDataset(test_dataset)\n",
    "\n",
    "def get_loader(dataset, batch_size=32, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Real Data Only] Training size: 20\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.1799, Train Acc=0.2000\n",
      "Epoch 2: Train Loss=0.6302, Train Acc=0.8500\n",
      "Epoch 3: Train Loss=0.4520, Train Acc=0.9000\n",
      "Epoch 4: Train Loss=0.3685, Train Acc=0.9000\n",
      "Epoch 5: Train Loss=0.2514, Train Acc=0.9500\n",
      "Epoch 6: Train Loss=0.1963, Train Acc=0.9500\n",
      "Epoch 7: Train Loss=0.1725, Train Acc=0.9500\n",
      "Epoch 8: Train Loss=0.1280, Train Acc=0.9500\n",
      "Epoch 9: Train Loss=0.1233, Train Acc=1.0000\n",
      "Epoch 10: Train Loss=0.1033, Train Acc=1.0000\n",
      "DNN Test Accuracy (Real Data) for sample size 20: 0.7998\n",
      "[GAN Epoch 1/150] D loss: 0.6943, G loss: 0.6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 2/150] D loss: 0.6401, G loss: 0.6762\n",
      "[GAN Epoch 3/150] D loss: 0.5923, G loss: 0.6804\n",
      "[GAN Epoch 4/150] D loss: 0.5437, G loss: 0.6831\n",
      "[GAN Epoch 5/150] D loss: 0.4953, G loss: 0.6868\n",
      "[GAN Epoch 6/150] D loss: 0.4530, G loss: 0.6875\n",
      "[GAN Epoch 7/150] D loss: 0.4226, G loss: 0.6845\n",
      "[GAN Epoch 8/150] D loss: 0.3968, G loss: 0.6970\n",
      "[GAN Epoch 9/150] D loss: 0.3742, G loss: 0.7268\n",
      "[GAN Epoch 10/150] D loss: 0.3505, G loss: 0.7781\n",
      "[GAN Epoch 11/150] D loss: 0.3318, G loss: 0.8412\n",
      "[GAN Epoch 12/150] D loss: 0.3173, G loss: 0.9018\n",
      "[GAN Epoch 13/150] D loss: 0.3149, G loss: 0.9163\n",
      "[GAN Epoch 14/150] D loss: 0.3228, G loss: 0.9109\n",
      "[GAN Epoch 15/150] D loss: 0.3369, G loss: 0.9190\n",
      "[GAN Epoch 16/150] D loss: 0.3529, G loss: 0.8681\n",
      "[GAN Epoch 17/150] D loss: 0.3476, G loss: 0.9241\n",
      "[GAN Epoch 18/150] D loss: 0.3323, G loss: 0.8699\n",
      "[GAN Epoch 19/150] D loss: 0.2956, G loss: 1.1981\n",
      "[GAN Epoch 20/150] D loss: 0.3582, G loss: 0.7209\n",
      "[GAN Epoch 21/150] D loss: 0.2250, G loss: 1.2385\n",
      "[GAN Epoch 22/150] D loss: 0.2279, G loss: 1.3666\n",
      "[GAN Epoch 23/150] D loss: 0.2649, G loss: 0.9945\n",
      "[GAN Epoch 24/150] D loss: 0.1948, G loss: 1.3688\n",
      "[GAN Epoch 25/150] D loss: 0.1790, G loss: 1.5621\n",
      "[GAN Epoch 26/150] D loss: 0.1773, G loss: 1.3517\n",
      "[GAN Epoch 27/150] D loss: 0.1463, G loss: 1.7196\n",
      "[GAN Epoch 28/150] D loss: 0.1292, G loss: 1.8111\n",
      "[GAN Epoch 29/150] D loss: 0.1685, G loss: 1.5444\n",
      "[GAN Epoch 30/150] D loss: 0.1438, G loss: 2.2756\n",
      "[GAN Epoch 31/150] D loss: 0.2890, G loss: 0.9704\n",
      "[GAN Epoch 32/150] D loss: 0.0763, G loss: 2.5808\n",
      "[GAN Epoch 33/150] D loss: 0.0934, G loss: 2.4129\n",
      "[GAN Epoch 34/150] D loss: 0.2379, G loss: 1.1015\n",
      "[GAN Epoch 35/150] D loss: 0.1949, G loss: 2.6167\n",
      "[GAN Epoch 36/150] D loss: 0.3665, G loss: 0.7477\n",
      "[GAN Epoch 37/150] D loss: 0.0452, G loss: 2.7829\n",
      "[GAN Epoch 38/150] D loss: 0.0462, G loss: 3.1538\n",
      "[GAN Epoch 39/150] D loss: 0.0851, G loss: 2.2315\n",
      "[GAN Epoch 40/150] D loss: 0.1855, G loss: 1.4649\n",
      "[GAN Epoch 41/150] D loss: 0.1583, G loss: 2.7177\n",
      "[GAN Epoch 42/150] D loss: 0.2159, G loss: 1.2179\n",
      "[GAN Epoch 43/150] D loss: 0.0319, G loss: 3.3899\n",
      "[GAN Epoch 44/150] D loss: 0.0341, G loss: 3.7887\n",
      "[GAN Epoch 45/150] D loss: 0.0454, G loss: 2.8756\n",
      "[GAN Epoch 46/150] D loss: 0.1156, G loss: 1.8108\n",
      "[GAN Epoch 47/150] D loss: 0.0880, G loss: 3.1991\n",
      "[GAN Epoch 48/150] D loss: 0.0819, G loss: 2.1965\n",
      "[GAN Epoch 49/150] D loss: 0.0406, G loss: 3.1187\n",
      "[GAN Epoch 50/150] D loss: 0.0370, G loss: 3.3703\n",
      "[GAN Epoch 51/150] D loss: 0.0515, G loss: 2.7400\n",
      "[GAN Epoch 52/150] D loss: 0.0598, G loss: 2.7784\n",
      "[GAN Epoch 53/150] D loss: 0.0537, G loss: 3.2856\n",
      "[GAN Epoch 54/150] D loss: 0.0522, G loss: 2.7819\n",
      "[GAN Epoch 55/150] D loss: 0.0379, G loss: 3.6193\n",
      "[GAN Epoch 56/150] D loss: 0.0354, G loss: 3.3674\n",
      "[GAN Epoch 57/150] D loss: 0.0339, G loss: 3.2792\n",
      "[GAN Epoch 58/150] D loss: 0.0340, G loss: 3.3545\n",
      "[GAN Epoch 59/150] D loss: 0.0396, G loss: 3.1511\n",
      "[GAN Epoch 60/150] D loss: 0.0371, G loss: 3.4682\n",
      "[GAN Epoch 61/150] D loss: 0.0393, G loss: 3.1370\n",
      "[GAN Epoch 62/150] D loss: 0.0319, G loss: 3.8789\n",
      "[GAN Epoch 63/150] D loss: 0.0433, G loss: 2.8073\n",
      "[GAN Epoch 64/150] D loss: 0.0357, G loss: 4.4086\n",
      "[GAN Epoch 65/150] D loss: 0.0673, G loss: 2.6830\n",
      "[GAN Epoch 66/150] D loss: 0.1179, G loss: 5.8912\n",
      "[GAN Epoch 67/150] D loss: 0.5500, G loss: 0.5610\n",
      "[GAN Epoch 68/150] D loss: 0.5181, G loss: 9.5837\n",
      "[GAN Epoch 69/150] D loss: 0.0085, G loss: 4.5167\n",
      "[GAN Epoch 70/150] D loss: 0.5528, G loss: 0.5527\n",
      "[GAN Epoch 71/150] D loss: 0.0040, G loss: 5.2201\n",
      "[GAN Epoch 72/150] D loss: 0.0067, G loss: 6.0144\n",
      "[GAN Epoch 73/150] D loss: 0.0228, G loss: 5.3494\n",
      "[GAN Epoch 74/150] D loss: 0.0380, G loss: 3.6340\n",
      "[GAN Epoch 75/150] D loss: 0.2030, G loss: 1.2786\n",
      "[GAN Epoch 76/150] D loss: 0.4090, G loss: 3.5145\n",
      "[GAN Epoch 77/150] D loss: 1.3600, G loss: 0.1080\n",
      "[GAN Epoch 78/150] D loss: 0.2024, G loss: 4.5485\n",
      "[GAN Epoch 79/150] D loss: 0.0944, G loss: 4.0956\n",
      "[GAN Epoch 80/150] D loss: 0.1229, G loss: 1.8775\n",
      "[GAN Epoch 81/150] D loss: 0.1835, G loss: 1.4854\n",
      "[GAN Epoch 82/150] D loss: 0.3471, G loss: 2.8833\n",
      "[GAN Epoch 83/150] D loss: 0.9224, G loss: 0.2199\n",
      "[GAN Epoch 84/150] D loss: 0.4036, G loss: 3.5372\n",
      "[GAN Epoch 85/150] D loss: 0.1515, G loss: 1.6771\n",
      "[GAN Epoch 86/150] D loss: 0.1214, G loss: 1.8248\n",
      "[GAN Epoch 87/150] D loss: 0.0900, G loss: 2.7358\n",
      "[GAN Epoch 88/150] D loss: 0.1046, G loss: 2.4991\n",
      "[GAN Epoch 89/150] D loss: 0.1477, G loss: 1.8075\n",
      "[GAN Epoch 90/150] D loss: 0.1523, G loss: 2.3216\n",
      "[GAN Epoch 91/150] D loss: 0.2074, G loss: 1.5084\n",
      "[GAN Epoch 92/150] D loss: 0.2325, G loss: 2.7341\n",
      "[GAN Epoch 93/150] D loss: 0.5509, G loss: 0.5350\n",
      "[GAN Epoch 94/150] D loss: 0.7633, G loss: 5.2929\n",
      "[GAN Epoch 95/150] D loss: 0.1921, G loss: 1.2875\n",
      "[GAN Epoch 96/150] D loss: 0.1217, G loss: 1.6887\n",
      "[GAN Epoch 97/150] D loss: 0.0496, G loss: 3.1978\n",
      "[GAN Epoch 98/150] D loss: 0.0641, G loss: 3.5504\n",
      "[GAN Epoch 99/150] D loss: 0.0740, G loss: 2.7116\n",
      "[GAN Epoch 100/150] D loss: 0.1138, G loss: 2.0372\n",
      "[GAN Epoch 101/150] D loss: 0.1169, G loss: 2.4134\n",
      "[GAN Epoch 102/150] D loss: 0.1334, G loss: 2.2496\n",
      "[GAN Epoch 103/150] D loss: 0.1444, G loss: 1.9320\n",
      "[GAN Epoch 104/150] D loss: 0.1334, G loss: 2.5882\n",
      "[GAN Epoch 105/150] D loss: 0.1297, G loss: 1.8904\n",
      "[GAN Epoch 106/150] D loss: 0.1107, G loss: 3.3442\n",
      "[GAN Epoch 107/150] D loss: 0.1233, G loss: 1.7631\n",
      "[GAN Epoch 108/150] D loss: 0.0989, G loss: 3.9779\n",
      "[GAN Epoch 109/150] D loss: 0.0701, G loss: 2.4154\n",
      "[GAN Epoch 110/150] D loss: 0.0650, G loss: 2.7058\n",
      "[GAN Epoch 111/150] D loss: 0.0678, G loss: 3.3812\n",
      "[GAN Epoch 112/150] D loss: 0.1028, G loss: 2.1197\n",
      "[GAN Epoch 113/150] D loss: 0.1214, G loss: 3.9952\n",
      "[GAN Epoch 114/150] D loss: 0.1760, G loss: 1.3764\n",
      "[GAN Epoch 115/150] D loss: 0.2222, G loss: 5.5325\n",
      "[GAN Epoch 116/150] D loss: 0.0564, G loss: 2.5357\n",
      "[GAN Epoch 117/150] D loss: 0.0614, G loss: 2.3638\n",
      "[GAN Epoch 118/150] D loss: 0.0274, G loss: 3.7866\n",
      "[GAN Epoch 119/150] D loss: 0.0402, G loss: 3.9086\n",
      "[GAN Epoch 120/150] D loss: 0.0490, G loss: 3.0036\n",
      "[GAN Epoch 121/150] D loss: 0.0807, G loss: 2.4628\n",
      "[GAN Epoch 122/150] D loss: 0.1049, G loss: 3.7843\n",
      "[GAN Epoch 123/150] D loss: 0.1631, G loss: 1.4685\n",
      "[GAN Epoch 124/150] D loss: 0.3094, G loss: 6.0036\n",
      "[GAN Epoch 125/150] D loss: 0.1375, G loss: 1.6738\n",
      "[GAN Epoch 126/150] D loss: 0.0155, G loss: 4.2114\n",
      "[GAN Epoch 127/150] D loss: 0.0189, G loss: 4.8346\n",
      "[GAN Epoch 128/150] D loss: 0.0214, G loss: 4.5587\n",
      "[GAN Epoch 129/150] D loss: 0.0307, G loss: 3.4821\n",
      "[GAN Epoch 130/150] D loss: 0.0739, G loss: 2.6242\n",
      "[GAN Epoch 131/150] D loss: 0.1056, G loss: 4.0698\n",
      "[GAN Epoch 132/150] D loss: 0.1923, G loss: 1.5591\n",
      "[GAN Epoch 133/150] D loss: 0.5517, G loss: 6.2007\n",
      "[GAN Epoch 134/150] D loss: 0.5265, G loss: 0.6448\n",
      "[GAN Epoch 135/150] D loss: 0.3457, G loss: 7.2305\n",
      "[GAN Epoch 136/150] D loss: 0.0461, G loss: 5.4751\n",
      "[GAN Epoch 137/150] D loss: 0.0360, G loss: 3.2291\n",
      "[GAN Epoch 138/150] D loss: 0.0829, G loss: 2.4634\n",
      "[GAN Epoch 139/150] D loss: 0.0387, G loss: 3.5660\n",
      "[GAN Epoch 140/150] D loss: 0.0696, G loss: 3.5461\n",
      "[GAN Epoch 141/150] D loss: 0.1042, G loss: 2.8669\n",
      "[GAN Epoch 142/150] D loss: 0.1950, G loss: 2.4373\n",
      "[GAN Epoch 143/150] D loss: 0.3058, G loss: 3.9334\n",
      "[GAN Epoch 144/150] D loss: 0.9972, G loss: 0.5442\n",
      "[GAN Epoch 145/150] D loss: 1.0567, G loss: 7.5034\n",
      "[GAN Epoch 146/150] D loss: 0.2012, G loss: 4.4456\n",
      "[GAN Epoch 147/150] D loss: 0.5783, G loss: 0.9233\n",
      "[GAN Epoch 148/150] D loss: 0.1059, G loss: 3.3465\n",
      "[GAN Epoch 149/150] D loss: 0.1484, G loss: 3.7661\n",
      "[GAN Epoch 150/150] D loss: 0.1330, G loss: 2.4045\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1719, Train Acc=0.3265\n",
      "Epoch 2: Train Loss=1.1066, Train Acc=0.3381\n",
      "Epoch 3: Train Loss=1.0989, Train Acc=0.3381\n",
      "Epoch 4: Train Loss=1.0993, Train Acc=0.3417\n",
      "Epoch 5: Train Loss=1.0966, Train Acc=0.3387\n",
      "Epoch 6: Train Loss=1.0962, Train Acc=0.3586\n",
      "Epoch 7: Train Loss=1.0971, Train Acc=0.3401\n",
      "Epoch 8: Train Loss=1.0938, Train Acc=0.3553\n",
      "Epoch 9: Train Loss=1.0944, Train Acc=0.3579\n",
      "Epoch 10: Train Loss=1.0940, Train Acc=0.3493\n",
      "DNN Test Accuracy (After Concatenation) for sample size 20: 0.7749\n",
      "\n",
      "[Real Data Only] Training size: 50\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.2050, Train Acc=0.1600\n",
      "Epoch 2: Train Loss=0.7621, Train Acc=0.7200\n",
      "Epoch 3: Train Loss=0.5349, Train Acc=0.8000\n",
      "Epoch 4: Train Loss=0.4222, Train Acc=0.8400\n",
      "Epoch 5: Train Loss=0.3372, Train Acc=0.9000\n",
      "Epoch 6: Train Loss=0.2964, Train Acc=0.8800\n",
      "Epoch 7: Train Loss=0.2601, Train Acc=0.9400\n",
      "Epoch 8: Train Loss=0.2330, Train Acc=0.9200\n",
      "Epoch 9: Train Loss=0.2070, Train Acc=0.9400\n",
      "Epoch 10: Train Loss=0.2338, Train Acc=0.9000\n",
      "DNN Test Accuracy (Real Data) for sample size 50: 0.8638\n",
      "[GAN Epoch 1/150] D loss: 0.6903, G loss: 0.6680\n",
      "[GAN Epoch 2/150] D loss: 0.6299, G loss: 0.6676\n",
      "[GAN Epoch 3/150] D loss: 0.5765, G loss: 0.6698\n",
      "[GAN Epoch 4/150] D loss: 0.5249, G loss: 0.6752\n",
      "[GAN Epoch 5/150] D loss: 0.4770, G loss: 0.6813\n",
      "[GAN Epoch 6/150] D loss: 0.4365, G loss: 0.6886\n",
      "[GAN Epoch 7/150] D loss: 0.4052, G loss: 0.7004\n",
      "[GAN Epoch 8/150] D loss: 0.3788, G loss: 0.7246\n",
      "[GAN Epoch 9/150] D loss: 0.3506, G loss: 0.7747\n",
      "[GAN Epoch 10/150] D loss: 0.3212, G loss: 0.8506\n",
      "[GAN Epoch 11/150] D loss: 0.2977, G loss: 0.9338\n",
      "[GAN Epoch 12/150] D loss: 0.2865, G loss: 0.9896\n",
      "[GAN Epoch 13/150] D loss: 0.2932, G loss: 0.9736\n",
      "[GAN Epoch 14/150] D loss: 0.3150, G loss: 0.9394\n",
      "[GAN Epoch 15/150] D loss: 0.3478, G loss: 0.8817\n",
      "[GAN Epoch 16/150] D loss: 0.3709, G loss: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 17/150] D loss: 0.3606, G loss: 0.8777\n",
      "[GAN Epoch 18/150] D loss: 0.3190, G loss: 0.9352\n",
      "[GAN Epoch 19/150] D loss: 0.2578, G loss: 1.2033\n",
      "[GAN Epoch 20/150] D loss: 0.2357, G loss: 1.0863\n",
      "[GAN Epoch 21/150] D loss: 0.1917, G loss: 1.4783\n",
      "[GAN Epoch 22/150] D loss: 0.2112, G loss: 1.1868\n",
      "[GAN Epoch 23/150] D loss: 0.1907, G loss: 1.4326\n",
      "[GAN Epoch 24/150] D loss: 0.2033, G loss: 1.3024\n",
      "[GAN Epoch 25/150] D loss: 0.1893, G loss: 1.4590\n",
      "[GAN Epoch 26/150] D loss: 0.1841, G loss: 1.3982\n",
      "[GAN Epoch 27/150] D loss: 0.1576, G loss: 1.8003\n",
      "[GAN Epoch 28/150] D loss: 0.1690, G loss: 1.4571\n",
      "[GAN Epoch 29/150] D loss: 0.1189, G loss: 2.2774\n",
      "[GAN Epoch 30/150] D loss: 0.1608, G loss: 1.4711\n",
      "[GAN Epoch 31/150] D loss: 0.0933, G loss: 2.4516\n",
      "[GAN Epoch 32/150] D loss: 0.1233, G loss: 1.7815\n",
      "[GAN Epoch 33/150] D loss: 0.1301, G loss: 2.1081\n",
      "[GAN Epoch 34/150] D loss: 0.1459, G loss: 1.6478\n",
      "[GAN Epoch 35/150] D loss: 0.1531, G loss: 3.1013\n",
      "[GAN Epoch 36/150] D loss: 0.7018, G loss: 0.3560\n",
      "[GAN Epoch 37/150] D loss: 0.0308, G loss: 3.8857\n",
      "[GAN Epoch 38/150] D loss: 0.0863, G loss: 4.1319\n",
      "[GAN Epoch 39/150] D loss: 0.0984, G loss: 1.9477\n",
      "[GAN Epoch 40/150] D loss: 0.3751, G loss: 0.7562\n",
      "[GAN Epoch 41/150] D loss: 0.2370, G loss: 3.1477\n",
      "[GAN Epoch 42/150] D loss: 0.1718, G loss: 1.3622\n",
      "[GAN Epoch 43/150] D loss: 0.0917, G loss: 1.9616\n",
      "[GAN Epoch 44/150] D loss: 0.0505, G loss: 2.7068\n",
      "[GAN Epoch 45/150] D loss: 0.0534, G loss: 2.8141\n",
      "[GAN Epoch 46/150] D loss: 0.0804, G loss: 2.3378\n",
      "[GAN Epoch 47/150] D loss: 0.0895, G loss: 2.3184\n",
      "[GAN Epoch 48/150] D loss: 0.0702, G loss: 2.7558\n",
      "[GAN Epoch 49/150] D loss: 0.0585, G loss: 2.6447\n",
      "[GAN Epoch 50/150] D loss: 0.0512, G loss: 2.8370\n",
      "[GAN Epoch 51/150] D loss: 0.0462, G loss: 3.0860\n",
      "[GAN Epoch 52/150] D loss: 0.0552, G loss: 2.8422\n",
      "[GAN Epoch 53/150] D loss: 0.0498, G loss: 3.2341\n",
      "[GAN Epoch 54/150] D loss: 0.0565, G loss: 2.6965\n",
      "[GAN Epoch 55/150] D loss: 0.0489, G loss: 3.3510\n",
      "[GAN Epoch 56/150] D loss: 0.0714, G loss: 2.4182\n",
      "[GAN Epoch 57/150] D loss: 0.0724, G loss: 4.2911\n",
      "[GAN Epoch 58/150] D loss: 0.1618, G loss: 1.4887\n",
      "[GAN Epoch 59/150] D loss: 0.0528, G loss: 5.9355\n",
      "[GAN Epoch 60/150] D loss: 0.0144, G loss: 5.5333\n",
      "[GAN Epoch 61/150] D loss: 0.0136, G loss: 4.0497\n",
      "[GAN Epoch 62/150] D loss: 0.0615, G loss: 2.4799\n",
      "[GAN Epoch 63/150] D loss: 0.0291, G loss: 3.7085\n",
      "[GAN Epoch 64/150] D loss: 0.0450, G loss: 3.6265\n",
      "[GAN Epoch 65/150] D loss: 0.0758, G loss: 2.3274\n",
      "[GAN Epoch 66/150] D loss: 0.0707, G loss: 4.2799\n",
      "[GAN Epoch 67/150] D loss: 0.0797, G loss: 2.0784\n",
      "[GAN Epoch 68/150] D loss: 0.0252, G loss: 4.2451\n",
      "[GAN Epoch 69/150] D loss: 0.0334, G loss: 3.9850\n",
      "[GAN Epoch 70/150] D loss: 0.0793, G loss: 2.2418\n",
      "[GAN Epoch 71/150] D loss: 0.1725, G loss: 4.2719\n",
      "[GAN Epoch 72/150] D loss: 1.7856, G loss: 0.0416\n",
      "[GAN Epoch 73/150] D loss: 1.3242, G loss: 8.2050\n",
      "[GAN Epoch 74/150] D loss: 0.1662, G loss: 1.4597\n",
      "[GAN Epoch 75/150] D loss: 0.3427, G loss: 0.8205\n",
      "[GAN Epoch 76/150] D loss: 0.0188, G loss: 4.1094\n",
      "[GAN Epoch 77/150] D loss: 0.0424, G loss: 4.8653\n",
      "[GAN Epoch 78/150] D loss: 0.0517, G loss: 4.1114\n",
      "[GAN Epoch 79/150] D loss: 0.0854, G loss: 2.3391\n",
      "[GAN Epoch 80/150] D loss: 0.2307, G loss: 1.2120\n",
      "[GAN Epoch 81/150] D loss: 0.3807, G loss: 2.8602\n",
      "[GAN Epoch 82/150] D loss: 0.9443, G loss: 0.2033\n",
      "[GAN Epoch 83/150] D loss: 0.8582, G loss: 3.7780\n",
      "[GAN Epoch 84/150] D loss: 0.6648, G loss: 0.3700\n",
      "[GAN Epoch 85/150] D loss: 0.1920, G loss: 2.0269\n",
      "[GAN Epoch 86/150] D loss: 0.2188, G loss: 2.5914\n",
      "[GAN Epoch 87/150] D loss: 0.2164, G loss: 1.4130\n",
      "[GAN Epoch 88/150] D loss: 0.2060, G loss: 1.6059\n",
      "[GAN Epoch 89/150] D loss: 0.2068, G loss: 2.1420\n",
      "[GAN Epoch 90/150] D loss: 0.2243, G loss: 1.4787\n",
      "[GAN Epoch 91/150] D loss: 0.2079, G loss: 1.9169\n",
      "[GAN Epoch 92/150] D loss: 0.1974, G loss: 1.7856\n",
      "[GAN Epoch 93/150] D loss: 0.1879, G loss: 1.8388\n",
      "[GAN Epoch 94/150] D loss: 0.1728, G loss: 2.1053\n",
      "[GAN Epoch 95/150] D loss: 0.1808, G loss: 1.7359\n",
      "[GAN Epoch 96/150] D loss: 0.1829, G loss: 2.7788\n",
      "[GAN Epoch 97/150] D loss: 0.3301, G loss: 0.8935\n",
      "[GAN Epoch 98/150] D loss: 0.5666, G loss: 5.1428\n",
      "[GAN Epoch 99/150] D loss: 0.2476, G loss: 1.0737\n",
      "[GAN Epoch 100/150] D loss: 0.0591, G loss: 2.7636\n",
      "[GAN Epoch 101/150] D loss: 0.0601, G loss: 3.5814\n",
      "[GAN Epoch 102/150] D loss: 0.0725, G loss: 2.8851\n",
      "[GAN Epoch 103/150] D loss: 0.1307, G loss: 1.8904\n",
      "[GAN Epoch 104/150] D loss: 0.1363, G loss: 2.5139\n",
      "[GAN Epoch 105/150] D loss: 0.1568, G loss: 1.9689\n",
      "[GAN Epoch 106/150] D loss: 0.1495, G loss: 2.1524\n",
      "[GAN Epoch 107/150] D loss: 0.1364, G loss: 2.1742\n",
      "[GAN Epoch 108/150] D loss: 0.1124, G loss: 2.4667\n",
      "[GAN Epoch 109/150] D loss: 0.0988, G loss: 2.4401\n",
      "[GAN Epoch 110/150] D loss: 0.0857, G loss: 2.7995\n",
      "[GAN Epoch 111/150] D loss: 0.0841, G loss: 2.5266\n",
      "[GAN Epoch 112/150] D loss: 0.0801, G loss: 2.8662\n",
      "[GAN Epoch 113/150] D loss: 0.0845, G loss: 2.5591\n",
      "[GAN Epoch 114/150] D loss: 0.0835, G loss: 2.9549\n",
      "[GAN Epoch 115/150] D loss: 0.0871, G loss: 2.4050\n",
      "[GAN Epoch 116/150] D loss: 0.0828, G loss: 3.3296\n",
      "[GAN Epoch 117/150] D loss: 0.1111, G loss: 1.9450\n",
      "[GAN Epoch 118/150] D loss: 0.1410, G loss: 4.5720\n",
      "[GAN Epoch 119/150] D loss: 0.1273, G loss: 1.7098\n",
      "[GAN Epoch 120/150] D loss: 0.0636, G loss: 4.9586\n",
      "[GAN Epoch 121/150] D loss: 0.0346, G loss: 4.1495\n",
      "[GAN Epoch 122/150] D loss: 0.0532, G loss: 2.7027\n",
      "[GAN Epoch 123/150] D loss: 0.0558, G loss: 2.9731\n",
      "[GAN Epoch 124/150] D loss: 0.0738, G loss: 3.8306\n",
      "[GAN Epoch 125/150] D loss: 0.0903, G loss: 2.2959\n",
      "[GAN Epoch 126/150] D loss: 0.0932, G loss: 4.0039\n",
      "[GAN Epoch 127/150] D loss: 0.1588, G loss: 1.9370\n",
      "[GAN Epoch 128/150] D loss: 0.2819, G loss: 6.4745\n",
      "[GAN Epoch 129/150] D loss: 0.1016, G loss: 2.3138\n",
      "[GAN Epoch 130/150] D loss: 0.0230, G loss: 3.9659\n",
      "[GAN Epoch 131/150] D loss: 0.0249, G loss: 4.4675\n",
      "[GAN Epoch 132/150] D loss: 0.0474, G loss: 3.4735\n",
      "[GAN Epoch 133/150] D loss: 0.0782, G loss: 3.1647\n",
      "[GAN Epoch 134/150] D loss: 0.1105, G loss: 3.7333\n",
      "[GAN Epoch 135/150] D loss: 0.2319, G loss: 1.6215\n",
      "[GAN Epoch 136/150] D loss: 0.7176, G loss: 7.6136\n",
      "[GAN Epoch 137/150] D loss: 0.3628, G loss: 1.2766\n",
      "[GAN Epoch 138/150] D loss: 0.2209, G loss: 7.4225\n",
      "[GAN Epoch 139/150] D loss: 0.0804, G loss: 6.2082\n",
      "[GAN Epoch 140/150] D loss: 0.0536, G loss: 3.1602\n",
      "[GAN Epoch 141/150] D loss: 0.1331, G loss: 1.9491\n",
      "[GAN Epoch 142/150] D loss: 0.1669, G loss: 5.1931\n",
      "[GAN Epoch 143/150] D loss: 0.1261, G loss: 2.7695\n",
      "[GAN Epoch 144/150] D loss: 0.1901, G loss: 2.0270\n",
      "[GAN Epoch 145/150] D loss: 0.3653, G loss: 4.6058\n",
      "[GAN Epoch 146/150] D loss: 0.8298, G loss: 0.4995\n",
      "[GAN Epoch 147/150] D loss: 1.2914, G loss: 9.3149\n",
      "[GAN Epoch 148/150] D loss: 0.2464, G loss: 5.8604\n",
      "[GAN Epoch 149/150] D loss: 0.3001, G loss: 1.3116\n",
      "[GAN Epoch 150/150] D loss: 0.0644, G loss: 3.2943\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1646, Train Acc=0.3197\n",
      "Epoch 2: Train Loss=1.1040, Train Acc=0.3403\n",
      "Epoch 3: Train Loss=1.0978, Train Acc=0.3416\n",
      "Epoch 4: Train Loss=1.0948, Train Acc=0.3275\n",
      "Epoch 5: Train Loss=1.0924, Train Acc=0.3495\n",
      "Epoch 6: Train Loss=1.0918, Train Acc=0.3495\n",
      "Epoch 7: Train Loss=1.0897, Train Acc=0.3557\n",
      "Epoch 8: Train Loss=1.0892, Train Acc=0.3528\n",
      "Epoch 9: Train Loss=1.0872, Train Acc=0.3564\n",
      "Epoch 10: Train Loss=1.0852, Train Acc=0.3623\n",
      "DNN Test Accuracy (After Concatenation) for sample size 50: 0.8018\n",
      "\n",
      "[Real Data Only] Training size: 70\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=0.9962, Train Acc=0.5571\n",
      "Epoch 2: Train Loss=0.6424, Train Acc=0.8571\n",
      "Epoch 3: Train Loss=0.4671, Train Acc=0.8571\n",
      "Epoch 4: Train Loss=0.3709, Train Acc=0.9000\n",
      "Epoch 5: Train Loss=0.3039, Train Acc=0.8857\n",
      "Epoch 6: Train Loss=0.2661, Train Acc=0.9286\n",
      "Epoch 7: Train Loss=0.2798, Train Acc=0.8857\n",
      "Epoch 8: Train Loss=0.3015, Train Acc=0.8714\n",
      "Epoch 9: Train Loss=0.3097, Train Acc=0.8857\n",
      "Epoch 10: Train Loss=0.2690, Train Acc=0.8714\n",
      "DNN Test Accuracy (Real Data) for sample size 70: 0.8796\n",
      "[GAN Epoch 1/150] D loss: 0.6885, G loss: 0.7012\n",
      "[GAN Epoch 2/150] D loss: 0.6243, G loss: 0.7001\n",
      "[GAN Epoch 3/150] D loss: 0.5686, G loss: 0.6994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 4/150] D loss: 0.5157, G loss: 0.6974\n",
      "[GAN Epoch 5/150] D loss: 0.4685, G loss: 0.6950\n",
      "[GAN Epoch 6/150] D loss: 0.4322, G loss: 0.6910\n",
      "[GAN Epoch 7/150] D loss: 0.4041, G loss: 0.6963\n",
      "[GAN Epoch 8/150] D loss: 0.3801, G loss: 0.7173\n",
      "[GAN Epoch 9/150] D loss: 0.3525, G loss: 0.7677\n",
      "[GAN Epoch 10/150] D loss: 0.3224, G loss: 0.8465\n",
      "[GAN Epoch 11/150] D loss: 0.2999, G loss: 0.9278\n",
      "[GAN Epoch 12/150] D loss: 0.2867, G loss: 0.9973\n",
      "[GAN Epoch 13/150] D loss: 0.2976, G loss: 0.9751\n",
      "[GAN Epoch 14/150] D loss: 0.3313, G loss: 0.9051\n",
      "[GAN Epoch 15/150] D loss: 0.3824, G loss: 0.8252\n",
      "[GAN Epoch 16/150] D loss: 0.4206, G loss: 0.7311\n",
      "[GAN Epoch 17/150] D loss: 0.4150, G loss: 0.7893\n",
      "[GAN Epoch 18/150] D loss: 0.3755, G loss: 0.7759\n",
      "[GAN Epoch 19/150] D loss: 0.3109, G loss: 1.1083\n",
      "[GAN Epoch 20/150] D loss: 0.2908, G loss: 0.9091\n",
      "[GAN Epoch 21/150] D loss: 0.2100, G loss: 1.3119\n",
      "[GAN Epoch 22/150] D loss: 0.1987, G loss: 1.3802\n",
      "[GAN Epoch 23/150] D loss: 0.2119, G loss: 1.2247\n",
      "[GAN Epoch 24/150] D loss: 0.2102, G loss: 1.3618\n",
      "[GAN Epoch 25/150] D loss: 0.2097, G loss: 1.3105\n",
      "[GAN Epoch 26/150] D loss: 0.2027, G loss: 1.3774\n",
      "[GAN Epoch 27/150] D loss: 0.1926, G loss: 1.4341\n",
      "[GAN Epoch 28/150] D loss: 0.1859, G loss: 1.4949\n",
      "[GAN Epoch 29/150] D loss: 0.1726, G loss: 1.6314\n",
      "[GAN Epoch 30/150] D loss: 0.1549, G loss: 1.6494\n",
      "[GAN Epoch 31/150] D loss: 0.1459, G loss: 1.9258\n",
      "[GAN Epoch 32/150] D loss: 0.1627, G loss: 1.5188\n",
      "[GAN Epoch 33/150] D loss: 0.1792, G loss: 2.6874\n",
      "[GAN Epoch 34/150] D loss: 0.7353, G loss: 0.3118\n",
      "[GAN Epoch 35/150] D loss: 0.0478, G loss: 2.8205\n",
      "[GAN Epoch 36/150] D loss: 0.0870, G loss: 3.1696\n",
      "[GAN Epoch 37/150] D loss: 0.1448, G loss: 1.6897\n",
      "[GAN Epoch 38/150] D loss: 0.2379, G loss: 1.2013\n",
      "[GAN Epoch 39/150] D loss: 0.2186, G loss: 2.3744\n",
      "[GAN Epoch 40/150] D loss: 0.3022, G loss: 0.9099\n",
      "[GAN Epoch 41/150] D loss: 0.0561, G loss: 2.9113\n",
      "[GAN Epoch 42/150] D loss: 0.0648, G loss: 3.0618\n",
      "[GAN Epoch 43/150] D loss: 0.1084, G loss: 1.9076\n",
      "[GAN Epoch 44/150] D loss: 0.1234, G loss: 1.8601\n",
      "[GAN Epoch 45/150] D loss: 0.1181, G loss: 2.4573\n",
      "[GAN Epoch 46/150] D loss: 0.1287, G loss: 1.7178\n",
      "[GAN Epoch 47/150] D loss: 0.0687, G loss: 3.1998\n",
      "[GAN Epoch 48/150] D loss: 0.0611, G loss: 2.5389\n",
      "[GAN Epoch 49/150] D loss: 0.0713, G loss: 2.3224\n",
      "[GAN Epoch 50/150] D loss: 0.0643, G loss: 3.0095\n",
      "[GAN Epoch 51/150] D loss: 0.0734, G loss: 2.4575\n",
      "[GAN Epoch 52/150] D loss: 0.0571, G loss: 3.0143\n",
      "[GAN Epoch 53/150] D loss: 0.0652, G loss: 2.5149\n",
      "[GAN Epoch 54/150] D loss: 0.0559, G loss: 3.1688\n",
      "[GAN Epoch 55/150] D loss: 0.0638, G loss: 2.5302\n",
      "[GAN Epoch 56/150] D loss: 0.0551, G loss: 3.4286\n",
      "[GAN Epoch 57/150] D loss: 0.0782, G loss: 2.2266\n",
      "[GAN Epoch 58/150] D loss: 0.0738, G loss: 4.2779\n",
      "[GAN Epoch 59/150] D loss: 0.1043, G loss: 1.8552\n",
      "[GAN Epoch 60/150] D loss: 0.0315, G loss: 4.7618\n",
      "[GAN Epoch 61/150] D loss: 0.0264, G loss: 4.4879\n",
      "[GAN Epoch 62/150] D loss: 0.0515, G loss: 2.6499\n",
      "[GAN Epoch 63/150] D loss: 0.0492, G loss: 2.9148\n",
      "[GAN Epoch 64/150] D loss: 0.0760, G loss: 2.9865\n",
      "[GAN Epoch 65/150] D loss: 0.1723, G loss: 1.3993\n",
      "[GAN Epoch 66/150] D loss: 1.2918, G loss: 6.5112\n",
      "[GAN Epoch 67/150] D loss: 2.6688, G loss: 0.0066\n",
      "[GAN Epoch 68/150] D loss: 1.3392, G loss: 0.0892\n",
      "[GAN Epoch 69/150] D loss: 0.0191, G loss: 4.4093\n",
      "[GAN Epoch 70/150] D loss: 0.1395, G loss: 5.7701\n",
      "[GAN Epoch 71/150] D loss: 0.0957, G loss: 4.6848\n",
      "[GAN Epoch 72/150] D loss: 0.0650, G loss: 2.8132\n",
      "[GAN Epoch 73/150] D loss: 0.2293, G loss: 1.1661\n",
      "[GAN Epoch 74/150] D loss: 0.2158, G loss: 1.3879\n",
      "[GAN Epoch 75/150] D loss: 0.2876, G loss: 2.2758\n",
      "[GAN Epoch 76/150] D loss: 0.2618, G loss: 1.2174\n",
      "[GAN Epoch 77/150] D loss: 0.1809, G loss: 2.0770\n",
      "[GAN Epoch 78/150] D loss: 0.1788, G loss: 1.9134\n",
      "[GAN Epoch 79/150] D loss: 0.2290, G loss: 1.4734\n",
      "[GAN Epoch 80/150] D loss: 0.3059, G loss: 1.9294\n",
      "[GAN Epoch 81/150] D loss: 0.6725, G loss: 0.3954\n",
      "[GAN Epoch 82/150] D loss: 1.2767, G loss: 3.6306\n",
      "[GAN Epoch 83/150] D loss: 1.0564, G loss: 0.1488\n",
      "[GAN Epoch 84/150] D loss: 0.4512, G loss: 0.6380\n",
      "[GAN Epoch 85/150] D loss: 0.4175, G loss: 2.7237\n",
      "[GAN Epoch 86/150] D loss: 0.2126, G loss: 2.1183\n",
      "[GAN Epoch 87/150] D loss: 0.2155, G loss: 1.2884\n",
      "[GAN Epoch 88/150] D loss: 0.1817, G loss: 1.5113\n",
      "[GAN Epoch 89/150] D loss: 0.1538, G loss: 2.0738\n",
      "[GAN Epoch 90/150] D loss: 0.1593, G loss: 2.0956\n",
      "[GAN Epoch 91/150] D loss: 0.1806, G loss: 1.6790\n",
      "[GAN Epoch 92/150] D loss: 0.1968, G loss: 1.6747\n",
      "[GAN Epoch 93/150] D loss: 0.2196, G loss: 1.7951\n",
      "[GAN Epoch 94/150] D loss: 0.2379, G loss: 1.5303\n",
      "[GAN Epoch 95/150] D loss: 0.2380, G loss: 1.7217\n",
      "[GAN Epoch 96/150] D loss: 0.2260, G loss: 1.5992\n",
      "[GAN Epoch 97/150] D loss: 0.1976, G loss: 1.9971\n",
      "[GAN Epoch 98/150] D loss: 0.1856, G loss: 1.6378\n",
      "[GAN Epoch 99/150] D loss: 0.1590, G loss: 2.6002\n",
      "[GAN Epoch 100/150] D loss: 0.1709, G loss: 1.5851\n",
      "[GAN Epoch 101/150] D loss: 0.1406, G loss: 3.1589\n",
      "[GAN Epoch 102/150] D loss: 0.1319, G loss: 1.8366\n",
      "[GAN Epoch 103/150] D loss: 0.0979, G loss: 2.8186\n",
      "[GAN Epoch 104/150] D loss: 0.0970, G loss: 2.4237\n",
      "[GAN Epoch 105/150] D loss: 0.1074, G loss: 2.2057\n",
      "[GAN Epoch 106/150] D loss: 0.1090, G loss: 2.7022\n",
      "[GAN Epoch 107/150] D loss: 0.1164, G loss: 2.0876\n",
      "[GAN Epoch 108/150] D loss: 0.1067, G loss: 3.0549\n",
      "[GAN Epoch 109/150] D loss: 0.1131, G loss: 1.9483\n",
      "[GAN Epoch 110/150] D loss: 0.0971, G loss: 3.3828\n",
      "[GAN Epoch 111/150] D loss: 0.0851, G loss: 2.2968\n",
      "[GAN Epoch 112/150] D loss: 0.0725, G loss: 2.8052\n",
      "[GAN Epoch 113/150] D loss: 0.0800, G loss: 2.6844\n",
      "[GAN Epoch 114/150] D loss: 0.0873, G loss: 2.5104\n",
      "[GAN Epoch 115/150] D loss: 0.0838, G loss: 2.8254\n",
      "[GAN Epoch 116/150] D loss: 0.0858, G loss: 2.4096\n",
      "[GAN Epoch 117/150] D loss: 0.0772, G loss: 3.1516\n",
      "[GAN Epoch 118/150] D loss: 0.0776, G loss: 2.4334\n",
      "[GAN Epoch 119/150] D loss: 0.0664, G loss: 3.3271\n",
      "[GAN Epoch 120/150] D loss: 0.0682, G loss: 2.6349\n",
      "[GAN Epoch 121/150] D loss: 0.0588, G loss: 3.2418\n",
      "[GAN Epoch 122/150] D loss: 0.0620, G loss: 2.8121\n",
      "[GAN Epoch 123/150] D loss: 0.0589, G loss: 3.0869\n",
      "[GAN Epoch 124/150] D loss: 0.0629, G loss: 2.9026\n",
      "[GAN Epoch 125/150] D loss: 0.0582, G loss: 3.1926\n",
      "[GAN Epoch 126/150] D loss: 0.0597, G loss: 2.8320\n",
      "[GAN Epoch 127/150] D loss: 0.0568, G loss: 3.2937\n",
      "[GAN Epoch 128/150] D loss: 0.0590, G loss: 2.9347\n",
      "[GAN Epoch 129/150] D loss: 0.0566, G loss: 3.4168\n",
      "[GAN Epoch 130/150] D loss: 0.0559, G loss: 2.9743\n",
      "[GAN Epoch 131/150] D loss: 0.0527, G loss: 3.4255\n",
      "[GAN Epoch 132/150] D loss: 0.0651, G loss: 2.8246\n",
      "[GAN Epoch 133/150] D loss: 0.0679, G loss: 4.0784\n",
      "[GAN Epoch 134/150] D loss: 0.0820, G loss: 2.3401\n",
      "[GAN Epoch 135/150] D loss: 0.0931, G loss: 5.1965\n",
      "[GAN Epoch 136/150] D loss: 0.0511, G loss: 2.8677\n",
      "[GAN Epoch 137/150] D loss: 0.0280, G loss: 3.7868\n",
      "[GAN Epoch 138/150] D loss: 0.0326, G loss: 3.8593\n",
      "[GAN Epoch 139/150] D loss: 0.0450, G loss: 3.2337\n",
      "[GAN Epoch 140/150] D loss: 0.0521, G loss: 3.3615\n",
      "[GAN Epoch 141/150] D loss: 0.0666, G loss: 2.9906\n",
      "[GAN Epoch 142/150] D loss: 0.0677, G loss: 3.8432\n",
      "[GAN Epoch 143/150] D loss: 0.1235, G loss: 1.9947\n",
      "[GAN Epoch 144/150] D loss: 0.3891, G loss: 7.5304\n",
      "[GAN Epoch 145/150] D loss: 0.2122, G loss: 1.4743\n",
      "[GAN Epoch 146/150] D loss: 0.0259, G loss: 7.1248\n",
      "[GAN Epoch 147/150] D loss: 0.0581, G loss: 8.2819\n",
      "[GAN Epoch 148/150] D loss: 0.0191, G loss: 6.7930\n",
      "[GAN Epoch 149/150] D loss: 0.0096, G loss: 4.8993\n",
      "[GAN Epoch 150/150] D loss: 0.0431, G loss: 3.0175\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1350, Train Acc=0.3329\n",
      "Epoch 2: Train Loss=1.1040, Train Acc=0.3397\n",
      "Epoch 3: Train Loss=1.0941, Train Acc=0.3511\n",
      "Epoch 4: Train Loss=1.0917, Train Acc=0.3319\n",
      "Epoch 5: Train Loss=1.0877, Train Acc=0.3440\n",
      "Epoch 6: Train Loss=1.0858, Train Acc=0.3547\n",
      "Epoch 7: Train Loss=1.0845, Train Acc=0.3577\n",
      "Epoch 8: Train Loss=1.0855, Train Acc=0.3339\n",
      "Epoch 9: Train Loss=1.0827, Train Acc=0.3612\n",
      "Epoch 10: Train Loss=1.0818, Train Acc=0.3570\n",
      "DNN Test Accuracy (After Concatenation) for sample size 70: 0.8765\n",
      "\n",
      "[Real Data Only] Training size: 100\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.0319, Train Acc=0.4900\n",
      "Epoch 2: Train Loss=0.6339, Train Acc=0.9000\n",
      "Epoch 3: Train Loss=0.4844, Train Acc=0.8800\n",
      "Epoch 4: Train Loss=0.3942, Train Acc=0.8500\n",
      "Epoch 5: Train Loss=0.3398, Train Acc=0.9000\n",
      "Epoch 6: Train Loss=0.2985, Train Acc=0.9000\n",
      "Epoch 7: Train Loss=0.2947, Train Acc=0.8900\n",
      "Epoch 8: Train Loss=0.2893, Train Acc=0.9100\n",
      "Epoch 9: Train Loss=0.2953, Train Acc=0.9200\n",
      "Epoch 10: Train Loss=0.2903, Train Acc=0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Test Accuracy (Real Data) for sample size 100: 0.8857\n",
      "[GAN Epoch 1/150] D loss: 0.6917, G loss: 0.6911\n",
      "[GAN Epoch 2/150] D loss: 0.6280, G loss: 0.6918\n",
      "[GAN Epoch 3/150] D loss: 0.5712, G loss: 0.6962\n",
      "[GAN Epoch 4/150] D loss: 0.5173, G loss: 0.7017\n",
      "[GAN Epoch 5/150] D loss: 0.4673, G loss: 0.7079\n",
      "[GAN Epoch 6/150] D loss: 0.4263, G loss: 0.7136\n",
      "[GAN Epoch 7/150] D loss: 0.3949, G loss: 0.7242\n",
      "[GAN Epoch 8/150] D loss: 0.3699, G loss: 0.7467\n",
      "[GAN Epoch 9/150] D loss: 0.3462, G loss: 0.7892\n",
      "[GAN Epoch 10/150] D loss: 0.3265, G loss: 0.8440\n",
      "[GAN Epoch 11/150] D loss: 0.3143, G loss: 0.9053\n",
      "[GAN Epoch 12/150] D loss: 0.3156, G loss: 0.9370\n",
      "[GAN Epoch 13/150] D loss: 0.3295, G loss: 0.9115\n",
      "[GAN Epoch 14/150] D loss: 0.3563, G loss: 0.8632\n",
      "[GAN Epoch 15/150] D loss: 0.3725, G loss: 0.8672\n",
      "[GAN Epoch 16/150] D loss: 0.3711, G loss: 0.8217\n",
      "[GAN Epoch 17/150] D loss: 0.3388, G loss: 0.9935\n",
      "[GAN Epoch 18/150] D loss: 0.3176, G loss: 0.8686\n",
      "[GAN Epoch 19/150] D loss: 0.2741, G loss: 1.2570\n",
      "[GAN Epoch 20/150] D loss: 0.2813, G loss: 0.9486\n",
      "[GAN Epoch 21/150] D loss: 0.2370, G loss: 1.2093\n",
      "[GAN Epoch 22/150] D loss: 0.2418, G loss: 1.2500\n",
      "[GAN Epoch 23/150] D loss: 0.2626, G loss: 1.0569\n",
      "[GAN Epoch 24/150] D loss: 0.2277, G loss: 1.3472\n",
      "[GAN Epoch 25/150] D loss: 0.2316, G loss: 1.1922\n",
      "[GAN Epoch 26/150] D loss: 0.1941, G loss: 1.5057\n",
      "[GAN Epoch 27/150] D loss: 0.1893, G loss: 1.3909\n",
      "[GAN Epoch 28/150] D loss: 0.1683, G loss: 1.6627\n",
      "[GAN Epoch 29/150] D loss: 0.1743, G loss: 1.4891\n",
      "[GAN Epoch 30/150] D loss: 0.1559, G loss: 1.9640\n",
      "[GAN Epoch 31/150] D loss: 0.2130, G loss: 1.2696\n",
      "[GAN Epoch 32/150] D loss: 0.1528, G loss: 2.6766\n",
      "[GAN Epoch 33/150] D loss: 0.3110, G loss: 0.8861\n",
      "[GAN Epoch 34/150] D loss: 0.0705, G loss: 2.7216\n",
      "[GAN Epoch 35/150] D loss: 0.1010, G loss: 2.4266\n",
      "[GAN Epoch 36/150] D loss: 0.2561, G loss: 1.1275\n",
      "[GAN Epoch 37/150] D loss: 0.2144, G loss: 2.8208\n",
      "[GAN Epoch 38/150] D loss: 0.4041, G loss: 0.7143\n",
      "[GAN Epoch 39/150] D loss: 0.0428, G loss: 3.0633\n",
      "[GAN Epoch 40/150] D loss: 0.0587, G loss: 3.2367\n",
      "[GAN Epoch 41/150] D loss: 0.1179, G loss: 1.8724\n",
      "[GAN Epoch 42/150] D loss: 0.1875, G loss: 1.4377\n",
      "[GAN Epoch 43/150] D loss: 0.1702, G loss: 2.6823\n",
      "[GAN Epoch 44/150] D loss: 0.2084, G loss: 1.2563\n",
      "[GAN Epoch 45/150] D loss: 0.0402, G loss: 3.3295\n",
      "[GAN Epoch 46/150] D loss: 0.0425, G loss: 3.5563\n",
      "[GAN Epoch 47/150] D loss: 0.0576, G loss: 2.6230\n",
      "[GAN Epoch 48/150] D loss: 0.1069, G loss: 1.9248\n",
      "[GAN Epoch 49/150] D loss: 0.0826, G loss: 3.1384\n",
      "[GAN Epoch 50/150] D loss: 0.0760, G loss: 2.4010\n",
      "[GAN Epoch 51/150] D loss: 0.0559, G loss: 2.8166\n",
      "[GAN Epoch 52/150] D loss: 0.0519, G loss: 3.0444\n",
      "[GAN Epoch 53/150] D loss: 0.0596, G loss: 2.7600\n",
      "[GAN Epoch 54/150] D loss: 0.0577, G loss: 2.8948\n",
      "[GAN Epoch 55/150] D loss: 0.0641, G loss: 2.7941\n",
      "[GAN Epoch 56/150] D loss: 0.0611, G loss: 3.0133\n",
      "[GAN Epoch 57/150] D loss: 0.0601, G loss: 2.8445\n",
      "[GAN Epoch 58/150] D loss: 0.0542, G loss: 3.1969\n",
      "[GAN Epoch 59/150] D loss: 0.0584, G loss: 2.7370\n",
      "[GAN Epoch 60/150] D loss: 0.0551, G loss: 3.4924\n",
      "[GAN Epoch 61/150] D loss: 0.0746, G loss: 2.3309\n",
      "[GAN Epoch 62/150] D loss: 0.0818, G loss: 4.7181\n",
      "[GAN Epoch 63/150] D loss: 0.1097, G loss: 1.8183\n",
      "[GAN Epoch 64/150] D loss: 0.0189, G loss: 5.2625\n",
      "[GAN Epoch 65/150] D loss: 0.0279, G loss: 5.4977\n",
      "[GAN Epoch 66/150] D loss: 0.0210, G loss: 3.7798\n",
      "[GAN Epoch 67/150] D loss: 0.0840, G loss: 2.1646\n",
      "[GAN Epoch 68/150] D loss: 0.1233, G loss: 4.5400\n",
      "[GAN Epoch 69/150] D loss: 0.5118, G loss: 0.5884\n",
      "[GAN Epoch 70/150] D loss: 1.4205, G loss: 8.2981\n",
      "[GAN Epoch 71/150] D loss: 0.4306, G loss: 0.6704\n",
      "[GAN Epoch 72/150] D loss: 0.0996, G loss: 1.8988\n",
      "[GAN Epoch 73/150] D loss: 0.0337, G loss: 3.0826\n",
      "[GAN Epoch 74/150] D loss: 0.0667, G loss: 2.6949\n",
      "[GAN Epoch 75/150] D loss: 0.1668, G loss: 1.8283\n",
      "[GAN Epoch 76/150] D loss: 0.2423, G loss: 1.9977\n",
      "[GAN Epoch 77/150] D loss: 0.2189, G loss: 1.5805\n",
      "[GAN Epoch 78/150] D loss: 0.1922, G loss: 3.0189\n",
      "[GAN Epoch 79/150] D loss: 0.4088, G loss: 0.7823\n",
      "[GAN Epoch 80/150] D loss: 0.9591, G loss: 5.5161\n",
      "[GAN Epoch 81/150] D loss: 1.0630, G loss: 0.1644\n",
      "[GAN Epoch 82/150] D loss: 0.1522, G loss: 3.2247\n",
      "[GAN Epoch 83/150] D loss: 0.2248, G loss: 2.9703\n",
      "[GAN Epoch 84/150] D loss: 0.4320, G loss: 0.7468\n",
      "[GAN Epoch 85/150] D loss: 0.3339, G loss: 1.9497\n",
      "[GAN Epoch 86/150] D loss: 0.3271, G loss: 1.3452\n",
      "[GAN Epoch 87/150] D loss: 0.2656, G loss: 1.5668\n",
      "[GAN Epoch 88/150] D loss: 0.1918, G loss: 2.1263\n",
      "[GAN Epoch 89/150] D loss: 0.1483, G loss: 2.0700\n",
      "[GAN Epoch 90/150] D loss: 0.1293, G loss: 2.1316\n",
      "[GAN Epoch 91/150] D loss: 0.1305, G loss: 2.2458\n",
      "[GAN Epoch 92/150] D loss: 0.1457, G loss: 2.1926\n",
      "[GAN Epoch 93/150] D loss: 0.1695, G loss: 2.0054\n",
      "[GAN Epoch 94/150] D loss: 0.1882, G loss: 1.9841\n",
      "[GAN Epoch 95/150] D loss: 0.2030, G loss: 1.8332\n",
      "[GAN Epoch 96/150] D loss: 0.1870, G loss: 2.2657\n",
      "[GAN Epoch 97/150] D loss: 0.1999, G loss: 1.5320\n",
      "[GAN Epoch 98/150] D loss: 0.2312, G loss: 3.7921\n",
      "[GAN Epoch 99/150] D loss: 0.2771, G loss: 1.0478\n",
      "[GAN Epoch 100/150] D loss: 0.1896, G loss: 4.8429\n",
      "[GAN Epoch 101/150] D loss: 0.0570, G loss: 3.6847\n",
      "[GAN Epoch 102/150] D loss: 0.1158, G loss: 1.8753\n",
      "[GAN Epoch 103/150] D loss: 0.0623, G loss: 3.0609\n",
      "[GAN Epoch 104/150] D loss: 0.0880, G loss: 2.9722\n",
      "[GAN Epoch 105/150] D loss: 0.1276, G loss: 2.0714\n",
      "[GAN Epoch 106/150] D loss: 0.1353, G loss: 2.7380\n",
      "[GAN Epoch 107/150] D loss: 0.1526, G loss: 1.8764\n",
      "[GAN Epoch 108/150] D loss: 0.1437, G loss: 3.5094\n",
      "[GAN Epoch 109/150] D loss: 0.1391, G loss: 1.8587\n",
      "[GAN Epoch 110/150] D loss: 0.0990, G loss: 3.9002\n",
      "[GAN Epoch 111/150] D loss: 0.0639, G loss: 2.9811\n",
      "[GAN Epoch 112/150] D loss: 0.0897, G loss: 2.3417\n",
      "[GAN Epoch 113/150] D loss: 0.0782, G loss: 3.5395\n",
      "[GAN Epoch 114/150] D loss: 0.0958, G loss: 2.5940\n",
      "[GAN Epoch 115/150] D loss: 0.0919, G loss: 2.9891\n",
      "[GAN Epoch 116/150] D loss: 0.0966, G loss: 2.6996\n",
      "[GAN Epoch 117/150] D loss: 0.1121, G loss: 2.6882\n",
      "[GAN Epoch 118/150] D loss: 0.1073, G loss: 3.3244\n",
      "[GAN Epoch 119/150] D loss: 0.1455, G loss: 2.0602\n",
      "[GAN Epoch 120/150] D loss: 0.2061, G loss: 5.1637\n",
      "[GAN Epoch 121/150] D loss: 0.1520, G loss: 1.8651\n",
      "[GAN Epoch 122/150] D loss: 0.0706, G loss: 4.9718\n",
      "[GAN Epoch 123/150] D loss: 0.0481, G loss: 4.2551\n",
      "[GAN Epoch 124/150] D loss: 0.1077, G loss: 2.2443\n",
      "[GAN Epoch 125/150] D loss: 0.1005, G loss: 4.2786\n",
      "[GAN Epoch 126/150] D loss: 0.1306, G loss: 2.2850\n",
      "[GAN Epoch 127/150] D loss: 0.1455, G loss: 4.1234\n",
      "[GAN Epoch 128/150] D loss: 0.2881, G loss: 1.2938\n",
      "[GAN Epoch 129/150] D loss: 0.6428, G loss: 7.4757\n",
      "[GAN Epoch 130/150] D loss: 0.0849, G loss: 2.8802\n",
      "[GAN Epoch 131/150] D loss: 0.1628, G loss: 1.7470\n",
      "[GAN Epoch 132/150] D loss: 0.1054, G loss: 5.3632\n",
      "[GAN Epoch 133/150] D loss: 0.0903, G loss: 4.5132\n",
      "[GAN Epoch 134/150] D loss: 0.2155, G loss: 1.5048\n",
      "[GAN Epoch 135/150] D loss: 0.2705, G loss: 4.8484\n",
      "[GAN Epoch 136/150] D loss: 0.2230, G loss: 1.6380\n",
      "[GAN Epoch 137/150] D loss: 0.2183, G loss: 4.2357\n",
      "[GAN Epoch 138/150] D loss: 0.2881, G loss: 1.6950\n",
      "[GAN Epoch 139/150] D loss: 0.3475, G loss: 4.4900\n",
      "[GAN Epoch 140/150] D loss: 0.4906, G loss: 1.0515\n",
      "[GAN Epoch 141/150] D loss: 0.5611, G loss: 6.6864\n",
      "[GAN Epoch 142/150] D loss: 0.3239, G loss: 3.7681\n",
      "[GAN Epoch 143/150] D loss: 0.0932, G loss: 4.5963\n",
      "[GAN Epoch 144/150] D loss: 0.0754, G loss: 3.5519\n",
      "[GAN Epoch 145/150] D loss: 0.1610, G loss: 1.6869\n",
      "[GAN Epoch 146/150] D loss: 0.1501, G loss: 3.2302\n",
      "[GAN Epoch 147/150] D loss: 0.2173, G loss: 2.0230\n",
      "[GAN Epoch 148/150] D loss: 0.2393, G loss: 3.1886\n",
      "[GAN Epoch 149/150] D loss: 0.4377, G loss: 0.7979\n",
      "[GAN Epoch 150/150] D loss: 0.8618, G loss: 7.0506\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1334, Train Acc=0.3461\n",
      "Epoch 2: Train Loss=1.1007, Train Acc=0.3329\n",
      "Epoch 3: Train Loss=1.0948, Train Acc=0.3445\n",
      "Epoch 4: Train Loss=1.0870, Train Acc=0.3513\n",
      "Epoch 5: Train Loss=1.0847, Train Acc=0.3542\n",
      "Epoch 6: Train Loss=1.0843, Train Acc=0.3439\n",
      "Epoch 7: Train Loss=1.0864, Train Acc=0.3529\n",
      "Epoch 8: Train Loss=1.0834, Train Acc=0.3574\n",
      "Epoch 9: Train Loss=1.0792, Train Acc=0.3606\n",
      "Epoch 10: Train Loss=1.0804, Train Acc=0.3497\n",
      "DNN Test Accuracy (After Concatenation) for sample size 100: 0.8577\n",
      "\n",
      "[Real Data Only] Training size: 300\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=1.0723, Train Acc=0.4333\n",
      "Epoch 2: Train Loss=0.6339, Train Acc=0.8033\n",
      "Epoch 3: Train Loss=0.4375, Train Acc=0.8900\n",
      "Epoch 4: Train Loss=0.3368, Train Acc=0.9100\n",
      "Epoch 5: Train Loss=0.2942, Train Acc=0.9100\n",
      "Epoch 6: Train Loss=0.2909, Train Acc=0.9067\n",
      "Epoch 7: Train Loss=0.2786, Train Acc=0.9200\n",
      "Epoch 8: Train Loss=0.2852, Train Acc=0.8867\n",
      "Epoch 9: Train Loss=0.2948, Train Acc=0.9200\n",
      "Epoch 10: Train Loss=0.2964, Train Acc=0.9200\n",
      "DNN Test Accuracy (Real Data) for sample size 300: 0.8928\n",
      "[GAN Epoch 1/150] D loss: 0.6910, G loss: 0.7108\n",
      "[GAN Epoch 2/150] D loss: 0.6302, G loss: 0.7082\n",
      "[GAN Epoch 3/150] D loss: 0.5764, G loss: 0.7077\n",
      "[GAN Epoch 4/150] D loss: 0.5239, G loss: 0.7070\n",
      "[GAN Epoch 5/150] D loss: 0.4748, G loss: 0.7069\n",
      "[GAN Epoch 6/150] D loss: 0.4340, G loss: 0.7090\n",
      "[GAN Epoch 7/150] D loss: 0.4016, G loss: 0.7205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 8/150] D loss: 0.3715, G loss: 0.7542\n",
      "[GAN Epoch 9/150] D loss: 0.3388, G loss: 0.8210\n",
      "[GAN Epoch 10/150] D loss: 0.3086, G loss: 0.9109\n",
      "[GAN Epoch 11/150] D loss: 0.2867, G loss: 1.0011\n",
      "[GAN Epoch 12/150] D loss: 0.2796, G loss: 1.0420\n",
      "[GAN Epoch 13/150] D loss: 0.2902, G loss: 1.0145\n",
      "[GAN Epoch 14/150] D loss: 0.3155, G loss: 0.9804\n",
      "[GAN Epoch 15/150] D loss: 0.3444, G loss: 0.9162\n",
      "[GAN Epoch 16/150] D loss: 0.3524, G loss: 0.8967\n",
      "[GAN Epoch 17/150] D loss: 0.3173, G loss: 0.9910\n",
      "[GAN Epoch 18/150] D loss: 0.2592, G loss: 1.1082\n",
      "[GAN Epoch 19/150] D loss: 0.2046, G loss: 1.3792\n",
      "[GAN Epoch 20/150] D loss: 0.1782, G loss: 1.3441\n",
      "[GAN Epoch 21/150] D loss: 0.1554, G loss: 1.6253\n",
      "[GAN Epoch 22/150] D loss: 0.1674, G loss: 1.4318\n",
      "[GAN Epoch 23/150] D loss: 0.1655, G loss: 1.5870\n",
      "[GAN Epoch 24/150] D loss: 0.1823, G loss: 1.3679\n",
      "[GAN Epoch 25/150] D loss: 0.1705, G loss: 1.7165\n",
      "[GAN Epoch 26/150] D loss: 0.2131, G loss: 1.1937\n",
      "[GAN Epoch 27/150] D loss: 0.1340, G loss: 2.0985\n",
      "[GAN Epoch 28/150] D loss: 0.1650, G loss: 1.4485\n",
      "[GAN Epoch 29/150] D loss: 0.1131, G loss: 2.1265\n",
      "[GAN Epoch 30/150] D loss: 0.1293, G loss: 1.8408\n",
      "[GAN Epoch 31/150] D loss: 0.1097, G loss: 2.2123\n",
      "[GAN Epoch 32/150] D loss: 0.1266, G loss: 1.8410\n",
      "[GAN Epoch 33/150] D loss: 0.1020, G loss: 2.8390\n",
      "[GAN Epoch 34/150] D loss: 0.2419, G loss: 1.1517\n",
      "[GAN Epoch 35/150] D loss: 0.0950, G loss: 3.7676\n",
      "[GAN Epoch 36/150] D loss: 0.0858, G loss: 2.1549\n",
      "[GAN Epoch 37/150] D loss: 0.1687, G loss: 1.4790\n",
      "[GAN Epoch 38/150] D loss: 0.1593, G loss: 3.2334\n",
      "[GAN Epoch 39/150] D loss: 0.3880, G loss: 0.7554\n",
      "[GAN Epoch 40/150] D loss: 0.0334, G loss: 3.6980\n",
      "[GAN Epoch 41/150] D loss: 0.0540, G loss: 3.8679\n",
      "[GAN Epoch 42/150] D loss: 0.0764, G loss: 2.2717\n",
      "[GAN Epoch 43/150] D loss: 0.1901, G loss: 1.3122\n",
      "[GAN Epoch 44/150] D loss: 0.1432, G loss: 3.3819\n",
      "[GAN Epoch 45/150] D loss: 0.0974, G loss: 1.8863\n",
      "[GAN Epoch 46/150] D loss: 0.0518, G loss: 2.5151\n",
      "[GAN Epoch 47/150] D loss: 0.0309, G loss: 3.1830\n",
      "[GAN Epoch 48/150] D loss: 0.0402, G loss: 3.0777\n",
      "[GAN Epoch 49/150] D loss: 0.0613, G loss: 2.6481\n",
      "[GAN Epoch 50/150] D loss: 0.0691, G loss: 2.7919\n",
      "[GAN Epoch 51/150] D loss: 0.0648, G loss: 2.7717\n",
      "[GAN Epoch 52/150] D loss: 0.0586, G loss: 2.8078\n",
      "[GAN Epoch 53/150] D loss: 0.0517, G loss: 3.1260\n",
      "[GAN Epoch 54/150] D loss: 0.0593, G loss: 2.7427\n",
      "[GAN Epoch 55/150] D loss: 0.0599, G loss: 3.3097\n",
      "[GAN Epoch 56/150] D loss: 0.1068, G loss: 1.9949\n",
      "[GAN Epoch 57/150] D loss: 0.2419, G loss: 5.5911\n",
      "[GAN Epoch 58/150] D loss: 0.9489, G loss: 0.2319\n",
      "[GAN Epoch 59/150] D loss: 0.0325, G loss: 7.4525\n",
      "[GAN Epoch 60/150] D loss: 0.1658, G loss: 9.3346\n",
      "[GAN Epoch 61/150] D loss: 0.0255, G loss: 7.1568\n",
      "[GAN Epoch 62/150] D loss: 0.0143, G loss: 4.6646\n",
      "[GAN Epoch 63/150] D loss: 0.0809, G loss: 2.1257\n",
      "[GAN Epoch 64/150] D loss: 0.1131, G loss: 1.7999\n",
      "[GAN Epoch 65/150] D loss: 0.0463, G loss: 3.7089\n",
      "[GAN Epoch 66/150] D loss: 0.0708, G loss: 3.8473\n",
      "[GAN Epoch 67/150] D loss: 0.0891, G loss: 2.3639\n",
      "[GAN Epoch 68/150] D loss: 0.1051, G loss: 2.2690\n",
      "[GAN Epoch 69/150] D loss: 0.1208, G loss: 3.0968\n",
      "[GAN Epoch 70/150] D loss: 0.2146, G loss: 1.3371\n",
      "[GAN Epoch 71/150] D loss: 0.6965, G loss: 5.6367\n",
      "[GAN Epoch 72/150] D loss: 2.1362, G loss: 0.0198\n",
      "[GAN Epoch 73/150] D loss: 0.7112, G loss: 0.3387\n",
      "[GAN Epoch 74/150] D loss: 0.1783, G loss: 6.0278\n",
      "[GAN Epoch 75/150] D loss: 0.3137, G loss: 6.5760\n",
      "[GAN Epoch 76/150] D loss: 0.0597, G loss: 3.8138\n",
      "[GAN Epoch 77/150] D loss: 0.1377, G loss: 1.6704\n",
      "[GAN Epoch 78/150] D loss: 0.2068, G loss: 1.3261\n",
      "[GAN Epoch 79/150] D loss: 0.1154, G loss: 2.5637\n",
      "[GAN Epoch 80/150] D loss: 0.1635, G loss: 2.6983\n",
      "[GAN Epoch 81/150] D loss: 0.2306, G loss: 1.5553\n",
      "[GAN Epoch 82/150] D loss: 0.2745, G loss: 1.6700\n",
      "[GAN Epoch 83/150] D loss: 0.3419, G loss: 1.6294\n",
      "[GAN Epoch 84/150] D loss: 0.4062, G loss: 1.0672\n",
      "[GAN Epoch 85/150] D loss: 0.4764, G loss: 2.4584\n",
      "[GAN Epoch 86/150] D loss: 0.9311, G loss: 0.2178\n",
      "[GAN Epoch 87/150] D loss: 0.3473, G loss: 3.0541\n",
      "[GAN Epoch 88/150] D loss: 0.2121, G loss: 2.2141\n",
      "[GAN Epoch 89/150] D loss: 0.2646, G loss: 1.2824\n",
      "[GAN Epoch 90/150] D loss: 0.1942, G loss: 2.3751\n",
      "[GAN Epoch 91/150] D loss: 0.1952, G loss: 2.2346\n",
      "[GAN Epoch 92/150] D loss: 0.2208, G loss: 1.6167\n",
      "[GAN Epoch 93/150] D loss: 0.2070, G loss: 2.2223\n",
      "[GAN Epoch 94/150] D loss: 0.2132, G loss: 1.8030\n",
      "[GAN Epoch 95/150] D loss: 0.2054, G loss: 1.9936\n",
      "[GAN Epoch 96/150] D loss: 0.2032, G loss: 1.9242\n",
      "[GAN Epoch 97/150] D loss: 0.1893, G loss: 2.1576\n",
      "[GAN Epoch 98/150] D loss: 0.1893, G loss: 1.8378\n",
      "[GAN Epoch 99/150] D loss: 0.1852, G loss: 2.8621\n",
      "[GAN Epoch 100/150] D loss: 0.3392, G loss: 0.9600\n",
      "[GAN Epoch 101/150] D loss: 0.6291, G loss: 5.6776\n",
      "[GAN Epoch 102/150] D loss: 0.2606, G loss: 1.1504\n",
      "[GAN Epoch 103/150] D loss: 0.0665, G loss: 2.8276\n",
      "[GAN Epoch 104/150] D loss: 0.0769, G loss: 3.4267\n",
      "[GAN Epoch 105/150] D loss: 0.1157, G loss: 2.3995\n",
      "[GAN Epoch 106/150] D loss: 0.1988, G loss: 1.7952\n",
      "[GAN Epoch 107/150] D loss: 0.2671, G loss: 2.6866\n",
      "[GAN Epoch 108/150] D loss: 0.4946, G loss: 0.7583\n",
      "[GAN Epoch 109/150] D loss: 0.7365, G loss: 5.3008\n",
      "[GAN Epoch 110/150] D loss: 0.2016, G loss: 1.5678\n",
      "[GAN Epoch 111/150] D loss: 0.1493, G loss: 1.9693\n",
      "[GAN Epoch 112/150] D loss: 0.1339, G loss: 3.3659\n",
      "[GAN Epoch 113/150] D loss: 0.1445, G loss: 2.3106\n",
      "[GAN Epoch 114/150] D loss: 0.2048, G loss: 1.7713\n",
      "[GAN Epoch 115/150] D loss: 0.2428, G loss: 3.2400\n",
      "[GAN Epoch 116/150] D loss: 0.3856, G loss: 1.0517\n",
      "[GAN Epoch 117/150] D loss: 0.5325, G loss: 5.2388\n",
      "[GAN Epoch 118/150] D loss: 0.1942, G loss: 1.6440\n",
      "[GAN Epoch 119/150] D loss: 0.1214, G loss: 2.3002\n",
      "[GAN Epoch 120/150] D loss: 0.1346, G loss: 2.8685\n",
      "[GAN Epoch 121/150] D loss: 0.2052, G loss: 1.7854\n",
      "[GAN Epoch 122/150] D loss: 0.2528, G loss: 2.8524\n",
      "[GAN Epoch 123/150] D loss: 0.4619, G loss: 0.8707\n",
      "[GAN Epoch 124/150] D loss: 0.8967, G loss: 5.8513\n",
      "[GAN Epoch 125/150] D loss: 0.3234, G loss: 1.1219\n",
      "[GAN Epoch 126/150] D loss: 0.0955, G loss: 3.1563\n",
      "[GAN Epoch 127/150] D loss: 0.1086, G loss: 3.1922\n",
      "[GAN Epoch 128/150] D loss: 0.1455, G loss: 2.0364\n",
      "[GAN Epoch 129/150] D loss: 0.1711, G loss: 2.0509\n",
      "[GAN Epoch 130/150] D loss: 0.2015, G loss: 2.4504\n",
      "[GAN Epoch 131/150] D loss: 0.2430, G loss: 1.5567\n",
      "[GAN Epoch 132/150] D loss: 0.2747, G loss: 3.5616\n",
      "[GAN Epoch 133/150] D loss: 0.3886, G loss: 0.9293\n",
      "[GAN Epoch 134/150] D loss: 0.4950, G loss: 6.0478\n",
      "[GAN Epoch 135/150] D loss: 0.0734, G loss: 2.9091\n",
      "[GAN Epoch 136/150] D loss: 0.3284, G loss: 0.9926\n",
      "[GAN Epoch 137/150] D loss: 0.1785, G loss: 5.4717\n",
      "[GAN Epoch 138/150] D loss: 0.1481, G loss: 4.8436\n",
      "[GAN Epoch 139/150] D loss: 0.1445, G loss: 1.9172\n",
      "[GAN Epoch 140/150] D loss: 0.1796, G loss: 1.7403\n",
      "[GAN Epoch 141/150] D loss: 0.2446, G loss: 3.5251\n",
      "[GAN Epoch 142/150] D loss: 0.3489, G loss: 0.9890\n",
      "[GAN Epoch 143/150] D loss: 0.5137, G loss: 4.6771\n",
      "[GAN Epoch 144/150] D loss: 0.3633, G loss: 0.9114\n",
      "[GAN Epoch 145/150] D loss: 0.2297, G loss: 4.4735\n",
      "[GAN Epoch 146/150] D loss: 0.1264, G loss: 3.1690\n",
      "[GAN Epoch 147/150] D loss: 0.2706, G loss: 1.1527\n",
      "[GAN Epoch 148/150] D loss: 0.2465, G loss: 4.0621\n",
      "[GAN Epoch 149/150] D loss: 0.1560, G loss: 2.2616\n",
      "[GAN Epoch 150/150] D loss: 0.2712, G loss: 1.2169\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.1442, Train Acc=0.3433\n",
      "Epoch 2: Train Loss=1.0654, Train Acc=0.3697\n",
      "Epoch 3: Train Loss=1.0461, Train Acc=0.3803\n",
      "Epoch 4: Train Loss=1.0353, Train Acc=0.3903\n",
      "Epoch 5: Train Loss=1.0275, Train Acc=0.3915\n",
      "Epoch 6: Train Loss=1.0212, Train Acc=0.4091\n",
      "Epoch 7: Train Loss=1.0267, Train Acc=0.3967\n",
      "Epoch 8: Train Loss=1.0202, Train Acc=0.4091\n",
      "Epoch 9: Train Loss=1.0186, Train Acc=0.4182\n",
      "Epoch 10: Train Loss=1.0195, Train Acc=0.4124\n",
      "DNN Test Accuracy (After Concatenation) for sample size 300: 0.9019\n",
      "\n",
      "[Real Data Only] Training size: 1000\n",
      "Training DNN on real data...\n",
      "Epoch 1: Train Loss=0.8579, Train Acc=0.6330\n",
      "Epoch 2: Train Loss=0.3837, Train Acc=0.8990\n",
      "Epoch 3: Train Loss=0.2882, Train Acc=0.9010\n",
      "Epoch 4: Train Loss=0.2823, Train Acc=0.9050\n",
      "Epoch 5: Train Loss=0.2983, Train Acc=0.9160\n",
      "Epoch 6: Train Loss=0.2812, Train Acc=0.9140\n",
      "Epoch 7: Train Loss=0.2854, Train Acc=0.9130\n",
      "Epoch 8: Train Loss=0.2929, Train Acc=0.9090\n",
      "Epoch 9: Train Loss=0.2774, Train Acc=0.9230\n",
      "Epoch 10: Train Loss=0.2614, Train Acc=0.9200\n",
      "DNN Test Accuracy (Real Data) for sample size 1000: 0.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 1/150] D loss: 0.6558, G loss: 0.6947\n",
      "[GAN Epoch 2/150] D loss: 0.5436, G loss: 0.6983\n",
      "[GAN Epoch 3/150] D loss: 0.4541, G loss: 0.6960\n",
      "[GAN Epoch 4/150] D loss: 0.4021, G loss: 0.6960\n",
      "[GAN Epoch 5/150] D loss: 0.3581, G loss: 0.7732\n",
      "[GAN Epoch 6/150] D loss: 0.3228, G loss: 0.9113\n",
      "[GAN Epoch 7/150] D loss: 0.3360, G loss: 0.8974\n",
      "[GAN Epoch 8/150] D loss: 0.3752, G loss: 0.8328\n",
      "[GAN Epoch 9/150] D loss: 0.3638, G loss: 0.8477\n",
      "[GAN Epoch 10/150] D loss: 0.3006, G loss: 1.0094\n",
      "[GAN Epoch 11/150] D loss: 0.2363, G loss: 1.1898\n",
      "[GAN Epoch 12/150] D loss: 0.2165, G loss: 1.2302\n",
      "[GAN Epoch 13/150] D loss: 0.2064, G loss: 1.3387\n",
      "[GAN Epoch 14/150] D loss: 0.1878, G loss: 1.4991\n",
      "[GAN Epoch 15/150] D loss: 0.1729, G loss: 1.7335\n",
      "[GAN Epoch 16/150] D loss: 0.2374, G loss: 1.7641\n",
      "[GAN Epoch 17/150] D loss: 0.1610, G loss: 1.5744\n",
      "[GAN Epoch 18/150] D loss: 0.2137, G loss: 1.5384\n",
      "[GAN Epoch 19/150] D loss: 0.3700, G loss: 1.9617\n",
      "[GAN Epoch 20/150] D loss: 0.0420, G loss: 3.2604\n",
      "[GAN Epoch 21/150] D loss: 0.1950, G loss: 1.5098\n",
      "[GAN Epoch 22/150] D loss: 0.3593, G loss: 1.7199\n",
      "[GAN Epoch 23/150] D loss: 0.0322, G loss: 3.9791\n",
      "[GAN Epoch 24/150] D loss: 0.0574, G loss: 2.9375\n",
      "[GAN Epoch 25/150] D loss: 0.1226, G loss: 2.1163\n",
      "[GAN Epoch 26/150] D loss: 0.0969, G loss: 2.2480\n",
      "[GAN Epoch 27/150] D loss: 0.0724, G loss: 2.6667\n",
      "[GAN Epoch 28/150] D loss: 0.0799, G loss: 2.5453\n",
      "[GAN Epoch 29/150] D loss: 0.0865, G loss: 2.5875\n",
      "[GAN Epoch 30/150] D loss: 0.0924, G loss: 2.8058\n",
      "[GAN Epoch 31/150] D loss: 0.1744, G loss: 3.2880\n",
      "[GAN Epoch 32/150] D loss: 0.0783, G loss: 2.1234\n",
      "[GAN Epoch 33/150] D loss: 0.0306, G loss: 4.0462\n",
      "[GAN Epoch 34/150] D loss: 0.0977, G loss: 2.2815\n",
      "[GAN Epoch 35/150] D loss: 0.5778, G loss: 1.6748\n",
      "[GAN Epoch 36/150] D loss: 0.8793, G loss: 4.1462\n",
      "[GAN Epoch 37/150] D loss: 0.2886, G loss: 1.9757\n",
      "[GAN Epoch 38/150] D loss: 0.1645, G loss: 3.0119\n",
      "[GAN Epoch 39/150] D loss: 0.2456, G loss: 1.8048\n",
      "[GAN Epoch 40/150] D loss: 0.2320, G loss: 1.7271\n",
      "[GAN Epoch 41/150] D loss: 0.3520, G loss: 1.6800\n",
      "[GAN Epoch 42/150] D loss: 0.8479, G loss: 1.6788\n",
      "[GAN Epoch 43/150] D loss: 0.2918, G loss: 2.3224\n",
      "[GAN Epoch 44/150] D loss: 0.2366, G loss: 1.4893\n",
      "[GAN Epoch 45/150] D loss: 0.2073, G loss: 1.9437\n",
      "[GAN Epoch 46/150] D loss: 0.2333, G loss: 1.7036\n",
      "[GAN Epoch 47/150] D loss: 0.2602, G loss: 1.6133\n",
      "[GAN Epoch 48/150] D loss: 0.2792, G loss: 1.5993\n",
      "[GAN Epoch 49/150] D loss: 0.3568, G loss: 1.8250\n",
      "[GAN Epoch 50/150] D loss: 0.3385, G loss: 2.5874\n",
      "[GAN Epoch 51/150] D loss: 0.1601, G loss: 2.1870\n",
      "[GAN Epoch 52/150] D loss: 0.2686, G loss: 1.9978\n",
      "[GAN Epoch 53/150] D loss: 0.5666, G loss: 2.7099\n",
      "[GAN Epoch 54/150] D loss: 0.2079, G loss: 1.6128\n",
      "[GAN Epoch 55/150] D loss: 0.1563, G loss: 3.1693\n",
      "[GAN Epoch 56/150] D loss: 0.3783, G loss: 2.4772\n",
      "[GAN Epoch 57/150] D loss: 0.2268, G loss: 1.6479\n",
      "[GAN Epoch 58/150] D loss: 0.2792, G loss: 1.8477\n",
      "[GAN Epoch 59/150] D loss: 0.3804, G loss: 2.5363\n",
      "[GAN Epoch 60/150] D loss: 0.1739, G loss: 2.3918\n",
      "[GAN Epoch 61/150] D loss: 0.2170, G loss: 1.8384\n",
      "[GAN Epoch 62/150] D loss: 0.4579, G loss: 1.8402\n",
      "[GAN Epoch 63/150] D loss: 0.4669, G loss: 4.2249\n",
      "[GAN Epoch 64/150] D loss: 0.1920, G loss: 1.9638\n",
      "[GAN Epoch 65/150] D loss: 0.1579, G loss: 2.3278\n",
      "[GAN Epoch 66/150] D loss: 0.1767, G loss: 2.0871\n",
      "[GAN Epoch 67/150] D loss: 0.2062, G loss: 1.8527\n",
      "[GAN Epoch 68/150] D loss: 0.2867, G loss: 1.8418\n",
      "[GAN Epoch 69/150] D loss: 0.3914, G loss: 3.5880\n",
      "[GAN Epoch 70/150] D loss: 0.1811, G loss: 2.6541\n",
      "[GAN Epoch 71/150] D loss: 0.2112, G loss: 1.6592\n",
      "[GAN Epoch 72/150] D loss: 0.4025, G loss: 3.0439\n",
      "[GAN Epoch 73/150] D loss: 0.1724, G loss: 2.2173\n",
      "[GAN Epoch 74/150] D loss: 0.2831, G loss: 2.4864\n",
      "[GAN Epoch 75/150] D loss: 0.2300, G loss: 2.0248\n",
      "[GAN Epoch 76/150] D loss: 0.2597, G loss: 2.6965\n",
      "[GAN Epoch 77/150] D loss: 0.2494, G loss: 2.0734\n",
      "[GAN Epoch 78/150] D loss: 0.3500, G loss: 2.9492\n",
      "[GAN Epoch 79/150] D loss: 0.3071, G loss: 1.9054\n",
      "[GAN Epoch 80/150] D loss: 0.2630, G loss: 2.4662\n",
      "[GAN Epoch 81/150] D loss: 0.3562, G loss: 2.6038\n",
      "[GAN Epoch 82/150] D loss: 0.1764, G loss: 1.9732\n",
      "[GAN Epoch 83/150] D loss: 0.1913, G loss: 1.9439\n",
      "[GAN Epoch 84/150] D loss: 0.3157, G loss: 1.6085\n",
      "[GAN Epoch 85/150] D loss: 0.7139, G loss: 3.6567\n",
      "[GAN Epoch 86/150] D loss: 0.3810, G loss: 3.1048\n",
      "[GAN Epoch 87/150] D loss: 0.2680, G loss: 1.4046\n",
      "[GAN Epoch 88/150] D loss: 0.3983, G loss: 4.4463\n",
      "[GAN Epoch 89/150] D loss: 0.2515, G loss: 1.9866\n",
      "[GAN Epoch 90/150] D loss: 0.2287, G loss: 1.7642\n",
      "[GAN Epoch 91/150] D loss: 0.3368, G loss: 2.9182\n",
      "[GAN Epoch 92/150] D loss: 0.3009, G loss: 2.5570\n",
      "[GAN Epoch 93/150] D loss: 0.2909, G loss: 2.2372\n",
      "[GAN Epoch 94/150] D loss: 0.2315, G loss: 1.8344\n",
      "[GAN Epoch 95/150] D loss: 0.4062, G loss: 2.7243\n",
      "[GAN Epoch 96/150] D loss: 0.3015, G loss: 1.7319\n",
      "[GAN Epoch 97/150] D loss: 0.2022, G loss: 2.0224\n",
      "[GAN Epoch 98/150] D loss: 0.2119, G loss: 1.9973\n",
      "[GAN Epoch 99/150] D loss: 0.2181, G loss: 1.9689\n",
      "[GAN Epoch 100/150] D loss: 0.2393, G loss: 2.3402\n",
      "[GAN Epoch 101/150] D loss: 0.6289, G loss: 3.6821\n",
      "[GAN Epoch 102/150] D loss: 0.2178, G loss: 2.3702\n",
      "[GAN Epoch 103/150] D loss: 0.1582, G loss: 2.3916\n",
      "[GAN Epoch 104/150] D loss: 0.2219, G loss: 2.0107\n",
      "[GAN Epoch 105/150] D loss: 0.3351, G loss: 2.5411\n",
      "[GAN Epoch 106/150] D loss: 0.3232, G loss: 2.5249\n",
      "[GAN Epoch 107/150] D loss: 0.1901, G loss: 1.9137\n",
      "[GAN Epoch 108/150] D loss: 0.2305, G loss: 3.8001\n",
      "[GAN Epoch 109/150] D loss: 0.2091, G loss: 2.2911\n",
      "[GAN Epoch 110/150] D loss: 0.2327, G loss: 2.3045\n",
      "[GAN Epoch 111/150] D loss: 0.2996, G loss: 2.3686\n",
      "[GAN Epoch 112/150] D loss: 0.6994, G loss: 3.8652\n",
      "[GAN Epoch 113/150] D loss: 0.2975, G loss: 2.9759\n",
      "[GAN Epoch 114/150] D loss: 0.1878, G loss: 2.6432\n",
      "[GAN Epoch 115/150] D loss: 0.1901, G loss: 2.0537\n",
      "[GAN Epoch 116/150] D loss: 0.2884, G loss: 1.8549\n",
      "[GAN Epoch 117/150] D loss: 0.5348, G loss: 3.3543\n",
      "[GAN Epoch 118/150] D loss: 0.1988, G loss: 2.3749\n",
      "[GAN Epoch 119/150] D loss: 0.2302, G loss: 1.9909\n",
      "[GAN Epoch 120/150] D loss: 0.2680, G loss: 1.8319\n",
      "[GAN Epoch 121/150] D loss: 0.3180, G loss: 1.8402\n",
      "[GAN Epoch 122/150] D loss: 0.4816, G loss: 3.1060\n",
      "[GAN Epoch 123/150] D loss: 0.1518, G loss: 2.9789\n",
      "[GAN Epoch 124/150] D loss: 0.2084, G loss: 2.3231\n",
      "[GAN Epoch 125/150] D loss: 0.2605, G loss: 2.1449\n",
      "[GAN Epoch 126/150] D loss: 0.4685, G loss: 3.0394\n",
      "[GAN Epoch 127/150] D loss: 0.3747, G loss: 2.4520\n",
      "[GAN Epoch 128/150] D loss: 0.2089, G loss: 2.0564\n",
      "[GAN Epoch 129/150] D loss: 0.2810, G loss: 2.5960\n",
      "[GAN Epoch 130/150] D loss: 0.2846, G loss: 2.0592\n",
      "[GAN Epoch 131/150] D loss: 0.4405, G loss: 2.5594\n",
      "[GAN Epoch 132/150] D loss: 0.2751, G loss: 2.7310\n",
      "[GAN Epoch 133/150] D loss: 0.1951, G loss: 2.3405\n",
      "[GAN Epoch 134/150] D loss: 0.2438, G loss: 2.2174\n",
      "[GAN Epoch 135/150] D loss: 0.4925, G loss: 3.1980\n",
      "[GAN Epoch 136/150] D loss: 0.2997, G loss: 2.3761\n",
      "[GAN Epoch 137/150] D loss: 0.2000, G loss: 2.1592\n",
      "[GAN Epoch 138/150] D loss: 0.3580, G loss: 2.4512\n",
      "[GAN Epoch 139/150] D loss: 0.4684, G loss: 2.1961\n",
      "[GAN Epoch 140/150] D loss: 0.3964, G loss: 4.0178\n",
      "[GAN Epoch 141/150] D loss: 0.2710, G loss: 2.5120\n",
      "[GAN Epoch 142/150] D loss: 0.2411, G loss: 1.8807\n",
      "[GAN Epoch 143/150] D loss: 0.3620, G loss: 1.9456\n",
      "[GAN Epoch 144/150] D loss: 0.4615, G loss: 2.6042\n",
      "[GAN Epoch 145/150] D loss: 0.2780, G loss: 2.4727\n",
      "[GAN Epoch 146/150] D loss: 0.2701, G loss: 2.0351\n",
      "[GAN Epoch 147/150] D loss: 0.3547, G loss: 1.6682\n",
      "[GAN Epoch 148/150] D loss: 0.7420, G loss: 2.1143\n",
      "[GAN Epoch 149/150] D loss: 0.4144, G loss: 3.7998\n",
      "[GAN Epoch 150/150] D loss: 0.3304, G loss: 2.5276\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=1.0960, Train Acc=0.4377\n",
      "Epoch 2: Train Loss=0.9651, Train Acc=0.4650\n",
      "Epoch 3: Train Loss=0.9219, Train Acc=0.4755\n",
      "Epoch 4: Train Loss=0.9044, Train Acc=0.4923\n",
      "Epoch 5: Train Loss=0.9002, Train Acc=0.4858\n",
      "Epoch 6: Train Loss=0.8935, Train Acc=0.4855\n",
      "Epoch 7: Train Loss=0.8882, Train Acc=0.4878\n",
      "Epoch 8: Train Loss=0.8880, Train Acc=0.4940\n",
      "Epoch 9: Train Loss=0.8869, Train Acc=0.4865\n",
      "Epoch 10: Train Loss=0.8866, Train Acc=0.4895\n",
      "DNN Test Accuracy (After Concatenation) for sample size 1000: 0.9050\n",
      "\n",
      "[Real Data Only] Training size: 7872\n",
      "Training DNN on real data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.3772, Train Acc=0.8697\n",
      "Epoch 2: Train Loss=0.2713, Train Acc=0.9061\n",
      "Epoch 3: Train Loss=0.2402, Train Acc=0.9131\n",
      "Epoch 4: Train Loss=0.2375, Train Acc=0.9130\n",
      "Epoch 5: Train Loss=0.2315, Train Acc=0.9153\n",
      "Epoch 6: Train Loss=0.2307, Train Acc=0.9163\n",
      "Epoch 7: Train Loss=0.2283, Train Acc=0.9160\n",
      "Epoch 8: Train Loss=0.2204, Train Acc=0.9174\n",
      "Epoch 9: Train Loss=0.2219, Train Acc=0.9178\n",
      "Epoch 10: Train Loss=0.2216, Train Acc=0.9190\n",
      "DNN Test Accuracy (Real Data) for sample size 7872: 0.9126\n",
      "[GAN Epoch 1/150] D loss: 0.4463, G loss: 0.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2156/1033922111.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  emb = torch.tensor(self.embeddings[idx], dtype=torch.float)\n",
      "/tmp/ipykernel_2156/3598883103.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch 2/150] D loss: 0.2802, G loss: 1.1918\n",
      "[GAN Epoch 3/150] D loss: 0.1580, G loss: 2.1290\n",
      "[GAN Epoch 4/150] D loss: 0.0692, G loss: 2.7417\n",
      "[GAN Epoch 5/150] D loss: 0.3929, G loss: 3.2845\n",
      "[GAN Epoch 6/150] D loss: 0.2914, G loss: 1.6665\n",
      "[GAN Epoch 7/150] D loss: 0.2330, G loss: 2.4159\n",
      "[GAN Epoch 8/150] D loss: 0.2661, G loss: 2.5799\n",
      "[GAN Epoch 9/150] D loss: 0.2796, G loss: 2.6299\n",
      "[GAN Epoch 10/150] D loss: 0.2902, G loss: 2.3489\n",
      "[GAN Epoch 11/150] D loss: 0.3097, G loss: 2.3648\n",
      "[GAN Epoch 12/150] D loss: 0.3707, G loss: 2.4565\n",
      "[GAN Epoch 13/150] D loss: 0.3505, G loss: 2.1617\n",
      "[GAN Epoch 14/150] D loss: 0.3820, G loss: 2.1035\n",
      "[GAN Epoch 15/150] D loss: 0.3781, G loss: 1.9140\n",
      "[GAN Epoch 16/150] D loss: 0.4129, G loss: 1.6732\n",
      "[GAN Epoch 17/150] D loss: 0.4624, G loss: 1.8350\n",
      "[GAN Epoch 18/150] D loss: 0.4460, G loss: 1.7501\n",
      "[GAN Epoch 19/150] D loss: 0.4399, G loss: 1.7790\n",
      "[GAN Epoch 20/150] D loss: 0.4116, G loss: 1.7938\n",
      "[GAN Epoch 21/150] D loss: 0.4532, G loss: 1.7824\n",
      "[GAN Epoch 22/150] D loss: 0.4402, G loss: 1.8111\n",
      "[GAN Epoch 23/150] D loss: 0.4540, G loss: 1.7644\n",
      "[GAN Epoch 24/150] D loss: 0.4912, G loss: 1.6037\n",
      "[GAN Epoch 25/150] D loss: 0.4816, G loss: 1.5797\n",
      "[GAN Epoch 26/150] D loss: 0.4740, G loss: 1.4913\n",
      "[GAN Epoch 27/150] D loss: 0.4913, G loss: 1.6408\n",
      "[GAN Epoch 28/150] D loss: 0.4506, G loss: 1.6012\n",
      "[GAN Epoch 29/150] D loss: 0.4644, G loss: 1.5883\n",
      "[GAN Epoch 30/150] D loss: 0.5246, G loss: 1.6573\n",
      "[GAN Epoch 31/150] D loss: 0.4566, G loss: 1.5897\n",
      "[GAN Epoch 32/150] D loss: 0.4514, G loss: 1.7615\n",
      "[GAN Epoch 33/150] D loss: 0.5133, G loss: 1.7821\n",
      "[GAN Epoch 34/150] D loss: 0.4886, G loss: 1.6579\n",
      "[GAN Epoch 35/150] D loss: 0.4676, G loss: 1.5095\n",
      "[GAN Epoch 36/150] D loss: 0.4557, G loss: 1.5971\n",
      "[GAN Epoch 37/150] D loss: 0.4949, G loss: 1.7403\n",
      "[GAN Epoch 38/150] D loss: 0.5105, G loss: 1.6795\n",
      "[GAN Epoch 39/150] D loss: 0.4907, G loss: 1.6014\n",
      "[GAN Epoch 40/150] D loss: 0.5072, G loss: 1.5303\n",
      "[GAN Epoch 41/150] D loss: 0.4313, G loss: 1.4892\n",
      "[GAN Epoch 42/150] D loss: 0.5150, G loss: 1.5581\n",
      "[GAN Epoch 43/150] D loss: 0.4831, G loss: 1.6816\n",
      "[GAN Epoch 44/150] D loss: 0.4773, G loss: 1.6178\n",
      "[GAN Epoch 45/150] D loss: 0.4290, G loss: 1.5051\n",
      "[GAN Epoch 46/150] D loss: 0.4470, G loss: 1.5635\n",
      "[GAN Epoch 47/150] D loss: 0.4778, G loss: 1.6053\n",
      "[GAN Epoch 48/150] D loss: 0.5078, G loss: 1.5026\n",
      "[GAN Epoch 49/150] D loss: 0.5121, G loss: 1.4919\n",
      "[GAN Epoch 50/150] D loss: 0.4782, G loss: 1.4966\n",
      "[GAN Epoch 51/150] D loss: 0.5004, G loss: 1.4155\n",
      "[GAN Epoch 52/150] D loss: 0.4887, G loss: 1.4168\n",
      "[GAN Epoch 53/150] D loss: 0.5513, G loss: 1.3505\n",
      "[GAN Epoch 54/150] D loss: 0.5280, G loss: 1.3276\n",
      "[GAN Epoch 55/150] D loss: 0.5525, G loss: 1.3753\n",
      "[GAN Epoch 56/150] D loss: 0.5283, G loss: 1.3108\n",
      "[GAN Epoch 57/150] D loss: 0.4963, G loss: 1.3322\n",
      "[GAN Epoch 58/150] D loss: 0.5036, G loss: 1.3366\n",
      "[GAN Epoch 59/150] D loss: 0.5422, G loss: 1.3975\n",
      "[GAN Epoch 60/150] D loss: 0.5128, G loss: 1.2765\n",
      "[GAN Epoch 61/150] D loss: 0.5388, G loss: 1.3040\n",
      "[GAN Epoch 62/150] D loss: 0.5350, G loss: 1.3197\n",
      "[GAN Epoch 63/150] D loss: 0.5244, G loss: 1.3185\n",
      "[GAN Epoch 64/150] D loss: 0.5202, G loss: 1.2914\n",
      "[GAN Epoch 65/150] D loss: 0.5374, G loss: 1.3076\n",
      "[GAN Epoch 66/150] D loss: 0.4974, G loss: 1.3810\n",
      "[GAN Epoch 67/150] D loss: 0.5250, G loss: 1.3344\n",
      "[GAN Epoch 68/150] D loss: 0.4974, G loss: 1.1837\n",
      "[GAN Epoch 69/150] D loss: 0.5788, G loss: 1.3255\n",
      "[GAN Epoch 70/150] D loss: 0.5115, G loss: 1.2450\n",
      "[GAN Epoch 71/150] D loss: 0.5142, G loss: 1.3243\n",
      "[GAN Epoch 72/150] D loss: 0.5375, G loss: 1.2909\n",
      "[GAN Epoch 73/150] D loss: 0.5238, G loss: 1.3341\n",
      "[GAN Epoch 74/150] D loss: 0.5461, G loss: 1.2546\n",
      "[GAN Epoch 75/150] D loss: 0.5479, G loss: 1.2596\n",
      "[GAN Epoch 76/150] D loss: 0.5012, G loss: 1.1003\n",
      "[GAN Epoch 77/150] D loss: 0.5530, G loss: 1.3232\n",
      "[GAN Epoch 78/150] D loss: 0.5187, G loss: 1.2286\n",
      "[GAN Epoch 79/150] D loss: 0.5576, G loss: 1.4097\n",
      "[GAN Epoch 80/150] D loss: 0.4887, G loss: 1.2254\n",
      "[GAN Epoch 81/150] D loss: 0.5446, G loss: 1.2047\n",
      "[GAN Epoch 82/150] D loss: 0.5401, G loss: 1.3229\n",
      "[GAN Epoch 83/150] D loss: 0.5385, G loss: 1.2645\n",
      "[GAN Epoch 84/150] D loss: 0.5200, G loss: 1.2270\n",
      "[GAN Epoch 85/150] D loss: 0.5312, G loss: 1.1424\n",
      "[GAN Epoch 86/150] D loss: 0.5070, G loss: 1.2039\n",
      "[GAN Epoch 87/150] D loss: 0.5104, G loss: 1.1522\n",
      "[GAN Epoch 88/150] D loss: 0.5656, G loss: 1.3753\n",
      "[GAN Epoch 89/150] D loss: 0.5182, G loss: 1.1929\n",
      "[GAN Epoch 90/150] D loss: 0.5063, G loss: 1.2055\n",
      "[GAN Epoch 91/150] D loss: 0.4983, G loss: 1.2381\n",
      "[GAN Epoch 92/150] D loss: 0.5403, G loss: 1.3082\n",
      "[GAN Epoch 93/150] D loss: 0.4895, G loss: 1.1691\n",
      "[GAN Epoch 94/150] D loss: 0.5412, G loss: 1.2820\n",
      "[GAN Epoch 95/150] D loss: 0.5116, G loss: 1.1549\n",
      "[GAN Epoch 96/150] D loss: 0.5025, G loss: 1.3311\n",
      "[GAN Epoch 97/150] D loss: 0.5312, G loss: 1.1665\n",
      "[GAN Epoch 98/150] D loss: 0.5251, G loss: 1.3461\n",
      "[GAN Epoch 99/150] D loss: 0.5051, G loss: 1.2043\n",
      "[GAN Epoch 100/150] D loss: 0.5104, G loss: 1.3823\n",
      "[GAN Epoch 101/150] D loss: 0.5046, G loss: 1.3128\n",
      "[GAN Epoch 102/150] D loss: 0.4764, G loss: 1.2668\n",
      "[GAN Epoch 103/150] D loss: 0.4812, G loss: 1.2654\n",
      "[GAN Epoch 104/150] D loss: 0.5141, G loss: 1.3489\n",
      "[GAN Epoch 105/150] D loss: 0.5055, G loss: 1.2538\n",
      "[GAN Epoch 106/150] D loss: 0.5425, G loss: 1.2895\n",
      "[GAN Epoch 107/150] D loss: 0.5431, G loss: 1.3022\n",
      "[GAN Epoch 108/150] D loss: 0.5569, G loss: 1.2635\n",
      "[GAN Epoch 109/150] D loss: 0.5098, G loss: 1.1033\n",
      "[GAN Epoch 110/150] D loss: 0.6066, G loss: 1.3111\n",
      "[GAN Epoch 111/150] D loss: 0.4994, G loss: 1.1283\n",
      "[GAN Epoch 112/150] D loss: 0.5501, G loss: 1.1078\n",
      "[GAN Epoch 113/150] D loss: 0.5569, G loss: 1.2873\n",
      "[GAN Epoch 114/150] D loss: 0.5346, G loss: 1.1138\n",
      "[GAN Epoch 115/150] D loss: 0.5098, G loss: 1.1487\n",
      "[GAN Epoch 116/150] D loss: 0.6076, G loss: 1.2459\n",
      "[GAN Epoch 117/150] D loss: 0.4871, G loss: 1.2250\n",
      "[GAN Epoch 118/150] D loss: 0.5020, G loss: 1.1397\n",
      "[GAN Epoch 119/150] D loss: 0.5557, G loss: 1.2535\n",
      "[GAN Epoch 120/150] D loss: 0.5417, G loss: 1.1453\n",
      "[GAN Epoch 121/150] D loss: 0.5435, G loss: 1.1765\n",
      "[GAN Epoch 122/150] D loss: 0.5258, G loss: 1.1356\n",
      "[GAN Epoch 123/150] D loss: 0.5282, G loss: 1.2872\n",
      "[GAN Epoch 124/150] D loss: 0.5429, G loss: 1.2281\n",
      "[GAN Epoch 125/150] D loss: 0.5251, G loss: 1.1241\n",
      "[GAN Epoch 126/150] D loss: 0.5746, G loss: 1.3364\n",
      "[GAN Epoch 127/150] D loss: 0.5807, G loss: 1.1561\n",
      "[GAN Epoch 128/150] D loss: 0.5522, G loss: 0.9998\n",
      "[GAN Epoch 129/150] D loss: 0.5480, G loss: 1.1764\n",
      "[GAN Epoch 130/150] D loss: 0.6351, G loss: 1.1564\n",
      "[GAN Epoch 131/150] D loss: 0.5462, G loss: 1.0286\n",
      "[GAN Epoch 132/150] D loss: 0.5647, G loss: 1.0913\n",
      "[GAN Epoch 133/150] D loss: 0.5533, G loss: 1.1153\n",
      "[GAN Epoch 134/150] D loss: 0.5717, G loss: 1.1540\n",
      "[GAN Epoch 135/150] D loss: 0.5678, G loss: 1.0947\n",
      "[GAN Epoch 136/150] D loss: 0.6196, G loss: 1.2384\n",
      "[GAN Epoch 137/150] D loss: 0.5716, G loss: 1.0578\n",
      "[GAN Epoch 138/150] D loss: 0.5460, G loss: 1.0048\n",
      "[GAN Epoch 139/150] D loss: 0.5649, G loss: 1.0387\n",
      "[GAN Epoch 140/150] D loss: 0.6248, G loss: 1.1655\n",
      "[GAN Epoch 141/150] D loss: 0.5194, G loss: 1.0023\n",
      "[GAN Epoch 142/150] D loss: 0.6354, G loss: 1.1396\n",
      "[GAN Epoch 143/150] D loss: 0.5407, G loss: 1.0948\n",
      "[GAN Epoch 144/150] D loss: 0.5845, G loss: 1.1189\n",
      "[GAN Epoch 145/150] D loss: 0.5640, G loss: 0.9777\n",
      "[GAN Epoch 146/150] D loss: 0.5891, G loss: 1.1509\n",
      "[GAN Epoch 147/150] D loss: 0.5825, G loss: 1.0891\n",
      "[GAN Epoch 148/150] D loss: 0.6075, G loss: 1.0785\n",
      "[GAN Epoch 149/150] D loss: 0.5600, G loss: 0.9844\n",
      "[GAN Epoch 150/150] D loss: 0.5647, G loss: 0.9811\n",
      "Training DNN on real + synthetic (concatenated) data...\n",
      "Epoch 1: Train Loss=0.6370, Train Acc=0.7402\n",
      "Epoch 2: Train Loss=0.5056, Train Acc=0.7625\n",
      "Epoch 3: Train Loss=0.4765, Train Acc=0.7733\n",
      "Epoch 4: Train Loss=0.4659, Train Acc=0.7837\n",
      "Epoch 5: Train Loss=0.4572, Train Acc=0.7896\n",
      "Epoch 6: Train Loss=0.4526, Train Acc=0.7930\n",
      "Epoch 7: Train Loss=0.4458, Train Acc=0.7958\n",
      "Epoch 8: Train Loss=0.4407, Train Acc=0.7982\n",
      "Epoch 9: Train Loss=0.4359, Train Acc=0.8076\n",
      "Epoch 10: Train Loss=0.4296, Train Acc=0.8082\n",
      "DNN Test Accuracy (After Concatenation) for sample size 7872: 0.9121\n",
      "Accuracy Summary:\n",
      "╒═════════════════╤══════════════════════╤════════════════════════════════╕\n",
      "│   Train Samples │   Real Only Accuracy │   After Concatenation Accuracy │\n",
      "╞═════════════════╪══════════════════════╪════════════════════════════════╡\n",
      "│              20 │             0.799797 │                       0.774898 │\n",
      "├─────────────────┼──────────────────────┼────────────────────────────────┤\n",
      "│              50 │             0.863821 │                       0.801829 │\n",
      "├─────────────────┼──────────────────────┼────────────────────────────────┤\n",
      "│              70 │             0.879573 │                       0.876524 │\n",
      "├─────────────────┼──────────────────────┼────────────────────────────────┤\n",
      "│             100 │             0.885671 │                       0.857724 │\n",
      "├─────────────────┼──────────────────────┼────────────────────────────────┤\n",
      "│             300 │             0.892785 │                       0.901931 │\n",
      "├─────────────────┼──────────────────────┼────────────────────────────────┤\n",
      "│            1000 │             0.900407 │                       0.90498  │\n",
      "├─────────────────┼──────────────────────┼────────────────────────────────┤\n",
      "│            7872 │             0.912602 │                       0.912093 │\n",
      "╘═════════════════╧══════════════════════╧════════════════════════════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n",
      "findfont: Font family 'Times New Roman' not found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_sizes = [20, 50, 70, 100, 300, 1000, len(train_ds)]\n",
    "dnn_accuracy_before = {}\n",
    "dnn_accuracy_after = {}\n",
    "\n",
    "for size in sample_sizes:\n",
    "    print(f\"\\n[Real Data Only] Training size: {size}\")\n",
    "    train_subset = torch.utils.data.Subset(train_ds, range(size))\n",
    "    train_loader = get_loader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = get_loader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = train_ds[0][0].shape[0]  \n",
    "    hidden_dim = 128\n",
    "    num_classes = len(np.unique(train_y_full)) if 'train_y_full' in globals() else 3\n",
    "    dnn_model = DNNClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, dropout=0.5)\n",
    "    dnn_model.to(device)\n",
    "    \n",
    "    print(\"Training DNN on real data...\")\n",
    "    acc_before, preds_before, labels_before = train_and_evaluate_dnn(dnn_model, train_loader, test_loader, num_epochs, learning_rate, device)\n",
    "    print(f\"DNN Test Accuracy (Real Data) for sample size {size}: {acc_before:.4f}\")\n",
    "    dnn_accuracy_before[size] = acc_before\n",
    "    \n",
    "\n",
    "    generator = Generator(\n",
    "        latent_dim=latent_dim,\n",
    "        condition_dim=condition_dim,\n",
    "        num_classes=num_classes,\n",
    "        start_dim=latent_dim * 2,\n",
    "        n_layer=3,\n",
    "        output_dim=input_dim  \n",
    "    )\n",
    "    discriminator = Discriminator(\n",
    "        condition_dim=condition_dim,\n",
    "        num_classes=num_classes,\n",
    "        start_dim=256,\n",
    "        n_layer=3,\n",
    "        input_dim=input_dim\n",
    "    )\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "    adversarial_loss = nn.BCELoss().to(device)\n",
    "\n",
    "\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "    \n",
    "    gan_loader = get_loader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(gan_epochs):\n",
    "        d_loss_epoch, g_loss_epoch = 0.0, 0.0\n",
    "        for embeddings, labels in gan_loader:\n",
    "            embeddings = embeddings.to(device)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            b_size = embeddings.size(0)\n",
    "            valid = torch.ones(b_size, 1, device=device)\n",
    "            fake = torch.zeros(b_size, 1, device=device)\n",
    "            # -----------------\n",
    "            # Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(b_size, latent_dim, device=device)\n",
    "            gen_data = generator(z, labels)\n",
    "            g_loss = adversarial_loss(discriminator(gen_data, labels), valid)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            # -----------------\n",
    "            # Train Discriminator\n",
    "            # -----------------\n",
    "            optimizer_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(embeddings, labels), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_data.detach(), labels), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            d_loss_epoch += d_loss.item()\n",
    "            g_loss_epoch += g_loss.item()\n",
    "        print(f\"[GAN Epoch {epoch+1}/{gan_epochs}] D loss: {d_loss_epoch/len(gan_loader):.4f}, G loss: {g_loss_epoch/len(gan_loader):.4f}\")\n",
    "    \n",
    "\n",
    "    synthetic_data_list = []\n",
    "    synthetic_labels_list = []\n",
    "    unique_labels = np.unique(train_y_full) if 'train_y_full' in globals() else [0,1,2]\n",
    "    for lab in unique_labels:\n",
    "        lab_tensor = torch.full((generation_size,), lab, dtype=torch.long, device=device)\n",
    "        z = torch.randn(generation_size, latent_dim, device=device)\n",
    "        synth = generator(z, lab_tensor).cpu().detach().numpy()\n",
    "        synthetic_data_list.append(synth)\n",
    "        synthetic_labels_list.append(np.full((generation_size,), lab))\n",
    "    synthetic_x = np.concatenate(synthetic_data_list, axis=0)\n",
    "    synthetic_y = np.concatenate(synthetic_labels_list, axis=0)\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Concatenate original training data with synthetic data\n",
    "    # -------------------------------\n",
    "    X_train = train_x_full[:size]\n",
    "    y_train = train_y_full[:size]\n",
    "    train_combined_x = np.concatenate([X_train, synthetic_x], axis=0)\n",
    "    train_combined_y = np.concatenate([y_train, synthetic_y], axis=0)\n",
    "    \n",
    "    train_combined_dataset = TensorDataset(torch.tensor(train_combined_x, dtype=torch.float),\n",
    "                                             torch.tensor(train_combined_y, dtype=torch.long))\n",
    "    train_combined_loader = DataLoader(train_combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dnn_model_aug = DNNClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, dropout=0.5)\n",
    "    dnn_model_aug.to(device)\n",
    "    \n",
    "    print(\"Training DNN on real + synthetic (concatenated) data...\")\n",
    "    acc_after, preds_after, labels_after = train_and_evaluate_dnn(dnn_model_aug, train_combined_loader, test_loader, num_epochs, learning_rate, device)\n",
    "    print(f\"DNN Test Accuracy (After Concatenation) for sample size {size}: {acc_after:.4f}\")\n",
    "    dnn_accuracy_after[size] = acc_after\n",
    "\n",
    "summary_df = pd.DataFrame(\n",
    "    [[s, dnn_accuracy_before[s], dnn_accuracy_after[s]] for s in sample_sizes],\n",
    "    columns=[\"Train Samples\", \"Real Only Accuracy\", \"After Concatenation Accuracy\"]\n",
    ")\n",
    "print(\"Accuracy Summary:\")\n",
    "print(tabulate.tabulate(summary_df.values, headers=summary_df.columns, tablefmt=\"fancy_grid\"))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(sample_sizes, list(dnn_accuracy_before.values()), marker='o', linestyle='-', color='b', markersize=8, label=\"Real Only\")\n",
    "plt.plot(sample_sizes, list(dnn_accuracy_after.values()), marker='s', linestyle='--', color='r', markersize=8, label=\"After Concatenation\")\n",
    "plt.xlabel(\"Training Size\", fontsize=14, fontfamily=\"Times New Roman\")\n",
    "plt.ylabel(\"Accuracy\", fontsize=14, fontfamily=\"Times New Roman\")\n",
    "plt.title(\"DNN Accuracy vs. Training Size (Real vs. Augmented)\", fontsize=16, fontfamily=\"Times New Roman\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
